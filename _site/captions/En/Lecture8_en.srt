1
00:00:00,000 --> 00:00:07,519
clocks let's let's get started so I know
lecture today a little bit of a break so

2
00:00:07,519 --> 00:00:11,269
today we're the last time we talked
about sort of we saw all the parts of

3
00:00:11,269 --> 00:00:14,439
comments we put everything together
today we're going to see some

4
00:00:14,439 --> 00:00:16,250
applications of contacts

5
00:00:16,250 --> 00:00:20,550
aspect actually dive inside images and
talk about spatial localization and

6
00:00:20,550 --> 00:00:25,550
detection we were we actually moved this
lecture up a little bit we had it later

7
00:00:25,550 --> 00:00:29,080
on the schedule we saw a lot of guys
were interested in this type of projects

8
00:00:29,079 --> 00:00:31,839
who wanted to move it earlier to kind of
give you an idea of what's what's

9
00:00:31,839 --> 00:00:38,378
feasible so first couple administrative
things are the project proposals were

10
00:00:38,378 --> 00:00:41,988
doing Saturday my inbox kind of exploded
over the weekend so I think most of you

11
00:00:41,988 --> 00:00:45,909
submit it but if you didn't you should
probably get on that we're in the

12
00:00:45,909 --> 00:00:49,328
process of looking through those will go
to make sure that the project proposals

13
00:00:49,329 --> 00:00:52,530
are reasonable never once admitted one
so we'll hopefully get back to you on

14
00:00:52,530 --> 00:01:02,149
your projects this week also home or two
is due on Friday so who's who's done who

15
00:01:02,149 --> 00:01:04,519
stuck on patch norm

16
00:01:04,519 --> 00:01:09,820
okay good good that's fewer hands then
we saw last week so we're making

17
00:01:09,819 --> 00:01:13,688
progress also keep in mind that we're
asking you to actually trained a pretty

18
00:01:13,688 --> 00:01:17,798
big continent on C far for this homework
so if you're starting to train on

19
00:01:17,799 --> 00:01:22,570
Thursday night that might be top so
maybe start early on that last part also

20
00:01:22,569 --> 00:01:25,618
homework 1 were in the process of
creating hopefully we'll have those back

21
00:01:25,618 --> 00:01:30,540
to this week you can get feedback before
homework to do also keep in mind though

22
00:01:30,540 --> 00:01:35,450
we actually have a in class midterm next
week on Wednesday so that's a week from

23
00:01:35,450 --> 00:01:41,159
Wednesday so be ready in class should be
a lot of fun

24
00:01:41,159 --> 00:01:46,359
alright so last lecture we were talking
about competition that works we can

25
00:01:46,358 --> 00:01:50,438
absolve the pieces we spent a long time
understanding how this convolution

26
00:01:50,438 --> 00:01:53,699
operator works how we're sort of
transforming feature maps from one to

27
00:01:53,700 --> 00:01:58,329
another by running into products over by
sliding this window over the map

28
00:01:58,328 --> 00:02:01,809
computing products and actually
transforming our representation through

29
00:02:01,810 --> 00:02:05,759
many layers of processing and if you
remember if you remember these lower

30
00:02:05,759 --> 00:02:09,299
layers of convolutions tent wherein
things like edges and colors and higher

31
00:02:09,299 --> 00:02:14,790
layers tend to learn more complex object
parts we talked about pulling which is

32
00:02:14,789 --> 00:02:18,509
used to some sample and downsize our
feature representations inside networks

33
00:02:18,509 --> 00:02:24,209
that's a common ingredient we saw we
also did case studies on particular

34
00:02:24,209 --> 00:02:27,479
content architectures you could see how
these things tend to get hooked up in

35
00:02:27,479 --> 00:02:31,568
practice so we talk about one at which
is something from 98 it's a little fiber

36
00:02:31,568 --> 00:02:35,189
content that was used four digit
recognition we talked about Alex not the

37
00:02:35,189 --> 00:02:38,949
kind of kicked off the big deep deep
learning boom in 2012 by winning image

38
00:02:38,949 --> 00:02:45,568
not come that we talked about ZF that
one image net classification in 2013 was

39
00:02:45,568 --> 00:02:51,108
pretty similar to Alex now and then we
saw that deeper is often better for

40
00:02:51,109 --> 00:02:55,709
classification we looked at Google Matt
and PGG that did really well in 2014

41
00:02:55,709 --> 00:03:00,609
competitions that were much much deeper
than Alex Natanz and a lot better and we

42
00:03:00,609 --> 00:03:05,430
also saw this new fancy crazy thing for
Microsoft called the ResNet that one

43
00:03:05,430 --> 00:03:10,909
just in december in 2015 with hundred
and fifty where architecture and as your

44
00:03:10,909 --> 00:03:14,579
caller just over the last couple years
these different architectures have been

45
00:03:14,579 --> 00:03:19,109
getting deeper and getting a lot better
but this is just for classification so

46
00:03:19,109 --> 00:03:23,980
now in this lecture we're going to talk
about localisation and detection which

47
00:03:23,979 --> 00:03:28,500
is actually another really big important
problem in computer vision and this idea

48
00:03:28,500 --> 00:03:32,699
of deeper networks doing better chance
that all kind of will revisit that a lot

49
00:03:32,699 --> 00:03:37,798
in these new attacks as well so so far
in the class we've really been talking

50
00:03:37,799 --> 00:03:42,639
about classification which is sort of
given an image we want to classify which

51
00:03:42,639 --> 00:03:47,049
are some number object categories it is
that's not nice basic problem in

52
00:03:47,049 --> 00:03:50,340
computer vision that we've using that
were using to understand comments and

53
00:03:50,340 --> 00:03:53,800
such but there's actually a lot of other
tasks that people were coming to

54
00:03:53,800 --> 00:03:59,350
so some of these are classification and
localisation now instead of just

55
00:03:59,349 --> 00:04:03,699
classifying an edge as well as some
category labels we also want to drop

56
00:04:03,699 --> 00:04:07,349
down box in the image to say where that
class occurs

57
00:04:07,349 --> 00:04:11,549
another problem people work on its
detection so here there's again some

58
00:04:11,550 --> 00:04:15,689
pics number of object categories but we
actually want to find all instances of

59
00:04:15,689 --> 00:04:20,238
those categories inside the image and
Dropbox around them another more recent

60
00:04:20,238 --> 00:04:24,189
task but people have started to work on
a bit as this crazy thing called instant

61
00:04:24,189 --> 00:04:27,490
segmentation where again you want you
have some pics number about two

62
00:04:27,490 --> 00:04:30,829
categories you want to find all
instances of those categories your image

63
00:04:30,829 --> 00:04:35,319
but instead of using a box you actually
want to draw little contour around and

64
00:04:35,319 --> 00:04:37,279
identify all the pixels

65
00:04:37,279 --> 00:04:41,549
belonging to each instance instance
segmentations kind of crazy so we're not

66
00:04:41,550 --> 00:04:44,710
going to talk about that today just
thought you should be aware of it and

67
00:04:44,709 --> 00:04:47,959
we're gonna really focus on this these
localisation and detection tasks today

68
00:04:47,959 --> 00:04:52,009
and the big difference between these is
the number of objects that were finding

69
00:04:52,009 --> 00:04:56,250
so and localisation there's sort of one
object or in general effects number of

70
00:04:56,250 --> 00:05:00,129
objects whereas in detection we might
have multiple objects or a variable

71
00:05:00,129 --> 00:05:04,000
number of objects and this seems like a
small difference but it'll turn out to

72
00:05:04,000 --> 00:05:05,360
actually make a big

73
00:05:05,360 --> 00:05:10,480
be pretty important for architectures so
we're gonna first talked about

74
00:05:10,480 --> 00:05:15,610
classification and localisation cuz its
kind of the simplest so just to recap

75
00:05:15,610 --> 00:05:16,389
what I just sad

76
00:05:16,389 --> 00:05:21,849
classification one image to a category
label localisation is image to a box and

77
00:05:21,850 --> 00:05:26,730
classification localisation means we're
gonna do both the same time just to give

78
00:05:26,730 --> 00:05:30,669
you an idea of the kinds of dance that
people use for this we talked we've

79
00:05:30,668 --> 00:05:33,849
talked about the image that
classification challenge image not also

80
00:05:33,850 --> 00:05:37,810
has run a classification + localisation
challenge so here

81
00:05:37,810 --> 00:05:42,269
similar to the classification task
there's a thousand classes and each

82
00:05:42,269 --> 00:05:46,319
training instance in those classes
actually has one class and several

83
00:05:46,319 --> 00:05:51,069
bounding boxes for that class inside the
image and now a test tinier algorithm

84
00:05:51,069 --> 00:05:55,709
organics bypasses where instead of your
guesses just being class labels it's a

85
00:05:55,709 --> 00:05:59,370
class label together with the bounding
box and to get it right you need to get

86
00:05:59,370 --> 00:06:03,288
the class label rights and the bounding
box rights we're getting a bounding box

87
00:06:03,288 --> 00:06:06,589
right just means you're close in some
thing called intersection of

88
00:06:06,589 --> 00:06:11,310
that you don't need to care about too
much right now so again you get it for

89
00:06:11,310 --> 00:06:15,259
image that at least you get it right if
you one of your 5 gases is correct and

90
00:06:15,259 --> 00:06:18,129
this is kind of the main dataset people
work on for classification +

91
00:06:18,129 --> 00:06:25,159
localisation so one really fundamental
paradigm it's really useful when

92
00:06:25,160 --> 00:06:28,700
thinking about localisation is this idea
of regression so I don't know if

93
00:06:28,699 --> 00:06:31,219
thinking back to a machine learning
class you kind of saw like

94
00:06:31,220 --> 00:06:36,160
classification and regression may be
with me regression or something fancier

95
00:06:36,160 --> 00:06:39,689
and when we're talking about
localisation it's really implies we can

96
00:06:39,689 --> 00:06:42,980
really just frame this as a regression
problem where we have an image that's

97
00:06:42,980 --> 00:06:46,700
coming in that image is going to go
through some some processing and/or

98
00:06:46,699 --> 00:06:49,990
eventually going to produce for
real-valued numbers that promote rise

99
00:06:49,990 --> 00:06:53,829
this box there's different
parameterizations people use common is

100
00:06:53,829 --> 00:06:57,759
XY coordinates of the upper left hand
corner and the width and height of the

101
00:06:57,759 --> 00:07:01,000
box but you'll see some other variants
as well but always four numbers for

102
00:07:01,000 --> 00:07:04,680
bounding box and then there's some
ground truth bounding box which again is

103
00:07:04,680 --> 00:07:08,810
just four numbers and now we have we can
compute a loss like maybe Euclidean

104
00:07:08,810 --> 00:07:12,699
losses a pretty pretty standard choice
between the numbers that we produced in

105
00:07:12,699 --> 00:07:16,339
the correct numbers and now we can just
turn this thing just like we did our

106
00:07:16,339 --> 00:07:20,489
classification networks where we sample
so many batch of data with some ground

107
00:07:20,490 --> 00:07:24,210
truth boxes we propagate forward
computer lost between our predictions

108
00:07:24,209 --> 00:07:29,359
and the correct predictions back
propagate and just update the network so

109
00:07:29,360 --> 00:07:33,250
this paradigm is is really easy that's
actually makes this localization task

110
00:07:33,250 --> 00:07:37,269
actually pretty easy to implement so
here's a really simple recipe for how

111
00:07:37,269 --> 00:07:41,289
you could implement classification +
localisation so first you just download

112
00:07:41,290 --> 00:07:44,370
some existing preteen model are you
train yourself if you're ambitious

113
00:07:44,370 --> 00:07:48,139
something like Alex Knight BGG Google
met all these things we talked about

114
00:07:48,139 --> 00:07:53,180
last lecture now we're going to take
those fully connected layers that were

115
00:07:53,180 --> 00:07:57,100
producing our class scores were gonna
set those aside for the moment and we're

116
00:07:57,100 --> 00:08:00,410
gonna attach a couple new fully
connected layers to some point in the

117
00:08:00,410 --> 00:08:04,840
network this will be called call this a
regression had but I mean it's basically

118
00:08:04,839 --> 00:08:08,119
the same thing as a couple fully
connected layers and then I'll puts some

119
00:08:08,120 --> 00:08:13,889
real valued numbers now we train this
thing just like we train our

120
00:08:13,889 --> 00:08:17,209
classification network the only
difference is that now instead of class

121
00:08:17,209 --> 00:08:18,359
wars

122
00:08:18,360 --> 00:08:24,550
and graduate classes we use Lt loss and
crown jewel boxes of Matt we train this

123
00:08:24,550 --> 00:08:28,918
network exactly the same way now it s
time we just use both heads to do

124
00:08:28,918 --> 00:08:32,218
classification and localisation we have
an image we've changed the

125
00:08:32,219 --> 00:08:36,700
classification has we train
delocalization heads we pass it through

126
00:08:36,700 --> 00:08:40,620
we get class course we get boxes and
when we're done like really that's all

127
00:08:40,620 --> 00:08:44,259
you need to do so this is kind of a
really nice simple recipe that you guys

128
00:08:44,259 --> 00:08:50,208
could use for classification +
localisation on your projects so other

129
00:08:50,208 --> 00:08:54,750
one slight detail with this approach
there's sort of two main ways that

130
00:08:54,750 --> 00:08:59,990
people do this regression task you could
imagine a class agnostic regresar or

131
00:08:59,990 --> 00:09:04,190
class-specific regresar you could
imagine that no matter what class I'm

132
00:09:04,190 --> 00:09:07,760
going to use the same architecture the
same weights in those fully connected

133
00:09:07,759 --> 00:09:11,600
layers to produce my bounding box that
would be in your sort of outputting

134
00:09:11,600 --> 00:09:15,379
always four numbers which are just the
box no matter the class I'm an

135
00:09:15,379 --> 00:09:19,139
alternative you'll see sometimes it's
class-specific regression we're now

136
00:09:19,139 --> 00:09:23,389
you're gonna put see times for numbers
that's sort of like one bounding box per

137
00:09:23,389 --> 00:09:27,569
class and different people have found
that sometimes these work better and

138
00:09:27,570 --> 00:09:31,269
different cases but it i mean
intuitively it kind of makes sense that

139
00:09:31,269 --> 00:09:35,470
something that the way you might think
about localizing a cat could be a little

140
00:09:35,470 --> 00:09:38,129
bit different than the way you localize
are trained so maybe you wanna have

141
00:09:38,129 --> 00:09:42,289
different parts of your network that are
responsible for those things but it's

142
00:09:42,289 --> 00:09:45,569
it's pretty easy venue just it changes
your back the way you came to Los a

143
00:09:45,570 --> 00:09:49,329
little bit you compute loss only using
the ground truth class

144
00:09:49,328 --> 00:09:52,809
the box for the ground truth class but
even that still basically the same idea

145
00:09:52,809 --> 00:09:57,750
and other design choice here is where
exactly you attach the regression had

146
00:09:57,750 --> 00:10:01,360
again this isn't too important different
people if you'll see different people do

147
00:10:01,360 --> 00:10:05,120
it in different ways some common choices
would be to attach it right after the

148
00:10:05,120 --> 00:10:09,948
fall of the last convolutional air just
sort of mean like you're really serious

149
00:10:09,948 --> 00:10:14,909
initializing new fully connected layers
will see things like over feet and BG

150
00:10:14,909 --> 00:10:18,909
localisation work this way another
common choice is to just attach your

151
00:10:18,909 --> 00:10:22,939
aggression had actually after the last
fully connected layers from the

152
00:10:22,940 --> 00:10:27,310
classification of work and you'll see
some other things like depots on our CNN

153
00:10:27,309 --> 00:10:31,099
kind of work in this labor but either
one works fine

154
00:10:31,100 --> 00:10:38,129
you could attach to just about anywhere
and do something so as an aside this is

155
00:10:38,129 --> 00:10:42,029
we can actually generalize this
framework to localizing more than one

156
00:10:42,029 --> 00:10:46,610
object so normally with this
classification localisation task that we

157
00:10:46,610 --> 00:10:50,440
sort of set up an image that we care
about producing exactly one object

158
00:10:50,440 --> 00:10:54,620
bounding box for the input image but in
some cases you might know ahead of time

159
00:10:54,620 --> 00:10:59,279
that you always want to localize some
fixed number of objects so here this is

160
00:10:59,279 --> 00:11:03,730
really easy to generalize now your
aggression had just outputs box for each

161
00:11:03,730 --> 00:11:07,039
of those objects that you care about and
again you train the network in the same

162
00:11:07,039 --> 00:11:12,839
way and this idea of actually localizing
multiple objects the same time is pretty

163
00:11:12,840 --> 00:11:16,790
general and pretty powerful so for
example this kind of approach has been

164
00:11:16,789 --> 00:11:21,559
used for human pose estimation so the
idea is we want to input a crime a

165
00:11:21,559 --> 00:11:25,299
close-up view of a person and anyone to
figure out what's the pose of that

166
00:11:25,299 --> 00:11:29,789
person so well people sort of generally
have a fixed number of joints like their

167
00:11:29,789 --> 00:11:34,370
breasts and their neck and their elbows
and that sort of stuff so we just know

168
00:11:34,370 --> 00:11:39,060
that we need to find all the joints so
we import our image we run it through a

169
00:11:39,059 --> 00:11:43,829
convolutional network and we regress xy
coordinates for each joint location and

170
00:11:43,830 --> 00:11:47,490
that gives us our action that actually
lets you predict a whole human pose

171
00:11:47,490 --> 00:11:52,409
using the sort of localisation framework
in this paper and there's a paper from

172
00:11:52,409 --> 00:11:55,819
Google from a year or two ago that does
this sort of approach that a couple

173
00:11:55,820 --> 00:11:59,740
other bells and whistles but the basic
idea was just regressing using a CNN to

174
00:11:59,740 --> 00:12:05,100
these joint sessions so overall this
idea of localisation and treating it as

175
00:12:05,100 --> 00:12:09,769
regression 46 number of objects is
really really simple so I know some of

176
00:12:09,769 --> 00:12:12,659
you guys on your projects have been
thinking about you want to actually run

177
00:12:12,659 --> 00:12:16,850
detection cause you want to understand
like any parts of your images or find

178
00:12:16,850 --> 00:12:21,290
parts inside the image and if you're
thinking of a project along those lines

179
00:12:21,289 --> 00:12:25,019
I really encourage you to think about
this localization framework instead that

180
00:12:25,019 --> 00:12:27,750
if there's actually a fixed number of
objects that you know you want to

181
00:12:27,750 --> 00:12:31,929
localize and every image you should try
to frame it as a localization problem

182
00:12:31,929 --> 00:12:38,129
that's tends to be a lot easier to setup
alright so actually the simple idea of

183
00:12:38,129 --> 00:12:42,019
localisation via regression actually is
really simple it'll actually work I

184
00:12:42,019 --> 00:12:44,120
would really encourage you to try it for
projects

185
00:12:44,120 --> 00:12:47,330
but if you wanna win competitions like
image that you need to add a little bit

186
00:12:47,330 --> 00:12:52,330
of other fancy stuff so another thing
that people do for localisation is this

187
00:12:52,330 --> 00:12:56,410
idea of sliding window so we'll step
through this in more detail but the idea

188
00:12:56,409 --> 00:13:00,809
is that you still have your
classification localisation two-headed

189
00:13:00,809 --> 00:13:04,929
network but you're actually gonna run it
not once on the image but at multiple

190
00:13:04,929 --> 00:13:08,269
positions on the image and you're gonna
aggregated across those different

191
00:13:08,269 --> 00:13:13,100
positions and you can actually do this
in an efficient way so it took sort of

192
00:13:13,100 --> 00:13:17,290
see more concretely how how this sliding
window localisation works we're gonna

193
00:13:17,289 --> 00:13:21,980
look at the over-the-air architecture so
over feat was actually the winner of the

194
00:13:21,980 --> 00:13:25,399
image that localisation challenge in
2013

195
00:13:25,399 --> 00:13:29,730
it this this architect this this sort of
setup looks basically like what we saw a

196
00:13:29,730 --> 00:13:33,839
couple nights ago we have an Alex not at
the beginning then we have a

197
00:13:33,839 --> 00:13:37,820
classification had to have a regression
had classification head is spinning out

198
00:13:37,820 --> 00:13:38,740
class for us

199
00:13:38,740 --> 00:13:44,450
regression had a speeding up the boxes
and this thing because it's in Alex nat

200
00:13:44,450 --> 00:13:51,120
type of architecture is expecting an
input of 221 221 but actually we can run

201
00:13:51,120 --> 00:13:55,679
this on larger images and this can help
sometimes so suppose we have a large

202
00:13:55,679 --> 00:14:02,799
larger image of what and when I say 257
by 257 now we could imagine taking our

203
00:14:02,799 --> 00:14:06,659
classification + localisation network
and running at just on the upper corner

204
00:14:06,659 --> 00:14:11,799
of this image and that'll give us some
some class score and also some summer

205
00:14:11,799 --> 00:14:15,979
grass bounding box and we're gonna
repeat this take our same classification

206
00:14:15,980 --> 00:14:21,820
+ localisation network and run it on all
four corners of this image and after

207
00:14:21,820 --> 00:14:26,230
doing so will end up with for grass
bounding boxes one from each of those

208
00:14:26,230 --> 00:14:30,509
four locations together with a class
classification score for each location

209
00:14:30,509 --> 00:14:35,700
but we actually want just a single
bounding box so then they use some

210
00:14:35,700 --> 00:14:39,770
heuristics to Mercedes bounding boxes in
scores and that puts a little bit ugly I

211
00:14:39,769 --> 00:14:42,809
don't wanna go into the details here
they have it in the paper but the idea

212
00:14:42,809 --> 00:14:46,699
is that public combining aggregating
these boxes across multiple locations

213
00:14:46,700 --> 00:14:50,959
can help that can help the model sort of
credits on airs and this tends to work

214
00:14:50,958 --> 00:14:55,058
really well and that mean that won them
the challenge that year

215
00:14:55,058 --> 00:14:58,149
but in practice they actually use many
more than four locations

216
00:14:58,149 --> 00:15:08,989
oh ya ought to be fully with them

217
00:15:08,989 --> 00:15:12,939
well I mean it's actually good point so
once you're doing regression you're just

218
00:15:12,938 --> 00:15:15,498
predicting for numbers you couldn't
crack you couldn't be reproduced

219
00:15:15,499 --> 00:15:20,149
anywhere it doesn't have to be inside
the image although I know that brings up

220
00:15:20,149 --> 00:15:23,698
a good point when you're doing this
especially when they when you're

221
00:15:23,698 --> 00:15:27,088
training this network in this sliding
window way you actually to ship the

222
00:15:27,089 --> 00:15:30,429
ground truth box in a little bit ship
ship the coordinate frame for those

223
00:15:30,428 --> 00:15:35,999
different slices that's kind of an ugly
details just worried about ya but in

224
00:15:35,999 --> 00:15:39,428
practice they use many more than four
image locations and they actually do

225
00:15:39,428 --> 00:15:43,629
multiple scales as well as you can see
this is actually figure from their paper

226
00:15:43,629 --> 00:15:47,129
I'm a left you see all the different
positions where they kind of evaluated

227
00:15:47,129 --> 00:15:52,058
this network in the middle you see those
output progressed boxes one for each of

228
00:15:52,058 --> 00:15:55,678
those positions on the bottom easy to
score map for each of those positions

229
00:15:55,678 --> 00:16:00,139
and then I mean they're pretty noisy but
it's kinda convert their kind of

230
00:16:00,139 --> 00:16:03,899
generally over the bear so they'd run
this fancy aggregation method and they

231
00:16:03,899 --> 00:16:07,839
get a final box for the bear and they
decide that the same as a pair and they

232
00:16:07,839 --> 00:16:12,869
actually won the challenge with this but
one problem you might anticipate is it

233
00:16:12,869 --> 00:16:15,759
could be pretty expensive to actually
run the network on every one of those

234
00:16:15,759 --> 00:16:20,259
crops but there's actually more
efficient with thing we could do so we

235
00:16:20,259 --> 00:16:23,489
normally think of these networks as
having convolutional errors and then

236
00:16:23,489 --> 00:16:26,048
fully connected Lares but when you think
about it

237
00:16:26,048 --> 00:16:31,108
a fully connected larry is just 4096
numbers right it's just a factor but

238
00:16:31,109 --> 00:16:34,679
instead of thinking of it as a vector we
could think of it as just another

239
00:16:34,678 --> 00:16:39,269
convolutional feature map is kinda crazy
we just transpose that added to

240
00:16:39,269 --> 00:16:45,019
one-by-one dimensions so now the idea is
that we can now treat our car fully

241
00:16:45,019 --> 00:16:49,499
connected layers and convert them into
convolutional there's a few imagined in

242
00:16:49,499 --> 00:16:54,339
our fully connected network we had this
convolutional feature map and we had one

243
00:16:54,339 --> 00:16:57,749
way from each element of that
competition will feature map to produce

244
00:16:57,749 --> 00:17:02,048
each element of our 4096 dimensional
vector but we instead of thinking about

245
00:17:02,048 --> 00:17:06,288
reshaping and having a fine layer that's
sort of equivalent to just having a five

246
00:17:06,288 --> 00:17:06,970
by five

247
00:17:06,970 --> 00:17:10,120
solution it's a little bit weird but if
you think about it it should make sense

248
00:17:10,119 --> 00:17:16,318
eventually but alright so then we take
this fully connected later turns into a

249
00:17:16,318 --> 00:17:21,899
five by five convolution than this than
we previously had another fully

250
00:17:21,900 --> 00:17:26,409
connected mayor going from 4096 4096
this is actually a one-by-one

251
00:17:26,409 --> 00:17:30,570
convolution right that's that's kinda
weird but if you if you think hard and

252
00:17:30,569 --> 00:17:35,369
work out the math on paper and go send a
quiet room you'll figure it out and so

253
00:17:35,369 --> 00:17:38,769
we basically can't earn each of these
fully connected layers and our network

254
00:17:38,769 --> 00:17:43,509
into a convolutional air and now now
this is pretty cool because now our

255
00:17:43,509 --> 00:17:47,589
network is composed entirely of just
contributions and pooling and elements

256
00:17:47,589 --> 00:17:51,819
operations so now we can actually run
the network on images of different sizes

257
00:17:51,819 --> 00:17:56,889
and this sort of will give us very
cheaply equip the equivalent of

258
00:17:56,890 --> 00:18:01,840
operating but not work independently on
different locations so to kind of see

259
00:18:01,839 --> 00:18:02,609
how that works

260
00:18:02,609 --> 00:18:07,219
you imagine a training time you may be
working over 14 by 14 template you run

261
00:18:07,220 --> 00:18:11,960
some convolutions and then here are are
fully connected layers that we're now

262
00:18:11,960 --> 00:18:17,140
re-imagining as convolutional Ayers said
and we have this by by five con block

263
00:18:17,140 --> 00:18:22,600
that gets turned into these one-by-one
specially sized elements so we've sort

264
00:18:22,599 --> 00:18:26,449
of eliminating not showing the depth
dimension here but these like this one

265
00:18:26,450 --> 00:18:30,900
by one would be one by one by 4096
rights or just converting these layers

266
00:18:30,900 --> 00:18:35,259
into a convolutional there's now that we
know that their convolutions we could

267
00:18:35,259 --> 00:18:39,700
actually run on in part of a larger size
and you can see that now we've got we've

268
00:18:39,700 --> 00:18:43,558
added a couple extra pixels and now we
actually run all these things the

269
00:18:43,558 --> 00:18:47,869
convolutions and get a two-by-two output
but what's really cool here is that

270
00:18:47,869 --> 00:18:52,058
we're able to share computation to make
this really efficient so now our output

271
00:18:52,058 --> 00:18:56,428
is four times as big but we've done much
less than four times the compute cuz if

272
00:18:56,429 --> 00:19:00,360
you think about the difference between
where we're doing computation here the

273
00:19:00,359 --> 00:19:04,449
only extra computation happened in these
yellow parts so now we're actually very

274
00:19:04,450 --> 00:19:08,610
efficiently evaluating the network at
many many different positions without

275
00:19:08,609 --> 00:19:11,918
actually spending much computation so
this is how they're able to evaluate

276
00:19:11,919 --> 00:19:15,240
that network in that very very dense
multiscale way that you saw a couple

277
00:19:15,240 --> 00:19:19,388
nights ago that make sense any questions
on this

278
00:19:19,388 --> 00:19:25,558
ok writes actually we can look at the
classification + localisation results on

279
00:19:25,558 --> 00:19:30,858
a mission over the last couple of years
so in 2012 Alex Alex Kozinski Jack

280
00:19:30,858 --> 00:19:36,358
Hinton they won not only classification
but also localisation but I wasn't able

281
00:19:36,358 --> 00:19:40,978
to find any published details of exactly
how they did that in 2013 was the

282
00:19:40,979 --> 00:19:45,249
over-the-top that we just saw actually
improved on Alex's results a little bit

283
00:19:45,249 --> 00:19:50,429
the year after we talked about VGG and
they're sort of really deep 19 their

284
00:19:50,429 --> 00:19:54,009
network they got second place on
classification but actually 1 I'm

285
00:19:54,009 --> 00:19:59,139
localisation and the BGG actually used
basically exactly the same strategy that

286
00:19:59,138 --> 00:20:03,918
over feat dead they just use the deeper
network and actually interesting the BGG

287
00:20:03,919 --> 00:20:08,288
used fewer scales they stand pat network
out in fewer places and used fewer

288
00:20:08,288 --> 00:20:12,878
skills but they actually decrease the
era quite a bit so basically the only

289
00:20:12,878 --> 00:20:17,868
difference being over feet and BG here
is that BGU the deeper network so here

290
00:20:17,868 --> 00:20:20,858
we could see that these really powerful
image features actually improve the

291
00:20:20,858 --> 00:20:24,098
localization performance quite a bit
with enough to change the localisation

292
00:20:24,098 --> 00:20:28,418
architecture at all we just swapped out
about her CNN and it improved results a

293
00:20:28,419 --> 00:20:34,169
lot and then this year in 2015 Microsoft
swept everything as that'll be a theme

294
00:20:34,169 --> 00:20:39,239
in this lecture as well this this
hundred fifty lair ResNet from Microsoft

295
00:20:39,239 --> 00:20:43,629
crushed localisation here and drunk
proper performance from 25 all the way

296
00:20:43,628 --> 00:20:48,738
down to nine but I mean this this is a
little bit and this is talk to really

297
00:20:48,739 --> 00:20:52,798
isolate the deep features so yes they
did have deeper features but Microsoft

298
00:20:52,798 --> 00:20:56,398
actually it's a different localization
method called rpms region proposal

299
00:20:56,398 --> 00:21:00,699
networks so it's not really clear
whether this which part whether it's a

300
00:21:00,700 --> 00:21:04,929
better localization strategy or whether
the better features but at any rate they

301
00:21:04,929 --> 00:21:10,139
did really well that's pretty much all I
want to say about classification

302
00:21:10,138 --> 00:21:13,848
localisation just consider doing it for
projects and if there's any questions

303
00:21:13,848 --> 00:21:19,509
about this task we should talk about
that now before moving on ya

304
00:21:19,509 --> 00:21:32,890
performance especially with a loss right
so then I'll two losses when having

305
00:21:32,890 --> 00:21:37,050
outliers is actually really bad so
sometimes people don't use an L to loss

306
00:21:37,049 --> 00:21:40,609
instead you can try and sell one loss
that can help with outliers a little bit

307
00:21:40,609 --> 00:21:45,279
people also will do sometimes a smooth
one loss where it looks like he'll one

308
00:21:45,279 --> 00:21:49,339
sort of a tales but then near zero it'll
be quadratic so actually swapping out

309
00:21:49,339 --> 00:21:53,319
that regression loss function can help a
bit with outliers sometimes but also if

310
00:21:53,319 --> 00:21:56,399
you have a little bit of noise sometimes
hopefully you're not just figured out

311
00:21:56,400 --> 00:22:14,380
cross your fingers don't think too hard
questions questions so people do both

312
00:22:14,380 --> 00:22:18,560
actually I'm so over feet actually I
don't remember I don't remember exactly

313
00:22:18,559 --> 00:22:23,409
which oversee dead but BGG actually
backdrops into the entire network so

314
00:22:23,410 --> 00:22:27,230
it'll be it'll be faster to just
actually work fine if you just trained

315
00:22:27,230 --> 00:22:30,289
in the regression had but you'll tend to
get a little bit better results if you

316
00:22:30,289 --> 00:22:34,049
back drop into the home network and BG
did this experiment and they got maybe

317
00:22:34,049 --> 00:22:37,659
one or two points extra buyback dropping
through the whole thing but it at the

318
00:22:37,660 --> 00:22:41,320
expense of a lot more competition and
training time so it so I would I would

319
00:22:41,319 --> 00:22:44,769
say it like as a first thing don't just
talk tried not back dropping and the

320
00:22:44,769 --> 00:22:50,440
network

321
00:22:50,440 --> 00:22:57,110
generally not right because your testing
on the same classes that you saw

322
00:22:57,109 --> 00:23:00,839
training time you're gonna see different
instances obviously but I mean you're

323
00:23:00,839 --> 00:23:04,759
still bears a tough time in OC bears at
training time we're not expecting you to

324
00:23:04,759 --> 00:23:07,370
generalize across classes I'll be pretty
hard

325
00:23:07,369 --> 00:23:20,638
yea good question yes so sometimes
people will do that they'll train with

326
00:23:20,638 --> 00:23:24,349
both simultaneously also sometimes
people will just end up with separate

327
00:23:24,349 --> 00:23:27,089
networks one that sort of only
responsible for aggression when it's

328
00:23:27,089 --> 00:23:38,089
only responsible for classification
those both work well glad you asked

329
00:23:38,089 --> 00:23:40,558
that's that's actually the next thing
we're gonna talk about that's that's a

330
00:23:40,558 --> 00:23:50,740
different task of object detection so

331
00:23:50,740 --> 00:23:56,808
well yeah well so I mean it kinda
depends on the training strategy if

332
00:23:56,808 --> 00:23:59,920
you're like if you also kind of goes
back to this idea of class agnostic

333
00:23:59,920 --> 00:24:03,610
first-class Pacific regression class
agnostic regression it doesn't matter

334
00:24:03,609 --> 00:24:06,889
you just regress to the boxes tomorrow
the class class specific you're sort of

335
00:24:06,890 --> 00:24:13,950
training separate aggressors for each
class right let's talk about object

336
00:24:13,950 --> 00:24:19,220
detection so object detection is is much
fancier much cooler but also a lot

337
00:24:19,220 --> 00:24:22,890
harrier so the idea is that again we
have an input image we have some sort of

338
00:24:22,890 --> 00:24:26,660
classes we want to find all instances of
those classes in that in that input

339
00:24:26,660 --> 00:24:31,670
image so I mean you know regression
worked pretty well for localisation why

340
00:24:31,670 --> 00:24:37,470
don't we try it for for detection to
mark as an SMS we have these these dogs

341
00:24:37,470 --> 00:24:41,429
and cats and we have four things we have
16 numbers thats looks like that looks

342
00:24:41,429 --> 00:24:46,250
like regression rate image in numbers
out but if we look at another image then

343
00:24:46,250 --> 00:24:50,609
you know this one only has two things
coming out so it has eight numbers they

344
00:24:50,609 --> 00:24:54,589
look at this one there's a whole bunch
of cats we need a bunch of numbers so I

345
00:24:54,589 --> 00:24:57,519
mean it's it's kind of hard to treat
detection a straight-up regression

346
00:24:57,519 --> 00:25:01,450
because we have this problem of variable
size outputs so we're gonna have to do

347
00:25:01,450 --> 00:25:04,460
something fancier although actually
there is a method will talk about later

348
00:25:04,460 --> 00:25:09,539
that sort of does this anyway and does
treated as as regression but we'll get

349
00:25:09,539 --> 00:25:12,950
to that we'll get to that later but in
general you wanna not treat this as

350
00:25:12,950 --> 00:25:18,360
regression because you have very precise
outputs so we're really easy problem

351
00:25:18,359 --> 00:25:22,779
really easy way to solve this is to
think of detection not as regression but

352
00:25:22,779 --> 00:25:25,960
as classification right in machine
learning regression and classification

353
00:25:25,960 --> 00:25:29,929
are your two hammers you just want to
use those to eat all your problems right

354
00:25:29,929 --> 00:25:34,250
so we regression and works will do
classification instead we know how to

355
00:25:34,250 --> 00:25:38,558
classify image regions we just for CNN
right we're going to do is we're gonna

356
00:25:38,558 --> 00:25:43,349
take many of these input regions of the
image of a classifier there and say like

357
00:25:43,349 --> 00:25:46,129
alright this region of the alleged
attack at No

358
00:25:46,130 --> 00:25:50,770
as a dog know that over a little bit we
found a cat that's great but over a

359
00:25:50,769 --> 00:25:54,460
little bit that's that's not anything so
then we can actually just try out a

360
00:25:54,460 --> 00:25:58,558
whole bunch different image regions run
a classifier in each one and this will

361
00:25:58,558 --> 00:26:02,490
basically solve our variable size output
problem

362
00:26:02,490 --> 00:26:11,160
so there's there's no question so the
question of how decide how to decide

363
00:26:11,160 --> 00:26:14,558
what the window size the answer is we
just tried them all right just literally

364
00:26:14,558 --> 00:26:18,879
tried them all so that's that's that's
actually a big problem right because we

365
00:26:18,880 --> 00:26:21,910
need to try Windows of different sizes
of different positions of different

366
00:26:21,910 --> 00:26:25,290
scales me do this properly test and this
is going to be really expensive right

367
00:26:25,289 --> 00:26:39,089
there's a whole lot of places we need to
look yeah also when you're doing this

368
00:26:39,089 --> 00:26:45,058
you add an extra two things one you can
add an extra class to say background and

369
00:26:45,058 --> 00:26:49,569
say like oh there's nothing here another
thing you can do is not is to actually

370
00:26:49,569 --> 00:26:54,159
multi-label classification you cannot
put multiple positive things right

371
00:26:54,160 --> 00:26:56,950
that's actually pretty easy to do and
just instead of a soft max loss you have

372
00:26:56,950 --> 00:27:01,390
independent regression loss of
independent logistic regression class so

373
00:27:01,390 --> 00:27:05,100
I can actually let you say yes I
multiple classes at one point but that's

374
00:27:05,099 --> 00:27:10,189
just walking on a loss function so
that's that's pretty easy to do right so

375
00:27:10,190 --> 00:27:13,220
actually like what we see a problem with
this approach is that there's a whole

376
00:27:13,220 --> 00:27:17,690
bunch of different positions we need to
evaluate the solution sort of a couple

377
00:27:17,690 --> 00:27:21,308
you as a couple of years ago was just
usually class fat used really fast

378
00:27:21,308 --> 00:27:26,299
classifiers try them all so actually
detection is this really all problem in

379
00:27:26,299 --> 00:27:29,119
computer vision so you should probably
have a little bit more historical

380
00:27:29,119 --> 00:27:34,109
perspective so starting in about 2005
there was this really successful

381
00:27:34,109 --> 00:27:38,490
approach to it but I'm really successful
detection that use this feature

382
00:27:38,490 --> 00:27:42,039
representation called histograms of
Oriented radiance so if you are call

383
00:27:42,039 --> 00:27:46,609
back to homework 1 you actually use this
feature on the last part to actually do

384
00:27:46,609 --> 00:27:50,979
classification as well so this was
actually a sort of the best feature that

385
00:27:50,980 --> 00:27:55,670
we had in computer vision Sircar in 2005
the idea is we're just gonna do linear

386
00:27:55,670 --> 00:27:59,550
classifiers on top of this feature and
that's going to be our our classifier so

387
00:27:59,549 --> 00:28:03,460
linear classifiers are really fast so if
this works is that we compute are

388
00:28:03,460 --> 00:28:08,250
oriented gradient feature for the whole
image at multiple scales and we run this

389
00:28:08,250 --> 00:28:12,660
linear classifier at every scale every
position just do it really fast just do

390
00:28:12,660 --> 00:28:13,210
it everywhere

391
00:28:13,210 --> 00:28:15,329
classifier and its past to evaluate

392
00:28:15,329 --> 00:28:21,029
and this worked really well in 2005 sort
of people took this idea and worked on

393
00:28:21,029 --> 00:28:25,029
it a little bit more in the next couple
of years so sort of the one of the most

394
00:28:25,029 --> 00:28:29,879
important detection paradigms 3d
planning is this thing called deep but

395
00:28:29,880 --> 00:28:34,470
deformable parts model so I don't wanna
go too much into the details are best

396
00:28:34,470 --> 00:28:39,309
but I mean the basic idea is that we're
still working on these history memorial

397
00:28:39,309 --> 00:28:42,619
gradient features but now our model
rather than just being a linear

398
00:28:42,619 --> 00:28:46,659
classifier we have this linear click
this linear sort of template for the

399
00:28:46,660 --> 00:28:51,370
object and we also have these templates
for parts that are allowed to sort of

400
00:28:51,369 --> 00:28:57,119
very over spatial positions and deform a
little bit and they have some some fancy

401
00:28:57,119 --> 00:29:01,939
fancy think about late in the AM to
learn these things and really fancy

402
00:29:01,940 --> 00:29:07,190
dynamic programming algorithms actually
evaluate this thing really fast test

403
00:29:07,190 --> 00:29:11,100
time is actually kind of fun if you
enjoy our thumbs this this thing at this

404
00:29:11,099 --> 00:29:16,119
part is kind of fun to think about but
the end result is that it's it's a much

405
00:29:16,119 --> 00:29:19,209
more powerful classifier that allows a
little bit of deformability in your

406
00:29:19,210 --> 00:29:23,079
model and you can still about weight
really fast so we're still just going to

407
00:29:23,079 --> 00:29:26,490
evaluate it everywhere every scale every
position every aspect ratio just do it

408
00:29:26,490 --> 00:29:33,039
everywhere its past and this actually
worked really well in 2010 around there

409
00:29:33,039 --> 00:29:37,619
that was sort of state of the art and
detection for many problems at a time so

410
00:29:37,619 --> 00:29:40,509
this is I don't spend too much time on
this but there was a really cool paper

411
00:29:40,509 --> 00:29:45,049
last year that argued that these dpi
models are actually just a certain type

412
00:29:45,049 --> 00:29:47,480
of content right and so right

413
00:29:47,480 --> 00:29:51,329
these these history I'm going crazy ants
are like little edges we can just look

414
00:29:51,329 --> 00:29:55,539
on delusions and history and was kinda
like pooling that sort of thing so if

415
00:29:55,539 --> 00:30:00,349
you're interested check out this paper
it's kind of fun to think about right

416
00:30:00,349 --> 00:30:02,250
but we really want to work on

417
00:30:02,250 --> 00:30:06,259
make this thing work on classifiers that
are not fast without weights like maybe

418
00:30:06,259 --> 00:30:11,809
a CNN so here this week this problem is
still hard right we have many different

419
00:30:11,809 --> 00:30:14,940
positions you want to try when we
probably can't actually afford to try

420
00:30:14,940 --> 00:30:19,220
them all so the solution is that we
don't try them all we have some other

421
00:30:19,220 --> 00:30:23,380
things that sort of guesses where we
want to look and then we only apply our

422
00:30:23,380 --> 00:30:28,720
expense of classifier at those smaller
number of locations so that idea

423
00:30:28,720 --> 00:30:35,419
is called region proposals so we on our
region proposal method is this thing

424
00:30:35,419 --> 00:30:39,900
that takes in an image and then outputs
a whole bunch of regions where maybe

425
00:30:39,900 --> 00:30:45,280
possibly an object might be located so
one way you can think about region

426
00:30:45,279 --> 00:30:48,428
proposals is that they're kinda like a
really fast

427
00:30:48,429 --> 00:30:53,038
class agnostic object detector right
they don't care about the class they're

428
00:30:53,038 --> 00:30:56,038
not very accurate but they're pretty
fast to run and they give us a whole

429
00:30:56,038 --> 00:31:00,769
bunch of boxes and the general intuition
behind behind these region proposal

430
00:31:00,769 --> 00:31:04,639
methods is that they're kinda looking
for blob like structure is an image rate

431
00:31:04,640 --> 00:31:09,740
so like objects are generally the dog i
mean if you can ask when it looks kinda

432
00:31:09,740 --> 00:31:13,940
like a white blob the cat looks like a
white blobs flowers I kinda blah be the

433
00:31:13,940 --> 00:31:17,929
eyes and nose are kinda blah be so
anyone these region proposal methods a

434
00:31:17,929 --> 00:31:21,650
lot of times what you'll see is the kind
of put boxes around a lot of these

435
00:31:21,650 --> 00:31:27,820
blobby regions in the image so probably
the most famous region proposal method

436
00:31:27,819 --> 00:31:31,538
is called selective search you don't
really need to know exact into much

437
00:31:31,538 --> 00:31:36,980
detail how this works but the idea is
that you start from your pixels and you

438
00:31:36,980 --> 00:31:40,919
kind of merger adjacent pixels together
if they have similar color and texture

439
00:31:40,919 --> 00:31:45,770
and form these are connected reid
disconnected blob like regions and then

440
00:31:45,769 --> 00:31:50,740
you merge yuppies blob like regions to
get bigger and bigger body parts and

441
00:31:50,740 --> 00:31:53,829
then for each of these different scales
you could actually convert each of these

442
00:31:53,829 --> 00:31:58,710
Bobby regions into a box by just drawing
a box around it so then by doing this

443
00:31:58,710 --> 00:32:02,548
over multiple scales you end up with a
whole bunch of boxes around sort of a

444
00:32:02,548 --> 00:32:06,359
lot of blobby stuff in the image and its
are reasonably fast to compute and

445
00:32:06,359 --> 00:32:11,500
actually cuts down the search space
quite a lot but selectively certainly

446
00:32:11,500 --> 00:32:14,720
isn't the only game in town is just may
be the most famous there's a whole bunch

447
00:32:14,720 --> 00:32:18,319
of different region proposal methods
that people have developed there was

448
00:32:18,319 --> 00:32:21,509
this paper last year that actually did a
really cool thorough scientific

449
00:32:21,509 --> 00:32:25,890
evaluation of all these different region
proposal methods and sort of gave you

450
00:32:25,890 --> 00:32:29,950
the pros and the cons of each and all
that kind of stuff but I mean my

451
00:32:29,950 --> 00:32:33,620
takeaway from this paper was just use
that boxes if you had to pick one so

452
00:32:33,619 --> 00:32:37,459
it's it's it's really fast it you can
run it in the bottom third of a second

453
00:32:37,460 --> 00:32:40,950
per image compared to about 10 seconds
for selective search

454
00:32:40,950 --> 00:32:49,000
but more stars is better and it gets a
lot of stars so it's going right so now

455
00:32:49,000 --> 00:32:51,970
that we have this idea region proposals
and we have this idea of a CNN

456
00:32:51,970 --> 00:32:56,679
classifier let's just put everything
altogether so that's and so this this

457
00:32:56,679 --> 00:33:02,830
idea was sort of first put together in a
really nice way in 2014 in this method

458
00:33:02,829 --> 00:33:08,740
called RCN on the idea is it's a
region-based CNN method so it's it's

459
00:33:08,740 --> 00:33:12,179
it's pretty simple man what we've seen
all the pieces we have an input image

460
00:33:12,179 --> 00:33:17,028
we're gonna run a region proposal method
like selective search to get about maybe

461
00:33:17,028 --> 00:33:21,929
two thousand boxes of different scales
and positions mean 2000 still a lot but

462
00:33:21,929 --> 00:33:26,380
it's a lot less than all possible boxes
in the image now for each of those boxes

463
00:33:26,380 --> 00:33:31,510
we're gonna have cropped and warp that
image region to some fixed size and then

464
00:33:31,509 --> 00:33:35,898
run it former through I CNN to classify
and then this CNN is going to have a

465
00:33:35,898 --> 00:33:41,199
regression head and the regression had
here and a classification had been used

466
00:33:41,200 --> 00:33:46,259
as PM's here so the idea is that this
this regression had can sort of correct

467
00:33:46,259 --> 00:33:50,369
for region proposals that were a little
bit off writes this this actually works

468
00:33:50,369 --> 00:33:55,219
really well it's really simple yeah it's
pretty cool but unfortunately so

469
00:33:55,220 --> 00:33:59,460
unfortunately the training pipeline is a
little bit complicated so the way that

470
00:33:59,460 --> 00:34:03,788
you end up train training src and a
model is you know like many like many

471
00:34:03,788 --> 00:34:06,970
models you first start by downloading a
model from the internet that works well

472
00:34:06,970 --> 00:34:13,240
for classification originally they were
using and how it's not then then next we

473
00:34:13,239 --> 00:34:16,868
actually want to fine tune this model
for detection rate because this this

474
00:34:16,869 --> 00:34:20,780
classification model was probably
trained on image that 4,000 classes but

475
00:34:20,780 --> 00:34:24,019
your detection dataset has a different
number of classes in the image that

476
00:34:24,019 --> 00:34:28,398
extra little bit different so you still
run this you still train this network

477
00:34:28,398 --> 00:34:29,679
for classification

478
00:34:29,679 --> 00:34:33,429
you have to add a couple new layers at
the end to deal with your classes and to

479
00:34:33,429 --> 00:34:38,068
help you deal with slightly different
statistics of your image data so here

480
00:34:38,068 --> 00:34:41,579
you're just doing classification still
but you're not running on hold images

481
00:34:41,579 --> 00:34:44,869
you're running out on positive and
negative regions of your images from

482
00:34:44,869 --> 00:34:49,950
your detection dataset right so you
initially as a new layer and you and you

483
00:34:49,949 --> 00:34:53,599
train this thing again for your day is
that

484
00:34:53,599 --> 00:34:57,889
next we actually want to casualties
features two desks so for every

485
00:34:57,889 --> 00:35:02,230
engineered in your data that you run
selective search you run that image you

486
00:35:02,230 --> 00:35:07,079
extract those regions you were down here
on the CNN and you cash those features

487
00:35:07,079 --> 00:35:12,319
to desk and something important for this
step is to have a large hard drive the

488
00:35:12,320 --> 00:35:16,289
passcode they decided not too big maybe
order a couple tens of thousands of

489
00:35:16,289 --> 00:35:20,170
images but extracting these features
actually takes hundreds of gigabytes so

490
00:35:20,170 --> 00:35:26,869
that's not so great and then next we
have this we want to train RSP arms to

491
00:35:26,869 --> 00:35:30,909
actually be able to classify different
are different classes based on these

492
00:35:30,909 --> 00:35:35,649
features so here we want to run a bunch
of we want to change a bunch of

493
00:35:35,650 --> 00:35:40,760
different binary as PM's to classify
image regions as to whether or not they

494
00:35:40,760 --> 00:35:45,220
contain or don't contain that that one
object to this goes back to a question a

495
00:35:45,219 --> 00:35:49,029
little bit ago that sometimes you
actually might wanna how one region have

496
00:35:49,030 --> 00:35:53,460
multiple positive be able to output YES
on multiple classes for the same image

497
00:35:53,460 --> 00:35:56,889
region and one way that they do that is
just my training separate binary SVM

498
00:35:56,889 --> 00:36:01,579
speech class right so then this is sort
of an offline process they just used the

499
00:36:01,579 --> 00:36:08,230
best p.m. so you have these features
these are maybe those are positive

500
00:36:08,230 --> 00:36:11,820
samples for a countess yeah it doesn't
make any sense right but you get the

501
00:36:11,820 --> 00:36:14,700
idea rate you have these different
imagery you have these different image

502
00:36:14,699 --> 00:36:18,599
regions you have these features that you
save to disk for those regions and then

503
00:36:18,599 --> 00:36:22,029
you divide them into positive and
negative samples for each for each class

504
00:36:22,030 --> 00:36:27,269
and you just train these these binary
SVM you do this you do this the same

505
00:36:27,269 --> 00:36:33,239
thing for dog and you just do this for
every class near to decide right now

506
00:36:33,239 --> 00:36:37,029
there's another stop right if so then
there's this idea of Cox regression so

507
00:36:37,030 --> 00:36:40,450
sometimes you region proposals aren't
perfect so what we actually want to do

508
00:36:40,449 --> 00:36:45,549
is be able to regress from from his cast
features to a correction on to the

509
00:36:45,550 --> 00:36:50,269
region proposal and that correction has
this kind of funny premise rise

510
00:36:50,269 --> 00:36:54,320
normalize representation the country
details about in the paper but kind of

511
00:36:54,320 --> 00:36:58,300
intuition is that maybe for this for
this for this region proposal it was

512
00:36:58,300 --> 00:37:02,030
pretty good we don't really need to make
any any corrections but maybe this one

513
00:37:02,030 --> 00:37:06,250
in the middle that proposal was too far
to the left it should be like the crib

514
00:37:06,250 --> 00:37:09,510
cracked ground truth as a little bit to
the right we want to regress to this

515
00:37:09,510 --> 00:37:12,530
correction factor that actually tell us
that we need to shift a little bit to

516
00:37:12,530 --> 00:37:15,780
the right or maybe this guy is a little
bit too wide

517
00:37:15,780 --> 00:37:19,100
they didn't lose too much of the stuff
outside the cat so we want to regress to

518
00:37:19,099 --> 00:37:21,880
this correction factor that tells us we
need to shrink

519
00:37:21,880 --> 00:37:26,539
region proposal a little bit so again
this is just let me just do linear

520
00:37:26,539 --> 00:37:30,340
regression which you can but you know
from 229 you have these these features

521
00:37:30,340 --> 00:37:35,490
you have these targets you you just ran
linear regression i SAT so before we

522
00:37:35,489 --> 00:37:39,219
look at the results we should talk to
talk a little bit about the different

523
00:37:39,219 --> 00:37:42,769
datasets the people used for detection
there's kind of three that you'll see in

524
00:37:42,769 --> 00:37:48,489
practice one as the Pascal the OC
dataset it was pretty important I think

525
00:37:48,489 --> 00:37:53,399
in the earlier to thousands but now it's
a little bit small this one's about 20

526
00:37:53,400 --> 00:37:57,820
classes and about 20,000 images and
hence have about two objects percentage

527
00:37:57,820 --> 00:38:01,550
so because this is a relatively small
ish dataset you'll see a lot of

528
00:38:01,550 --> 00:38:05,860
detection papers work on this just goes
it's easier to handle but there's also

529
00:38:05,860 --> 00:38:09,970
an image that detection dataset image
that runs a whole bunch of challenges as

530
00:38:09,969 --> 00:38:13,109
you've probably seen by now we saw a
classification we sought localisation

531
00:38:13,110 --> 00:38:17,820
there's also an image that detection
challenge but protection there's only

532
00:38:17,820 --> 00:38:21,600
two hundred classes not the thousand
from classification but it's it's very

533
00:38:21,599 --> 00:38:25,619
big almost half a million images so you
don't see as many papers work on it just

534
00:38:25,619 --> 00:38:29,819
cuz it's kind of annoying to handle but
there's only about 100 per image and

535
00:38:29,820 --> 00:38:32,760
then more weeks more recently there's
this one from Microsoft called Coco

536
00:38:32,760 --> 00:38:36,660
which has fewer classes images but
actually has a lot more objects

537
00:38:36,659 --> 00:38:42,649
percentage so people like to work I'm
not now has more interesting right

538
00:38:42,650 --> 00:38:45,300
there's also this this when you're
talking about detection there's this

539
00:38:45,300 --> 00:38:49,000
funny evaluation metric we use called
mean average precision and early wanna

540
00:38:49,000 --> 00:38:52,000
get too much into the details like what
you really need to know is that it's a

541
00:38:52,000 --> 00:38:56,570
number between 0 and hundreds and
hundreds good and it

542
00:38:56,570 --> 00:38:59,940
and it also I mean the kind of the
intuition is that it's you want to have

543
00:38:59,940 --> 00:39:04,079
the right you wanna have true positives
get high scores and you also have to

544
00:39:04,079 --> 00:39:08,230
have some threshold that your boxes you
produced need to be within some

545
00:39:08,230 --> 00:39:12,090
threshold of a crack box and you can
usually this that threshold this point

546
00:39:12,090 --> 00:39:15,420
by an intersection of a union but you'll
see different challenges you slightly

547
00:39:15,420 --> 00:39:19,740
different things for thats right so
let's now that we understand the data

548
00:39:19,739 --> 00:39:24,679
sets on the elevation of us via our CNN
did right so this is on the past two

549
00:39:24,679 --> 00:39:27,779
versions of the Pascal Davis at like I
said it's smaller as you'll see a lot of

550
00:39:27,780 --> 00:39:32,730
results on this there's different
versions one in 2007 2010 you often see

551
00:39:32,730 --> 00:39:35,990
people use those just because the test
is publicly available so it's easy to

552
00:39:35,989 --> 00:39:37,169
evaluate

553
00:39:37,170 --> 00:39:42,380
yeah but so in this this deformable
parts model that we saw from 2011 from

554
00:39:42,380 --> 00:39:48,579
couple slides ago is getting twenty
about 30 on average precision there's

555
00:39:48,579 --> 00:39:52,069
this other method called region let's
from 2013 that was sort of the state of

556
00:39:52,070 --> 00:39:55,280
the art that I could find right before
deep learning but it's it's sort of a

557
00:39:55,280 --> 00:39:58,130
similar flavor you have these features
in its class players on top of teachers

558
00:39:58,130 --> 00:40:02,840
and our CNN is this pretty simple thing
we just saw and actually jump and

559
00:40:02,840 --> 00:40:06,789
actually improves the performance quite
a lot so the first thing the Seas we had

560
00:40:06,789 --> 00:40:10,509
a big improvement when we just switch
this pretty simple framework using CNN's

561
00:40:10,510 --> 00:40:15,160
and actually this this result here is
without the bounding box repressions

562
00:40:15,159 --> 00:40:19,029
this is only using the region proposals
on ESPN's actually if you include this

563
00:40:19,030 --> 00:40:23,550
additional bonding proposal stop it
actually helps quite a bet another fun

564
00:40:23,550 --> 00:40:26,820
thing to note is that if you take our
CNN and you do everything the same

565
00:40:26,820 --> 00:40:31,080
except used eg 16 instead of Alex net
you get another pretty big boost in

566
00:40:31,079 --> 00:40:34,059
performance so this is kind of similar
to what we've seen before that just

567
00:40:34,059 --> 00:40:39,650
using these more powerful features tends
to help a lot of different tasks right

568
00:40:39,650 --> 00:40:42,840
this is really good right we've we've
done like a huge improvement on

569
00:40:42,840 --> 00:40:47,829
detection compared to 2013 that's
amazing but our CNN is not perfect it

570
00:40:47,829 --> 00:40:53,150
has some problems right so it's pretty
slow its test time right we saw that we

571
00:40:53,150 --> 00:40:57,110
have maybe two thousand regions means to
evaluate our CNN for each region that's

572
00:40:57,110 --> 00:41:02,910
kinda slow we also have this this
slightly subtle problem where r SVM

573
00:41:02,909 --> 00:41:07,009
regression those were sort of trained
off-line using likely best p.m.

574
00:41:07,010 --> 00:41:10,930
and linear regression actually the
weights of our of our CNN didn't really

575
00:41:10,929 --> 00:41:14,960
have the chance to update in response to
what those parts of of a network of

576
00:41:14,960 --> 00:41:19,039
those objectives wanted to do and we
also had this kind of complicated

577
00:41:19,039 --> 00:41:24,309
training pipeline that was a bit of a
mess so to fix these problems a year

578
00:41:24,309 --> 00:41:29,690
later we have this thing called fast our
CNN so fast our CNN it was presented

579
00:41:29,690 --> 00:41:34,950
pretty recently in ICC be just in
December but the idea is really simple

580
00:41:34,949 --> 00:41:39,819
we're just gonna swap the order of
extracting regions and running the CNN

581
00:41:39,820 --> 00:41:43,550
this is kind of a kind of related to the
sliding window idea we saw with

582
00:41:43,550 --> 00:41:48,450
over-the-top so here the pipeline that
test time looks kinda similar we have

583
00:41:48,449 --> 00:41:52,299
this input image we're gonna not we're
going to take this high-resolution input

584
00:41:52,300 --> 00:41:55,920
image and run it through the
convolutional layers of our network and

585
00:41:55,920 --> 00:42:00,150
now we're gonna get this high-resolution
convolutional feature map and now our

586
00:42:00,150 --> 00:42:03,940
region proposals were gonna extracts
directly features for those region

587
00:42:03,940 --> 00:42:07,610
proposals from this convolutional
feature map using this thing called ROI

588
00:42:07,610 --> 00:42:10,530
pooling and then the region's

589
00:42:10,530 --> 00:42:14,269
the features for these the compositional
features for those regions will be fed

590
00:42:14,269 --> 00:42:17,829
into our fully connected layers and will
again have a classification had a

591
00:42:17,829 --> 00:42:22,670
regression had like we saw before so
this is really cool it's it's pretty

592
00:42:22,670 --> 00:42:26,930
great it solves a lot of the problems
that we just saw with our CNN so our CNN

593
00:42:26,929 --> 00:42:31,039
is really slow at US time we solve this
problem by just sharing this this

594
00:42:31,039 --> 00:42:37,289
computation of convolutional features
across the region proposals are see our

595
00:42:37,289 --> 00:42:40,519
CNN also have these problems at training
time where we had this this message

596
00:42:40,519 --> 00:42:44,920
training pipeline we had this this
problem where we're training different

597
00:42:44,920 --> 00:42:48,760
parts of the network separately and the
solution is pretty simple we just you

598
00:42:48,760 --> 00:42:50,480
know training all together all at once

599
00:42:50,480 --> 00:42:53,800
don't don't have this complicated
pipeline which we can actually do it now

600
00:42:53,800 --> 00:42:58,140
that we have this this pretty nice
function from inputs to outputs right as

601
00:42:58,139 --> 00:43:01,299
you can see that are so that fast our
CNN actually solves quite a lot of the

602
00:43:01,300 --> 00:43:06,340
problems that we saw with our CNN sort
of them with a really interesting

603
00:43:06,340 --> 00:43:10,530
technical bit in fast our CNN was this
problem of our way region of interest

604
00:43:10,530 --> 00:43:15,519
pooling so the idea is that we have this
input image that's probably high

605
00:43:15,519 --> 00:43:19,068
resolution and we have this region
proposal that's becoming

606
00:43:19,068 --> 00:43:23,969
elective surgery boxes or something like
that and we can put this region this

607
00:43:23,969 --> 00:43:27,199
high resolution image through our
convolutional and pooling layers just

608
00:43:27,199 --> 00:43:30,880
fine because those are sort of
scale-invariant they're still up two

609
00:43:30,880 --> 00:43:34,318
different sizes of inputs but now the
problem is that the fully connected

610
00:43:34,318 --> 00:43:39,630
layers from our pre train network are
expecting these pretty low res con

611
00:43:39,630 --> 00:43:46,068
features whereas these features from the
whole image are high res so now we solve

612
00:43:46,068 --> 00:43:50,038
this problem in a pretty straightforward
way so given this region proposal we're

613
00:43:50,039 --> 00:43:53,930
gonna projected onto it sort of the
special part of that comment feature

614
00:43:53,929 --> 00:43:59,368
volume now we're going to divide that
Khan future vol into a little grid right

615
00:43:59,369 --> 00:44:04,910
divide that thing into this hiw grid
that downstream layers are expecting and

616
00:44:04,909 --> 00:44:09,798
we do Macs pulling within each of those
grid cells so now we've seen now we have

617
00:44:09,798 --> 00:44:14,349
this pretty simple strategy we've taken
this region proposal and we've shared

618
00:44:14,349 --> 00:44:19,430
compilation features extracted this
excites output for that region for that

619
00:44:19,429 --> 00:44:23,629
for that region proposal writes this is
basically just swapping the order of

620
00:44:23,630 --> 00:44:28,108
convolution and warping and cropping
that's one way to think about it and

621
00:44:28,108 --> 00:44:31,538
also this is a pretty nice operation
because since this thing is basically

622
00:44:31,539 --> 00:44:35,249
just max pulling and we know how to back
propagate through max pulling you can

623
00:44:35,248 --> 00:44:38,368
back propagate through these are these
are region of interest pulling there's

624
00:44:38,369 --> 00:44:42,269
just fine and that's what really allows
us to train this whole thing in a joint

625
00:44:42,268 --> 00:44:46,758
way rights let's see some results and
these are actually pretty cool pretty

626
00:44:46,759 --> 00:44:50,858
amazing great so for training time are
CNN it had this complicated pipeline

627
00:44:50,858 --> 00:44:54,098
would save all the stuff that desk where
to do all this stuff independently and

628
00:44:54,099 --> 00:44:57,789
even on that pretty small Pascale Denis
at it took eighty four hours to train

629
00:44:57,789 --> 00:45:05,229
passed our CNN is much faster you can
train and a at as far as test time in LR

630
00:45:05,228 --> 00:45:09,318
CNN is pretty slow because again we're
running these independent forward passes

631
00:45:09,318 --> 00:45:14,469
at the CNN for each region proposal
whereas for fast our CNN where we can

632
00:45:14,469 --> 00:45:17,979
sort of share the computation between
different region proposals and get this

633
00:45:17,978 --> 00:45:23,439
gigantic speed up a test I'm a hundred
and forty-six that's great amazing and

634
00:45:23,440 --> 00:45:26,690
in fact in terms of performance I mean
it does a little bit better it's not a

635
00:45:26,690 --> 00:45:30,048
drastic difference in performance but
this could probably be attributed to

636
00:45:30,048 --> 00:45:32,130
this fine tuning property that

637
00:45:32,130 --> 00:45:35,140
past our CNN you can actually find you
in all parts of the convolutional

638
00:45:35,139 --> 00:45:38,969
network jointly to help with these ALPA
tasks and that's probably why you see a

639
00:45:38,969 --> 00:45:43,230
bit of an increase here right so this is
great right what's what could possibly

640
00:45:43,230 --> 00:45:45,730
be wrong with fast our CNN and its looks
amazing

641
00:45:45,730 --> 00:45:51,699
the big problem is that these tests I'm
speeds don't include region proposals

642
00:45:51,699 --> 00:45:55,669
right so now fast our CNN the so good
that actually the bottleneck is

643
00:45:55,670 --> 00:46:00,750
computing region proposals that's pretty
cool so once you factor in the speed of

644
00:46:00,750 --> 00:46:04,789
computer actually computing these region
proposals on CPU you can see that a lot

645
00:46:04,789 --> 00:46:09,190
of our speed benefits disappear right
only 25 x faster and we kind of lost

646
00:46:09,190 --> 00:46:15,030
that beautiful hundred speed-up also now
because it takes me two seconds Tehran

647
00:46:15,030 --> 00:46:18,560
actually pretty magenta and you can't
really use this real-time it still kinda

648
00:46:18,559 --> 00:46:23,750
off-line processing thing right so the
solution of this should be pretty

649
00:46:23,750 --> 00:46:27,340
obvious rate we are all you're already
using a convolutional network for

650
00:46:27,340 --> 00:46:32,620
regression using it for classification
why not use it for a reason proposals to

651
00:46:32,619 --> 00:46:39,569
write should work may be kind of crazy
so that's that's a paper anyone want to

652
00:46:39,570 --> 00:46:46,570
guess the name yes it's faster our CNN

653
00:46:46,570 --> 00:46:50,789
yes they were they were really creative
here right but the idea is pretty simple

654
00:46:50,789 --> 00:46:55,460
right where some from fast our CNN where
are you taking our input image and where

655
00:46:55,460 --> 00:46:59,630
computing these big convolutional
feature maps over the entire input image

656
00:46:59,630 --> 00:47:05,170
so that instead of using some external
method to compute region proposals they

657
00:47:05,170 --> 00:47:09,010
add this little thing called the region
proposal network that looks directly at

658
00:47:09,010 --> 00:47:13,060
these looks at these last their
compositional features as able to

659
00:47:13,059 --> 00:47:17,599
produce region proposals directly from
that competition will feature map and

660
00:47:17,599 --> 00:47:21,190
then once you have region proposals you
just do the same thing as fast our CNN

661
00:47:21,190 --> 00:47:25,880
you use this ROI pooling and I'll be
upstream stop is the same as fast or CNN

662
00:47:25,880 --> 00:47:31,130
so really about the novel bit here is
this region proposal network it's it's

663
00:47:31,130 --> 00:47:34,180
really cool right we're doing the whole
thing and one giant competition at work

664
00:47:34,179 --> 00:47:40,500
right so the way this region proposal
network works is that were sort of we

665
00:47:40,500 --> 00:47:43,880
receive as input this competition will
feature map this may be coming out of

666
00:47:43,880 --> 00:47:47,820
the last layer of our convolutional
features and we're going to add you like

667
00:47:47,820 --> 00:47:52,570
like most things are recent post on that
worked as a convolutional network right

668
00:47:52,570 --> 00:47:57,570
so actually this is a typo this is a
free by freak on that right so we have a

669
00:47:57,570 --> 00:48:01,809
sort of a sliding window approach over
our convolutional feature map but

670
00:48:01,809 --> 00:48:06,820
sliding sliding window is just a
convolution rate so we just have a three

671
00:48:06,820 --> 00:48:10,920
by three convolution on top of this
feature map and then we have this this

672
00:48:10,920 --> 00:48:14,599
peculiar struck this familiar to head
structure inside the region proposal

673
00:48:14,599 --> 00:48:19,670
network where we're doing classification
we're here we just want to say whether

674
00:48:19,670 --> 00:48:25,430
or not it's an object and also
regression to regress from this sort of

675
00:48:25,429 --> 00:48:29,829
position on to an actual pridgen
proposal so the idea is that the

676
00:48:29,829 --> 00:48:33,909
position of the sliding window relative
to the feature map sort of tells us

677
00:48:33,909 --> 00:48:38,239
where we are in the image and then these
regression outputs sort of give us

678
00:48:38,239 --> 00:48:43,619
corrections on top of this this position
in the feature map but actually they

679
00:48:43,619 --> 00:48:46,940
make it a little bit more complicated
than that so instead of addressing

680
00:48:46,940 --> 00:48:51,110
directly from this position in the
convolution will feature map they have

681
00:48:51,110 --> 00:48:55,280
this notion of these different anchor
boxes you can imagine taking these

682
00:48:55,280 --> 00:48:59,910
different sized and shaped banker boxes
and sort of pasting them in the original

683
00:48:59,909 --> 00:49:03,538
image at the point of the image
corresponding to this point in the

684
00:49:03,539 --> 00:49:08,020
feature map right leg and fast RCMP were
projecting forward from the image into

685
00:49:08,019 --> 00:49:11,519
the feature map now we're doing the
opposite we're projecting from the

686
00:49:11,519 --> 00:49:17,288
feature map back into the image for
these boxes so then for each of these

687
00:49:17,289 --> 00:49:21,640
anchor boxes they use sort of an
convolutional anchor boxes may use the

688
00:49:21,639 --> 00:49:27,400
same ones at every position in the image
and they for each of these anchor boxes

689
00:49:27,400 --> 00:49:32,119
they produce score as to whether or not
that anchor box corresponds to an object

690
00:49:32,119 --> 00:49:36,809
and they also produce for regression
coordinates that's incorrect that anger

691
00:49:36,809 --> 00:49:41,880
box in similar ways that we saw before
and now this region proposal network you

692
00:49:41,880 --> 00:49:45,700
can just trained to try to predict it's
sort of a high-class agnostic object

693
00:49:45,699 --> 00:49:52,058
detector so faster our CNN in the
original paper they train this thing and

694
00:49:52,059 --> 00:49:55,490
kind of a funny way where first they
train read a proposal not work then they

695
00:49:55,489 --> 00:49:59,500
train passed our CNN then they do some
magic to merge together and at the end

696
00:49:59,500 --> 00:50:03,530
of the day they have one network that
produces everything so this this is a

697
00:50:03,530 --> 00:50:07,880
little bit messy but individual paper
they describe this thing but since then

698
00:50:07,880 --> 00:50:10,470
they've had some unpublished work where
they actually just change the whole

699
00:50:10,469 --> 00:50:14,909
thing jointly where they're sort of have
one big network where you have an image

700
00:50:14,909 --> 00:50:19,679
coming in you have this in inside the
region proposal network you have a

701
00:50:19,679 --> 00:50:23,538
classification lost to classify whether
each region proposal is or is not an

702
00:50:23,539 --> 00:50:27,670
object you have these bounding box
regressions inside the region proposal

703
00:50:27,670 --> 00:50:33,500
not work on top of your competition
anchors and then from fast then we do

704
00:50:33,500 --> 00:50:37,190
our life pooling and do this fast our
CNN trek and then at the end of the

705
00:50:37,190 --> 00:50:41,200
network we have this classification lost
to say which class that is and this

706
00:50:41,199 --> 00:50:47,659
regression lost to correct a correction
on top of the region proposal so this is

707
00:50:47,659 --> 00:50:53,170
this big thing is just one big network
with four losses yeah

708
00:50:53,170 --> 00:51:04,019
so the proposal and repression
coordinates are produced by a three by

709
00:51:04,019 --> 00:51:07,588
three three by three and an apparent
one-by-one convolutions often feature

710
00:51:07,588 --> 00:51:12,358
map right so the idea is that we're
looking at these different anchor boxes

711
00:51:12,358 --> 00:51:16,400
of different positions and scales but
we're actually looking at the same

712
00:51:16,400 --> 00:51:20,139
position in the feature map to classify
those different banker boxes but you

713
00:51:20,139 --> 00:51:26,179
have different you learn different
weights for the different anchors I

714
00:51:26,179 --> 00:51:29,969
think it's mostly empirical right so the
three by the idea is just you want to

715
00:51:29,969 --> 00:51:33,429
have a little bit of nonlinearity you
could imagine just doing sort of a

716
00:51:33,429 --> 00:51:38,098
direct one-by-one convolution directly
off the feature maps but I think they

717
00:51:38,099 --> 00:51:40,990
don't discuss this in the paper but I'm
guessing just a three by three times to

718
00:51:40,989 --> 00:51:44,669
work a bit better but there's no like
really deep reason why you why you do

719
00:51:44,670 --> 00:51:47,450
that you could be more could be less
that could be a bigger colonel is just

720
00:51:47,449 --> 00:51:50,548
sort of you have this little competition
at work with two heads that's the main

721
00:51:50,548 --> 00:51:53,710
point and your questions

722
00:51:53,710 --> 00:52:18,380
yeah I understand because

723
00:52:18,380 --> 00:52:22,140
corresponds to the whole image

724
00:52:22,139 --> 00:52:26,098
the point is that we don't actually want
to process the whole image you want to

725
00:52:26,099 --> 00:52:29,960
pick out some regions of the image to do
more processing on but we need to choose

726
00:52:29,960 --> 00:52:36,048
those regions somehow

727
00:52:36,048 --> 00:52:42,188
yes that's basically the that's
basically this idea of using external

728
00:52:42,188 --> 00:52:46,428
region proposals right so when you do
that external region proposals you're

729
00:52:46,429 --> 00:52:50,929
sort of picking it first before you do
the convolutions but it's just sort of a

730
00:52:50,929 --> 00:52:54,858
nice thing if you can do it all at once
so it's like I'm illusions are kind of

731
00:52:54,858 --> 00:52:58,748
this general like really general
processing processing but you can do to

732
00:52:58,748 --> 00:53:01,608
the image you're kinda hoping that
contributions are good enough for

733
00:53:01,608 --> 00:53:04,869
classification gonna aggression the
types of information that you have in

734
00:53:04,869 --> 00:53:07,439
those contributions is probably good
enough for classifying regions as well

735
00:53:07,438 --> 00:53:11,958
so it's actually it's actually a
computational savings because at the end

736
00:53:11,958 --> 00:53:15,719
of the day you end up using that same
convolutional Peter map for everything

737
00:53:15,719 --> 00:53:18,938
for the region proposals for the
downstream classification for the dam

738
00:53:18,938 --> 00:53:23,389
downstream regression that's actually
why you get the speed up here

739
00:53:23,389 --> 00:53:29,788
question yes we have this big network we
train with four losses and now we can do

740
00:53:29,789 --> 00:53:31,569
object detection sort of all at once

741
00:53:31,568 --> 00:53:37,858
pretty cool so if we look at results
comparing the free our CNN's of various

742
00:53:37,858 --> 00:53:43,630
velocities then we have original our CNN
it took about 50 seconds a test time per

743
00:53:43,630 --> 00:53:47,150
image this is counting the region
proposals this is counting running the

744
00:53:47,150 --> 00:53:52,439
CNN separately for each region proposal
that's pretty slow now passed our CNN we

745
00:53:52,438 --> 00:53:56,909
saw it was sort of bottleneck by the
region proposal time but once we move to

746
00:53:56,909 --> 00:54:01,768
faster our CNN than those region
proposals are basically coming for free

747
00:54:01,768 --> 00:54:06,139
since they're just the way we compute
region proposals is just a tiny three my

748
00:54:06,139 --> 00:54:09,199
free time dilution and a couple
one-by-one convolutions so they're very

749
00:54:09,199 --> 00:54:13,229
cheap to evaluate it sent a test times
faster our CNN runs in the fifth of a

750
00:54:13,228 --> 00:54:23,849
second a pretty high resolution image
that's actually yeah

751
00:54:23,849 --> 00:54:36,739
well I mean you're not one of the ideas
behind zero padding as you're hoping not

752
00:54:36,739 --> 00:54:40,699
too far away information from the edges
so I think maybe you might have a

753
00:54:40,699 --> 00:54:45,299
problem with if you didn't do the zero
padding and maybe more problem but I

754
00:54:45,300 --> 00:54:48,430
mean as we sort of discussed before and
the fact that you're adding that zero

755
00:54:48,429 --> 00:54:52,519
padding might affect the statistics of
those features so it could maybe be a

756
00:54:52,519 --> 00:54:56,900
bit of a problem but in practice it
seems to work just fine but actually

757
00:54:56,900 --> 00:55:00,099
about yeah that that's an analysis of
where do we have a failure cases where

758
00:55:00,099 --> 00:55:02,949
do we get things wrong as a really
important process when you develop new

759
00:55:02,949 --> 00:55:08,419
algorithms and I can give you insight
into what might make things better

760
00:55:08,420 --> 00:55:26,940
yeah yeah yeah

761
00:55:26,940 --> 00:55:35,858
so maybe it might help but it's actually
kinda hard to the next to do that

762
00:55:35,858 --> 00:55:40,108
experiment because the data sets are
different right because when you when

763
00:55:40,108 --> 00:55:43,789
you were kind of classification dataset
like image now that's one thing but then

764
00:55:43,789 --> 00:55:47,259
when you work on detection it's this
other data set and I haven't liked you

765
00:55:47,260 --> 00:55:51,000
could imagine trying to classify the
detection images based on what objects

766
00:55:51,000 --> 00:55:54,500
are present but I haven't really seen
any really good comparisons that try to

767
00:55:54,500 --> 00:56:00,630
study that apparently but I mean that
the experiment on your project

768
00:56:00,630 --> 00:56:18,088
yeah that's a very good question so then
you have this problem with our way

769
00:56:18,088 --> 00:56:22,119
pooling right because of the way that
the ROI pooling work as well as by

770
00:56:22,119 --> 00:56:25,720
dividing that thing into the sixth grade
and doing max pulling once you do

771
00:56:25,719 --> 00:56:29,949
rotations it's actually kind of
difficult there's this really cool paper

772
00:56:29,949 --> 00:56:33,159
from deep mind in the last over the
summer called spatial transformer

773
00:56:33,159 --> 00:56:39,250
networks that actually introduces a
really cool way to solve this problem in

774
00:56:39,250 --> 00:56:42,239
the idea is that instead of doing ROI
pooling we're gonna do by linear

775
00:56:42,239 --> 00:56:46,699
interpolation kinda like you might be
used for textures and graphics so once

776
00:56:46,699 --> 00:56:50,009
you do by linear interpolation than you
actually can do maybe these these crazy

777
00:56:50,010 --> 00:56:53,609
regions so yeah that's definitely
something people are thinking about but

778
00:56:53,608 --> 00:56:56,848
it hasn't been incorporated into the
into the whole pipeline yet

779
00:56:56,849 --> 00:57:00,338
yeah

780
00:57:00,338 --> 00:57:11,728
you could be slowed down your back in
this sort of our CNN regime right and

781
00:57:11,728 --> 00:57:12,449
look at that

782
00:57:12,449 --> 00:57:16,828
250 times slower you really want to pay
that price I mean I think another

783
00:57:16,829 --> 00:57:20,690
practical concern with rotated objects
is that we don't really have that ground

784
00:57:20,690 --> 00:57:25,318
truth data sets so for most of these
most of these detection dataset the only

785
00:57:25,318 --> 00:57:29,190
ground truth information we have are
these access online bounding boxes so

786
00:57:29,190 --> 00:57:33,150
it's hard you don't have a ground truth
position that's kind of a practical

787
00:57:33,150 --> 00:57:39,219
concern I think people haven't really
explored this so much so the end and

788
00:57:39,219 --> 00:57:43,009
story with past our CNN has its super
fast and it was about the same right

789
00:57:43,009 --> 00:57:49,798
that's good and works actually really
interesting is now at this point I knew

790
00:57:49,798 --> 00:57:52,949
it you can actually understand the state
of the art in object detection so this

791
00:57:52,949 --> 00:57:55,669
is this is one of the best object
detector in the world it crushed

792
00:57:55,670 --> 00:58:00,479
everyone at the image that challenge in
image and cocoa challenges in December

793
00:58:00,478 --> 00:58:06,710
and like most other thing is it's this
deep residual network so the best object

794
00:58:06,710 --> 00:58:10,548
in the world right now is a hundred and
one layer residual network plus faster

795
00:58:10,548 --> 00:58:17,298
our CNN plus a couple other goodies here
right so we talked about we talk about

796
00:58:17,298 --> 00:58:23,670
past our CNN we saw president last year
they have to get an extra they always

797
00:58:23,670 --> 00:58:26,389
for competitions you need to add a
couple of crazy things to get a little

798
00:58:26,389 --> 00:58:30,348
bit boost in performance right so here
in this box refinements actually do

799
00:58:30,349 --> 00:58:33,528
multiple steps of refining the bounding
box

800
00:58:33,528 --> 00:58:38,818
you saw that in the fast our CNN
framework you doing this correction on

801
00:58:38,818 --> 00:58:41,929
top of your region proposal could
actually feed that back into the network

802
00:58:41,929 --> 00:58:46,298
and reclassify Andrea get another
production so that's this box refinement

803
00:58:46,298 --> 00:58:50,929
step it gives you a little bit a boost
they add context so in addition to

804
00:58:50,929 --> 00:58:55,710
classifying just just the region they
get out of actor that gives you the

805
00:58:55,710 --> 00:59:00,309
whole features for the entire image that
sort of gives you more contacts than

806
00:59:00,309 --> 00:59:03,999
just that little crop net gives you a
little bit more apartments and they also

807
00:59:03,998 --> 00:59:08,179
do multiscale testing kinda like we saw
in over feet back so they actually run

808
00:59:08,179 --> 00:59:10,730
the thing on images at different size is
a test time

809
00:59:10,730 --> 00:59:13,949
an aggregate or those different sizes
and when you put all those things

810
00:59:13,949 --> 00:59:21,129
together you win a lot of competitions
so this thing one on SoCo actually

811
00:59:21,130 --> 00:59:24,960
Microsoft Coco actually runs a detection
challenge and they wonder detection

812
00:59:24,960 --> 00:59:29,199
challenge on cocoa we can also look at
the rapid progress on the image that

813
00:59:29,199 --> 00:59:32,909
detection challenges over the last
couple of years so you can see in 2013

814
00:59:32,909 --> 00:59:38,949
was sort of the first time that we had
these deep learning detection models so

815
00:59:38,949 --> 00:59:43,789
over feat that we saw for localisation
they actually submitted version of their

816
00:59:43,789 --> 00:59:47,949
system that works on detection as well
by sort of changing the logic with by

817
00:59:47,949 --> 00:59:51,849
which they merge bounding boxes and they
did pretty good but they were actually

818
00:59:51,849 --> 00:59:57,319
outperformed by this other this other
group called you vision that was sort of

819
00:59:57,320 --> 01:00:02,289
not a deep learning approach to use a
lot of features but none in 2014 we

820
01:00:02,289 --> 01:00:05,840
actually saw both of these were deep
learning approaches and Google actually

821
01:00:05,840 --> 01:00:09,740
won that one by using a Google Map plus
some other detection stuff on top of

822
01:00:09,739 --> 01:00:15,029
Google not and then in 2015 things went
crazy and these residual networks plus

823
01:00:15,030 --> 01:00:19,410
passer I CNN just crushed everything so
I think that action especially over the

824
01:00:19,409 --> 01:00:22,409
last couple years has been a really
exciting thing because we've seen this

825
01:00:22,409 --> 01:00:25,429
really rapid progress over the last
couple years in detection like most

826
01:00:25,429 --> 01:00:29,129
other things and another point I think
it's kind of fun to make is that

827
01:00:29,130 --> 01:00:33,800
actually for all I can to win
competitions you know Andre said you

828
01:00:33,800 --> 01:00:37,830
ensemble and get 2% so you always win
competitions with an ensemble but

829
01:00:37,829 --> 01:00:42,829
actually sort of fun microsoft also
submitted their best single resident

830
01:00:42,829 --> 01:00:47,440
model this was not an ensemble and just
a single resident model actually be all

831
01:00:47,440 --> 01:00:52,400
the other things from all the other
years that's actually pretty cool yeah

832
01:00:52,400 --> 01:00:58,130
that's that's the best actor out there
so this is kind of a funny thing right

833
01:00:58,130 --> 01:01:03,240
so this is a really so we we we talked
about this idea of localisation as

834
01:01:03,239 --> 01:01:08,439
regression so this funny thing called
Yolo you only look once actually tries

835
01:01:08,440 --> 01:01:13,519
to oppose the detection problem directly
as a regression problem so the idea is

836
01:01:13,519 --> 01:01:18,389
that we actually are going to take our
input image and we're gonna divided into

837
01:01:18,389 --> 01:01:22,190
some spatial grid they used to seven by
seven and then within

838
01:01:22,190 --> 01:01:26,480
each element about spatial grid we're
gonna make six number of bounding box

839
01:01:26,480 --> 01:01:31,039
predictions they use be equal to I think
in most of the experiments so then

840
01:01:31,039 --> 01:01:36,489
within each grid you're going to predict
maybe to be bounding boxes that's four

841
01:01:36,489 --> 01:01:41,229
numbers are also going to protect US
single score for how much you believe

842
01:01:41,230 --> 01:01:44,969
that bounding box and you're also going
to protect classification score for each

843
01:01:44,969 --> 01:01:49,659
class near Davis at so then you can sort
of take this this detection problem and

844
01:01:49,659 --> 01:01:53,969
it ends up being regression your input
is an image in your output is this maybe

845
01:01:53,969 --> 01:01:59,529
seven by seven by five B plus see answer
right now just a regression problem and

846
01:01:59,530 --> 01:02:04,820
just try it and that's pretty cool and
it's it's sort of a new approach to a

847
01:02:04,820 --> 01:02:07,900
bit different than these region proposal
things that we've seen before

848
01:02:07,900 --> 01:02:12,300
of course sort of a problem with this is
that there's an upper bound in the

849
01:02:12,300 --> 01:02:15,930
number of outputs that your model can
have so that might be a problem if

850
01:02:15,929 --> 01:02:20,279
you're testing data has many many more
ground truth boxes in your training data

851
01:02:20,280 --> 01:02:27,180
so this this yellow detector actually is
really fast it's actually faster and

852
01:02:27,179 --> 01:02:32,460
then faster our CNN which is pretty
crazy but unfortunately it tends to work

853
01:02:32,460 --> 01:02:36,769
a little bit worse so bad this other
thing called fast yellow that i dont

854
01:02:36,769 --> 01:02:39,460
wanna talk about but

855
01:02:39,460 --> 01:02:45,170
right but just as our number these are
mean AP numbers on passed on one of the

856
01:02:45,170 --> 01:02:49,619
Pascal data sets that we saw you can see
yellow actually gets 64 that's pretty

857
01:02:49,619 --> 01:02:53,329
good and runs at forty five frames per
second that this is obviously on a

858
01:02:53,329 --> 01:02:58,840
powerful GPU but still that's that's
pretty much real time that's amazing

859
01:02:58,840 --> 01:03:03,960
was also I don't wanna talk about that
right now knows these different versions

860
01:03:03,960 --> 01:03:09,309
of past and Pastor are CNN's you can see
that these actually pretty much all beat

861
01:03:09,309 --> 01:03:14,119
yo in terms of performance but are quite
a bit slower yeah that's that's actually

862
01:03:14,119 --> 01:03:20,119
kind of a neat twist on the detection
problem actually all these all these

863
01:03:20,119 --> 01:03:22,779
different detection metric all these
different detection models that we

864
01:03:22,780 --> 01:03:26,780
talked about today they all pretty much
have code up their released you should

865
01:03:26,780 --> 01:03:30,800
maybe consider using them for projects
probably don't use our CNN it's too slow

866
01:03:30,800 --> 01:03:36,090
fast are seen on pretty good but
requires MATLAB pastor our CNN there is

867
01:03:36,090 --> 01:03:39,720
actually a Persian a pastor our CNN that
doesn't require MATLAB is just Pipeline

868
01:03:39,719 --> 01:03:44,379
Cafe I haven't personally used it but
it's something you might want to try to

869
01:03:44,380 --> 01:03:48,070
use for your projects I'm not sure how
difficult it is to get running and

870
01:03:48,070 --> 01:03:52,050
yellow as actually I think maybe a good
choice for some of your projects because

871
01:03:52,050 --> 01:03:55,810
it's so fast that it might be easier to
work with if you have not be really big

872
01:03:55,809 --> 01:03:59,860
powerful GPUs and actually have caught
up as well

873
01:03:59,860 --> 01:04:03,480
yes that's actually I got through things
a little bit faster than expected so is

874
01:04:03,480 --> 01:04:10,559
there any questions on detection

875
01:04:10,559 --> 01:04:15,880
yeah

876
01:04:15,880 --> 01:04:22,630
yes in terms of model like model size
it's pretty much about the same as a

877
01:04:22,630 --> 01:04:26,039
classification model because when when
you're running on bigger image

878
01:04:26,039 --> 01:04:29,109
especially for faster our CNN right
cause your convolutions you don't really

879
01:04:29,108 --> 01:04:32,558
introduce any more parameters the full
impact of layers are not really anymore

880
01:04:32,559 --> 01:04:35,829
parameters you have a couple extra
parameters for the region proposal

881
01:04:35,829 --> 01:04:38,798
network but it's basically the same
number primaries as a classification

882
01:04:38,798 --> 01:04:45,619
model right I guess I guess we're done a
little early today

