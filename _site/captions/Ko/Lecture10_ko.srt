1
00:00:00,000 --> 00:00:04,129
마이크 테스트

2
00:00:04,129 --> 00:00:12,109
오늘은 주제는 Recurrent neural networks 입니다.

3
00:00:12,109 --> 00:00:15,199
개인적으로 가장 좋아하는 주제이고

4
00:00:15,199 --> 00:00:18,960
또 여러 형태로 사용하고 있는 NN 모델이기도 하죠. 재밌어요.

5
00:00:18,960 --> 00:00:23,009
강의 진행에 관해서 언급할 게 있는데,

6
00:00:23,009 --> 00:00:26,089
수요일에 중간 고사가 있어요.

7
00:00:26,089 --> 00:00:32,738
다들 중간고사 기대하고 있는거 다 알아요. 사실 별로 기대하는 것 같이 보이지는 않네요.

8
00:00:32,738 --> 00:00:37,979
수요일에 과제가 나갈 거에요.

9
00:00:37,979 --> 00:00:40,429
제출 기한은 2주 뒤 월요일까지입니다.

10
00:00:40,429 --> 00:00:43,399
그런데 저희가 원래 월요일에 이걸 발표하려 했는데 늦어져서

11
00:00:43,399 --> 00:00:47,129
아마 제출 기한이 수요일 즈음으로 미뤄질 것 같네요.

12
00:00:47,130 --> 00:00:51,179
2번째 과제는 금요일까지고, 3-late day를 사용할 수 있어요. 그런데 너무 일찍 사용하지는 마세요.

13
00:00:51,179 --> 00:00:55,119
2번째 과제는 금요일까지고, 3-late day를 사용할 수 있어요. 그런데 너무 일찍 사용하지는 마세요.

14
00:00:55,119 --> 00:01:01,089
몇 명이나 끝냈나요? 72명? 거의 다 끝냈네요, 좋아요.

15
00:01:01,090 --> 00:01:04,549
자 우리는 Convolutional Neural Network (CNN)에 대해서 얘기하고 있었죠.

16
00:01:04,549 --> 00:01:07,820
지난 수업에서는 CNN에 대한 시각화와 간단한 이해에 대해서 다루었고,

17
00:01:07,819 --> 00:01:11,618
이런 그림과 비디오들을 살펴보면서 CNN이 어떻게 작동하는지 살펴보았죠.

18
00:01:11,618 --> 00:01:14,938
이런 그림과 비디오들을 살펴보면서 CNN이 어떻게 작동하는지 살펴보았죠.

19
00:01:14,938 --> 00:01:17,828
이런 그림과 비디오들을 살펴보면서 CNN이 어떻게 작동하는지 살펴보았죠.

20
00:01:17,828 --> 00:01:24,188
그리고 맨 마지막 그림에서 본 것처럼 디버깅도 해 보았고요.

21
00:01:24,188 --> 00:01:27,408
지난 주말에 트위터에서 새로운 시각화 자료를 찾았는데요,

22
00:01:27,409 --> 00:01:32,569
신기하죠?

23
00:01:32,569 --> 00:01:37,118
사실 설명이 없어서 정확히 어떤 방법으로 이걸 만든 건지는 잘 모르겠네요.

24
00:01:37,118 --> 00:01:43,099
그래도 멋있지 않아요? 이건 거북이고, 저건 타란튤라 거미이고,

25
00:01:43,099 --> 00:01:47,468
이건 체인이고, 저건 개들인데,

26
00:01:47,468 --> 00:01:50,509
제가 보기에 이건 어떤 최적화 기법을 이미지에 적용한 것 같은데,

27
00:01:50,509 --> 00:01:53,679
뭔가 다른 regularization 방법을 적용한 것 같네요

28
00:01:53,679 --> 00:01:57,049
음, 여기에는 bilateral filter (쌍방향 필터) 를 적용한 것 같네요.

29
00:01:57,049 --> 00:01:59,420
음, 여기에는 bilateral filter (쌍방향 필터) 를 적용한 것 같네요.

30
00:01:59,420 --> 00:02:03,659
그래도 솔직히 정확히 어떤 기법을 적용한 것인지는 잘 모르겠어요.

31
00:02:03,659 --> 00:02:04,549
오늘의 주제는 RNN입니다.

32
00:02:04,549 --> 00:02:10,360
오늘의 주제는 RNN입니다.

33
00:02:10,360 --> 00:02:13,520
RNN의 강점은 네트워크 아키텍쳐를 구성하는 데에 자유도가 놓다는 것입니다.

34
00:02:13,520 --> 00:02:15,870
RNN의 강점은 네트워크 아키텍쳐를 구성하는 데에 자유도가 놓다는 것입니다.

35
00:02:15,870 --> 00:02:18,650
일반적으로 NN을 왼쪽 그림과 같이 구성할 때는 (역자주: Vanilla NN)

36
00:02:18,650 --> 00:02:22,849
여기 빨간색으로 표시된 것처럼 고정된 크기의 input vector를 사용하고,

37
00:02:22,848 --> 00:02:27,639
초록색의 hidden layer들을 통해 작동시키며, 마찬가지로 고정된 크기의 파란색 output vector를 출력합니다.

38
00:02:27,639 --> 00:02:30,738
마찬가지로 고정된 크기의 이미지를 입력으로 받고,

39
00:02:30,739 --> 00:02:34,469
고정된 크기의 이미지를 벡터 형태로 출력합니다.

40
00:02:34,469 --> 00:02:38,239
RNN에서는 이러한 작업을 계속 반복할 수 있습니다. input, output 모두에서 가능하죠.

41
00:02:38,239 --> 00:02:41,319
오늘 다룰 image captioning(이미지에 상응하는 자막/주석 생성) 을 예로 들면,

42
00:02:41,318 --> 00:02:44,689
고정된 크기의 이미지를 RNN에 입력하게 됩니다.

43
00:02:44,689 --> 00:02:47,829
그리고 그 RNN은 해당 이미지를 설명하는 단어/문장 들을 출력하게 되죠.

44
00:02:47,829 --> 00:02:52,560
그리고 그 RNN은 해당 이미지를 설명하는 단어/문장 들을 출력하게 되죠.

45
00:02:52,560 --> 00:02:55,969
Sentiment classification(감정 분류)를 예로 들면,

46
00:02:55,969 --> 00:02:59,759
(어떤 문장의) 단어들과 그 순서을 입력으로 받아서,

47
00:02:59,759 --> 00:03:03,828
그 문장의 느낌이 긍정적인지 또는 부정적인지를 출력하게 됩니다.

48
00:03:03,829 --> 00:03:07,590
또 다른 예로 machine translation (역자주: 구글 번역과 같은 알고리즘 번역) 에서는,

49
00:03:07,590 --> 00:03:12,069
어떤 영어 문장을 입력으로 받고, 프랑스어로 출력해야 합니다.

50
00:03:12,068 --> 00:03:17,119
그래서 우리는 이 영어 문장을 RNN에 입력하고 (이것을 Sequence to Sequence 라 부름)

51
00:03:17,120 --> 00:03:20,280
그래서 우리는 이 영어 문장을 RNN에 입력하고 (이것을 Sequence to Sequence 라 부름)

52
00:03:20,280 --> 00:03:25,169
RNN은 이 영어 문장을 프랑스어 문장으로 번역합니다.

53
00:03:25,169 --> 00:03:28,000
마지막 예 video classification(영상 분류) 에서는,

54
00:03:28,000 --> 00:03:31,699
각 프레임 (순간 캡쳐 화면) 이 어떤 속성을 지니는지,

55
00:03:31,699 --> 00:03:35,429
그리고 그 전의 모든 프레임과의 관계는 어떻게 되는지도 고려합니다.

56
00:03:35,430 --> 00:03:38,739
그리고 그 전의 모든 프레임과의 관계는 어떻게 되는지도 고려합니다.

57
00:03:38,739 --> 00:03:41,909
그러니까 RNN은 각각의 프레임이 어떤 속성을 지니는지 분류하고,

58
00:03:41,909 --> 00:03:44,680
이전까지의 모든 프레임을 입력으로 받는 함수가 되어,

59
00:03:44,680 --> 00:03:48,760
앞으로의 프레임을 예측하는 아키텍쳐를 제공합니다.

60
00:03:48,759 --> 00:03:52,388
만약 맨 왼쪽 그림과 같이 입력과 출력의 순서에 관한 정보를 가지고 있지 않아도 RNN을 사용할 수 있습니다.

61
00:03:52,389 --> 00:03:55,250
만약 맨 왼쪽 그림과 같이 입력과 출력의 순서에 관한 정보를 가지고 있지 않아도 RNN을 사용할 수 있습니다.

62
00:03:55,250 --> 00:04:01,560
예를 들어, 제가 좋아하는 딥마인드의 한 논문에서는

63
00:04:01,560 --> 00:04:05,189
번지로 된 집 주소 이미지를 문자로 변환했습니다.

64
00:04:05,189 --> 00:04:09,750
여기서는 단순히 CNN을 사용해서 이미지 자체가 몇 번지를 나타내는지를 분류하지 않고,

65
00:04:09,750 --> 00:04:13,530
여기서는 단순히 CNN을 사용해서 이미지 자체가 몇 번지를 나타내는지를 분류하지 않고,

66
00:04:13,530 --> 00:04:16,649
RNN을 사용해서 작은 CNN이 이미지를 돌아다니면서 읽어들였습니다.

67
00:04:16,649 --> 00:04:19,779
RNN을 사용해서 작은 CNN이 이미지를 돌아다니면서 읽어들였습니다.

68
00:04:19,779 --> 00:04:23,969
이렇게 RNN은 번지 주소 이미지를 왼쪽으로 오른쪽으로 순차적으로 읽는 방법을 학습했습니다.

69
00:04:23,970 --> 00:04:26,870
이렇게 RNN은 번지 주소 이미지를 왼쪽으로 오른쪽으로 순차적으로 읽는 방법을 학습했습니다.

70
00:04:26,870 --> 00:04:32,019
반대로 생각할 수도 있습니다. 이것은 DRAW라는 유명한 논문인데요,

71
00:04:32,019 --> 00:04:35,879
여기서는 이미지 샘플 하나하나가 무엇인지 개별적으로 판단하지 않고,

72
00:04:35,879 --> 00:04:39,490
여기서는 이미지 샘플 하나하나가 무엇인지 개별적으로 판단하지 않고,

73
00:04:39,490 --> 00:04:42,860
RNN이 여러 이미지를 하나의 큰 캔버스의 형태로 한번에 출력합니다.

74
00:04:42,860 --> 00:04:47,540
RNN이 여러 이미지를 하나의 큰 캔버스의 형태로 한번에 출력합니다.

75
00:04:47,540 --> 00:04:50,200
이 방법은 한 번지수 이미지에 대한 입력 결과를 곧바로 출력하지 않고, 보다 많은 계산을 거친다는 점에서 강력합니다.

76
00:04:50,199 --> 00:04:53,479
이 방법은 한 번지수 이미지에 대한 입력 결과를 곧바로 출력하지 않고, 보다 많은 계산을 거친다는 점에서 강력합니다. 질문 있나요?

77
00:04:53,480 --> 00:05:14,189
(질문) 그림에서 화살표는 무엇인가요?

78
00:05:14,189 --> 00:05:19,310
화살표는 functional dependence를 나타냅니다. 조금 있다가 좀 더 자세하게 살펴 볼 거에요.

79
00:05:19,310 --> 00:05:23,139
화살표는 functional dependence를 나타냅니다. 조금 있다가 좀 더 자세하게 살펴 볼 거에요.

80
00:05:23,139 --> 00:05:37,168
(질문) 그림에서 나타나는 숫자들은 무엇인가요?

81
00:05:37,168 --> 00:05:41,219
이것들은 실제 사진이 아니라 RNN이 학습 후 출력한 결과물입니다.

82
00:05:41,220 --> 00:05:44,830
이것들은 실제 사진이 아니라 RNN이 학습 후 출력한 결과물입니다.

83
00:05:44,829 --> 00:05:48,219
(질문) 그러니까 실제 사진이 아니라 만들어진 거라는 거죠?

84
00:05:48,220 --> 00:05:51,689
네, 꽤 실제 사진처럼 포이기는 하지만, 이것들은 만들어진 이미지입니다.

85
00:05:51,689 --> 00:05:55,809
RNN은 이런 초록색 박스처럼 생겼습니다.

86
00:05:55,809 --> 00:06:00,979
RNN은 계속해서 input vector를 입력받습니다.

87
00:06:00,978 --> 00:06:04,859
RNN은 계속해서 input vector를 입력받습니다.

88
00:06:04,860 --> 00:06:08,538
RNN 내부에는 여러 state가 있는데, 이는 매 시간에 입력받는 input vector의 형태로 나타낼 수 있습니다.

89
00:06:08,538 --> 00:06:12,988
RNN 내부에는 여러 state가 있는데, 이는 매 시간에 입력받는 input vector의 형태로 나타낼 수 있습니다.

90
00:06:12,988 --> 00:06:17,258
RNN에는 또한 weight(가중치)를 설정할 수 있고, 이를 조정함으로써 RNN의 작동을 조절할 수 있습니다.

91
00:06:17,259 --> 00:06:20,829
RNN에는 또한 weight(가중치)를 설정할 수 있고, 이를 조정함으로써 RNN의 작동을 조절할 수 있습니다.

92
00:06:20,829 --> 00:06:25,769
우리는 물론 RNN의 출력 결과물에도 관심을 갖고 있지만,

93
00:06:25,769 --> 00:06:30,429
우리는 물론 RNN의 출력 결과물에도 관심을 갖고 있지만,

94
00:06:30,428 --> 00:06:33,988
RNN은 이 중간에 있는, 시간에 따라 이미지를 입력받고 출력하는 단계인 이 초록색 박스라는 것을 알아두셨으면 합니다.

95
00:06:33,988 --> 00:06:36,688
RNN은 이 중간에 있는, 시간에 따라 이미지를 입력받고 출력하는 단계인 이 초록색 박스라는 것을 알아두셨으면 합니다.

96
00:06:36,689 --> 00:06:39,489
RNN은 이 중간에 있는, 시간에 따라 이미지를 입력받고 출력하는 단계인 이 초록색 박스라는 것을 알아두셨으면 합니다.

97
00:06:39,488 --> 00:06:44,838
RNN은 이 중간에 있는, 시간에 따라 이미지를 입력받고 출력하는 단계인 이 초록색 박스라는 것을 알아두셨으면 합니다.

98
00:06:44,838 --> 00:06:50,610
RNN의 각 state는 vector들의 집합으로 나타낼 수 있고, 여기서는 h로 표기하겠습니다.

99
00:06:50,610 --> 00:06:55,399
RNN의 각 state는 vector들의 집합으로 나타낼 수 있고, 여기서는 h로 표기하겠습니다.

100
00:06:55,399 --> 00:07:00,939
각각의 state(h_t) 는 바로 전 단계의 state(h_t-1)과 input vector(x_t)들의 함수로 나타낼 수 있습니다.

101
00:07:00,939 --> 00:07:05,769
각각의 state(h_t) 는 바로 전 단계의 state(h_t-1)과 input vector(x_t)들의 함수로 나타낼 수 있습니다.

102
00:07:05,769 --> 00:07:08,338
여기서의 함수는 Recurrence funtion 이라고 하고 파라미터 W(가중치)를 갖습니다.

103
00:07:08,338 --> 00:07:13,728
우리는 W 값을 변경함에 따라 RNN이 다른 결과를 보이는 걸 확인할 수 있습니다.

104
00:07:13,728 --> 00:07:16,228
우리는 W 값을 변경함에 따라 RNN이 다른 결과를 보이는 걸 확인할 수 있습니다.

105
00:07:16,228 --> 00:07:19,338
따라서 우리는 우리가 원하는 결과를 만들어낼 수 있는 적절한 W를 찾기 위해 training을 거칠 것이죠.

106
00:07:19,338 --> 00:07:23,639
따라서 우리는 우리가 원하는 결과를 만들어낼 수 있는 적절한 W를 찾기 위해 training을 거칠 것이죠.

107
00:07:23,639 --> 00:07:28,209
여기서 기억해야 할 것은 매 단계마다 같은 함수와 같은 W를 사용한다는 것입니다.

108
00:07:28,209 --> 00:07:31,778
여기서 기억해야 할 것은 매 단계마다 같은 함수와 같은 W를 사용한다는 것입니다.

109
00:07:31,778 --> 00:07:35,928
그래서 입력이나 출력 시퀀스의 길이를 고려할 필요가 없습니다.

110
00:07:35,928 --> 00:07:38,778
그래서 입력이나 출력 시퀀스의 길이를 고려할 필요가 없습니다.

111
00:07:38,778 --> 00:07:43,528
그래서 입력이나 출력 시퀀스의 길이를 고려할 필요가 없습니다.

112
00:07:43,528 --> 00:07:46,769
RNN을 구현하는 가장 간단한 방법은 Vanilla RNN 입니다.

113
00:07:46,769 --> 00:07:50,309
RNN을 구현하는 가장 간단한 방법은 Vanilla RNN 입니다.

114
00:07:50,309 --> 00:07:54,569
여기서 RNN을 구성하는 것은 단 하나의 hidden state h 입니다.

115
00:07:54,569 --> 00:08:00,569
여기서 RNN을 구성하는 것은 단 하나의 hidden state h 입니다.

116
00:08:00,569 --> 00:08:04,039
그리고 여기 Recurrence(재귀) 식은 각 hidden state를 시간과 현재 input (x_t)로 어떻게 나타낼 수 있는지 알려줍니다.

117
00:08:04,038 --> 00:08:04,688
그리고 여기 Recurrence(재귀) 식은 각 hidden state를 시간과 현재 input (x_t)로 어떻게 나타낼 수 있는지 알려줍니다.

118
00:08:04,689 --> 00:08:08,369
그리고 여기 Recurrence 식은 각 hidden state를 시간과 현재 input (x_t)로 어떻게 나타낼 수 있는지 알려줍니다.

119
00:08:08,369 --> 00:08:10,349
가중치 행렬 W_hh와 W_xh에 직전 단계의 hidden state h 와 input vector x가 각각 곱해지고,

120
00:08:10,348 --> 00:08:15,238
가중치 행렬 W_hh와 W_xh에 직전 단계의 hidden state h_t-1 와 input vector x가 각각 곱해지고,

121
00:08:15,238 --> 00:08:18,238
이것이 tanh 함수에 의해 새로운 hidden state h_t로 결정되는 방식으로 업데이트 됩니다.

122
00:08:18,238 --> 00:08:21,978
이것이 tanh 함수에 의해 새로운 hidden state h_t로 결정되는 방식으로 업데이트 됩니다.

123
00:08:21,978 --> 00:08:26,199
이러한 재귀 식은 h가 시간과 현재 입력에 따라 업데이트되는 함수라는 것을 보여줍니다.

124
00:08:26,199 --> 00:08:29,769
이러한 재귀 식은 h가 시간과 현재 입력에 따라 업데이트되는 함수라는 것을 보여줍니다.

125
00:08:29,769 --> 00:08:34,129
h 바로 다음에 결과물이 행렬의 형태로 출력되는 형태가 가장 간단한 형태의 RNN입니다.

126
00:08:34,129 --> 00:08:37,528
h 바로 다음에 결과물이 행렬의 형태로 출력되는 형태가 가장 간단한 형태의 RNN입니다.

127
00:08:37,528 --> 00:08:42,288
이게 어떻게 작동되는지 간단히 설명드리기 위해 예를 들자면,

128
00:08:42,288 --> 00:08:46,639
이게 어떻게 작동되는지 간단히 설명드리기 위해 예를 들자면,

129
00:08:46,639 --> 00:08:49,299
이런 추상적인 x, h, y 등에 의미를 부여할 수 있습니다.

130
00:08:49,299 --> 00:08:53,059
이런 추상적인 x, h, y 등에 의미를 부여할 수 있습니다.

131
00:08:53,059 --> 00:08:56,149
예를 들어 이러한 문자 수준 언어 모델에 RNN을 적용하는 것 말이죠.

132
00:08:56,149 --> 00:08:59,899
저는 이 예시를 참 좋아합니다. 직관적이고 재밌거든요.

133
00:08:59,899 --> 00:09:04,698
그래서 RNN 기반 문자 수준 언어 모델에서는, RNN에 문자열의 순서를 주고,

134
00:09:04,698 --> 00:09:07,859
그래서 RNN 기반 문자 수준 언어 모델에서는, RNN에 문자열의 순서를 주고,

135
00:09:07,860 --> 00:09:10,899
그래서 RNN 기반 문자 수준 언어 모델에서는, RNN에 문자열의 순서를 주고,

136
00:09:10,899 --> 00:09:14,299
지금까지의 관찰 결과를 바탕으로 각각의 단계에서 다음에 올 문자는 무엇인지 예측하게 합니다.

137
00:09:14,299 --> 00:09:16,909
지금까지의 관찰 결과를 바탕으로 각각의 단계에서 다음에 올 문자는 무엇인지 예측하게 합니다.

138
00:09:16,909 --> 00:09:21,120
간단한 예를 한번 보죠.

139
00:09:21,120 --> 00:09:25,610
여기서 training 문자열 'hello'를 주면,

140
00:09:25,610 --> 00:09:29,870
우리의 현재 어휘 목록에는 'h, e , l, o' 이렇게 4글자가 있겠죠

141
00:09:29,870 --> 00:09:33,289
그러니까 RNN은 우리의 training 문자열 데이터를 바탕으로 다음에 올 글자가 무엇인지 예측하게 됩니다.

142
00:09:33,289 --> 00:09:37,000
구체적으로, h, e, l, o를 각각 순서대로 하나씩 RNN에 입력해 줍니다.

143
00:09:37,000 --> 00:09:40,509
여기서 가로축은 시간입니다. (역자주: 오른쪽으로 갈수록 뒤)

144
00:09:40,509 --> 00:09:47,110
h는 첫번째, e는 두번째, 그다음 l, 그다음 l

145
00:09:47,110 --> 00:09:50,629
여기서는 'one-hot' 표기법을 사용하고 있습니다. (역자주: 0과 1로만 나타내는 것)

146
00:09:50,629 --> 00:09:53,889
여기서는 'one-hot' 표기법을 사용하고 있습니다. (역자주: 0과 1로만 나타내는 것)

147
00:09:53,889 --> 00:09:58,129
그리고 아까 본 재귀 식을 사용합니다.

148
00:09:58,129 --> 00:10:01,860
처음에 h에는 0만 들어가 있습니다.

149
00:10:01,860 --> 00:10:04,720
그래서 매 시간 단계마다 이 재귀 식을 이용해서 hidden state 벡터를 계산합니다.

150
00:10:04,720 --> 00:10:08,790
hidden state에 3개의 (안들림) 가 있습니다.

151
00:10:08,789 --> 00:10:11,099
각 시점에서 이전까지 입력받은 모든 문자들을 요약해서 표현합니다.

152
00:10:11,100 --> 00:10:13,040
각 시점에서 이전까지 입력받은 모든 문자들을 요약해서 표현합니다.

153
00:10:13,039 --> 00:10:15,759
각 시점에서 이전까지 입력받은 모든 문자들을 요약해서 표현합니다.

154
00:10:15,759 --> 00:10:20,159
이런 방법으로 매 시간 단계마다 바로 다음 순서 에 올 문자를 예측할 것입니다.

155
00:10:20,159 --> 00:10:23,139
이런 방법으로 매 시간 단계마다 바로 다음 순서에 올 문자를 예측할 것입니다.

156
00:10:23,139 --> 00:10:27,569
우리는 이 4 개의 문자(역자주: h, e, l, o)를 가지고 있고, 매 시간 단계마다 이 4개의 문자 중 어떤 문자가 오는지 예측할 것입니다.

157
00:10:27,570 --> 00:10:32,100
우리는 이 4 개의 문자(역자주: h, e, l, o)를 가지고 있고, 매 시간 단계마다 이 4개의 문자 중 어떤 문자가 오는지 예측할 것입니다.

158
00:10:32,100 --> 00:10:37,139
제일 처음에는 H를 입력할 것입니다.

159
00:10:37,139 --> 00:10:40,799
RNN은 현재의 weight를 바탕으로 다음에 어떤 문자가 올 지 예측합니다.

160
00:10:40,799 --> 00:10:42,959
RNN은 현재의 weight를 바탕으로 다음에 어떤 문자가 올 지 예측합니다.

161
00:10:42,960 --> 00:10:47,950
현재 normalized 되지 않은 수치로는, (역자주: 맨 위 왼쪽 사각형 안의 숫자) h는 1.0, e는 2.2,

162
00:10:47,950 --> 00:10:52,640
l은 -3.0 , o는 4.1라는 숫자의 정도로 나타날 것입니다.

163
00:10:52,639 --> 00:10:56,409
물론 우리는 이 training sequence에서 h 다음에 e가 온다는 것을 알고 있습니다.

164
00:10:56,409 --> 00:11:00,669
그러니까 여기 초록색으로 적혀 있는 e의 2.2라는 숫자가 정답이 되는 것이죠.

165
00:11:00,669 --> 00:11:04,559
그래서 이 숫자는 커야 하고, 다른 숫자들은 작아져야 합니다.

166
00:11:04,559 --> 00:11:07,799
이처럼 매 시간 단계마다 우리는 다음에 올 타겟 문자를 갖고 있습니다.

167
00:11:07,799 --> 00:11:12,209
타겟에 해당하는 숫자는 커야 하고, 나머지 숫자는 작아야 합니다.

168
00:11:12,210 --> 00:11:15,470
타겟에 해당하는 숫자는 커야 하고, 나머지 숫자는 작아야 합니다.

169
00:11:15,470 --> 00:11:19,950
그래서 이러한 정보는 loss function(손실 함수)의 gradient signal에 포함됩니다.

170
00:11:19,950 --> 00:11:23,220
그리고 그러한 loss 들은 이 연결들은 통해 back-propagation 됩니다.

171
00:11:23,220 --> 00:11:26,600
매 시간 단계에 softmax classifier을 갖고 있다고 합시다.

172
00:11:26,600 --> 00:11:31,300
그래서 매 시간 단계마다 softmax classifier가 다음에 어떤 문자가 와야 할 지를 예측하고,

173
00:11:31,299 --> 00:11:34,269
그리고 모든 loss들은 맨 위(역자주: output layer)부터 거꾸로 그래프를 내려오면서 계산되어서

174
00:11:34,269 --> 00:11:37,879
그리고 모든 loss들은 맨 위(역자주: output layer)부터 거꾸로 그래프를 내려오면서 계산되어서

175
00:11:37,879 --> 00:11:41,179
그리고 모든 loss들은 맨 위(역자주: output layer)부터 거꾸로 그래프를 내려오면서 계산되어서

176
00:11:41,179 --> 00:11:44,479
weight 행렬에 gradient를 주어 적절한 값으로 변화시켜 RNN이 문자를 보다 정확하게 예측하게 합니다.

177
00:11:44,480 --> 00:11:50,039
weight 행렬에 gradient를 주어 적절한 값으로 교정시켜 RNN이 문자를 보다 정확하게 예측하게 합니다.

178
00:11:50,039 --> 00:11:53,599
그러니까 여러분이 RNN에 문자를 입력하면 RNN은 보다 정확한 행동(역자주: 여기서는 문자 예측)을 하는 것이죠.

179
00:11:53,600 --> 00:11:57,750
이제 어떻게 데이터를 학습시키는지에 대해 상상이 좀 갈 거에요.

180
00:11:57,750 --> 00:12:02,879
여기 그림에 대해 질문이 있나요?

181
00:12:02,879 --> 00:12:08,750
(질문): W_xh와 W_hy는 항상 일정한 값을 가지나요?

182
00:12:08,750 --> 00:12:13,320
(답변): W(weight) 들은 매 recurrence 단계 마다 항상 일정한 값을 가집니다.

183
00:12:13,320 --> 00:12:17,010
(답변): W(weight) 들은 매 recurrence 단계 마다 항상 일정한 값을 가집니다.

184
00:12:17,009 --> 00:12:23,830
여기서 우리는 W_xh, W_hh, W_yh를 각각 4번씩 사용했습니다.

185
00:12:23,830 --> 00:12:27,720
여러분이 backpropagation을 할 때, 동일한 weight 행렬에 이러한 gradient 들을 계속 더한다는 것을 명심해야 합니다.

186
00:12:27,720 --> 00:12:30,750
여러분이 backpropagation을 할 때, 동일한 weight 행렬에 이러한 gradient 들을 계속 더한다는 것을 명심해야 합니다.

187
00:12:30,750 --> 00:12:35,879
그리고 이것은 우리가 길이가 다양한 입력값들을 사용할 수 있게 해 줍니다.

188
00:12:35,879 --> 00:12:38,960
그리고 이것은 우리가 길이가 다양한 입력값들을 사용할 수 있게 해 줍니다.

189
00:12:38,960 --> 00:12:48,540
그러니까 정해진 길이의 입력값들을 사용하지 않아도 된다는 것이죠.

190
00:12:48,539 --> 00:12:52,579
(질문): 처음 h_0를 어떻게 초기화하나요?

191
00:12:52,580 --> 00:13:00,650
(답변): 0으로 놓는 것이 가장 일반적입니다.

192
00:13:00,649 --> 00:13:01,289
(질문): 입력값의 순서는 영향을 미치나요?

193
00:13:01,289 --> 00:13:11,299
(질문): 입력값의 순서는 영향을 미치나요?
194
00:13:11,299 --> 00:13:14,359
(답변): 여기서는 중요하지 않습니다. hidden state는 지금까지 들어온 모든 값을 반영하거든요.

195
00:13:14,360 --> 00:13:17,870
(답변): 여기서는 중요하지 않습니다. hidden state는 지금까지 들어온 모든 값을 반영하거든요.

196
00:13:17,870 --> 00:13:21,299
(답변): 여기서는 중요하지 않습니다. hidden state는 지금까지 들어온 모든 값을 반영하거든요.

197
00:13:21,299 --> 00:13:26,859
(답변): 여기서는 중요하지 않습니다. hidden state는 지금까지 들어온 모든 값을 반영하거든요.

198
00:13:26,860 --> 00:13:31,590
보다 구체적인 예들로 확실히 설명드리겠습니다.

199
00:13:31,590 --> 00:13:36,149
문자 단위의 언어 모델 코드는 매우 간단합니다.

200
00:13:36,149 --> 00:13:38,980
여러분들이 나중에 찾아볼 수 있게 GitHub에 올려 놓았어요.

201
00:13:38,980 --> 00:13:43,350
이것은 NumPy 기반의 100줄 길이의 문자 단위 RNN 코드입니다.

202
00:13:43,350 --> 00:13:47,220
이것은 NumPy 기반의 100줄 길이의 문자 단위 RNN 코드입니다.

203
00:13:47,220 --> 00:13:49,840
실제로 RNN이 어떻게 학습하는지를 알기 위해서 이 코드를 단계별로 살펴볼게요.

204
00:13:49,840 --> 00:13:53,220
실제로 RNN이 어떻게 학습하는지를 알기 위해서 이 코드를 단계별로 살펴볼게요.

205
00:13:53,220 --> 00:13:58,250
코드를 블록들로 나누어 하나하나 살펴보겠습니다.

206
00:13:58,250 --> 00:14:02,389
처음에는 보다시피 NumPy만 사용합니다.

207
00:14:02,389 --> 00:14:05,569
여기에 우리가 입력받을 것은 문자들의 대용량 순서 .txt 데이터입니다.

208
00:14:05,570 --> 00:14:10,090
여기에 우리가 입력받을 것은 문자들의 대용량 순서 .txt 데이터입니다.

209
00:14:10,090 --> 00:14:14,810
이 파일의 모든 문자를 읽어들이고, mapping dictionary를 생성합니다.

210
00:14:14,809 --> 00:14:18,179
mapping dictionary는 문자에 index를 대응시키고, 또 반대로 index에 문자를 대응시킵니다.

211
00:14:18,179 --> 00:14:23,120
그러니까 문자를 순서대로 배열하는 것입니다.

212
00:14:23,120 --> 00:14:27,350
여기 보면 아주 긴 문자열이 들어 있는 큰 데이터를 읽어들이네요.

213
00:14:27,350 --> 00:14:30,860
우리는 이 데이터를 배열해서 각 문자에 index를 지정할 것입니다.

214
00:14:30,860 --> 00:14:36,300
그리고 여기에 보다시피 initialization(초깃값 설정)을 하게 됩니다.

215
00:14:36,299 --> 00:14:39,899
hidden size(hidden state의 크기)는 hyperparameter(바뀌지 않는 값) 입니다. 여기서는 100으로 설정했습니다.

216
00:14:39,899 --> 00:14:43,100
hidden size(hidden state의 크기)는 hyperparameter(바뀌지 않는 값) 입니다. 여기서는 100으로 설정했습니다.

217
00:14:43,100 --> 00:14:46,720
여기 있는 건 learning rate 이고요.

218
00:14:46,720 --> 00:14:51,019
25가 지정되어 있는 seq_length는 여러분이 RNN을 공부하다 보면 나오는 parameter 입니다.

219
00:14:51,019 --> 00:14:53,899
많은 경우 우리의 입력 데이터는 너무 커서 RNN에 한꺼번에 넣을 수가 없습니다.

220
00:14:53,899 --> 00:14:56,870
이것은 우리가 backpropagation을 하는 동안 메모리에 데이터를 저장해 두어야 하는데 여기에 한계가 있기 때문이죠

221
00:14:56,870 --> 00:15:00,070
이것은 우리가 backpropagation을 하는 동안 메모리에 데이터를 저장해 두어야 하는데 여기에 한계가 있기 때문이죠

222
00:15:00,070 --> 00:15:03,540
이것은 우리가 backpropagation을 하는 동안 메모리에 데이터를 저장해 두어야 하는데 여기에 한계가 있기 때문이죠

223
00:15:03,539 --> 00:15:07,139
그래서 우리는 입력 데이터를 몇 개의 데이터로 쪼개고, 여기서는 길이가 25인 데이터들로 쪼갰습니다.

224
00:15:07,139 --> 00:15:09,230
그래서 우리는 입력 데이터를 몇 개의 데이터로 쪼개고, 여기서는 길이가 25인 데이터들로 쪼갰습니다.

225
00:15:09,230 --> 00:15:14,769
그러니까 한 번에 처리할 문자의 개수가 25개인 것입니다.

226
00:15:14,769 --> 00:15:19,509
다시 설명하면, 한 번에 backpropagation 하는 문자의 개수가 25인 것이고,

227
00:15:19,509 --> 00:15:22,149
한 번에 모든 데이터를 기억해서 backpropagation 할 수 없기 때문에, 하나의 크기가 25개인 덩어리 데이터들로 나누어서 처리합니다.

228
00:15:22,149 --> 00:15:26,899
한 번에 모든 데이터를 기억해서 backpropagation 할 수 없기 때문에, 하나의 크기가 25개인 덩어리 데이터들로 나누어서 처리합니다.

229
00:15:26,899 --> 00:15:30,789
여기 보이는 행렬들은 random 함수를 이용해서 초기값이 무작위적으로 입력됩니다.

230
00:15:30,789 --> 00:15:34,709
Wxh, Whh, Wxy은 모두 우리가 backpropagation을 통해 학습시킬 대상들입니다.

231
00:15:34,710 --> 00:15:36,790
Wxh, Whh, Wxy은 모두 우리가 backpropagation을 통해 학습시킬 대상들입니다.

232
00:15:36,789 --> 00:15:40,699
loss function은 넘어가고 맨 밑 부분을 살펴보겠습니다.

233
00:15:40,700 --> 00:15:44,020
이 부분은 Main loop입니다. 이 중에서 몇 부분을 한번 살펴보죠.

234
00:15:44,019 --> 00:15:48,399
이 부분에서 어떤 변수들에 0을 대입하는 초기화가 진행됩니다.

235
00:15:48,399 --> 00:15:50,829
그리고 계속해서 loop을 돌리게 되죠.

236
00:15:50,830 --> 00:15:54,960
우리가 지금 보고 있는 것은 전체 데이터의 한 batch 입니다.

237
00:15:54,960 --> 00:15:58,970
전체 데이터 세트에서 크기 25의 문자 batch를 가지를 list input으로 넣어줍니다.

238
00:15:58,970 --> 00:16:03,019
그리고 그 list input은 각 문자에 대응되는 25개의 숫자를 갖고 있습니다.

239
00:16:03,019 --> 00:16:06,919
타겟들은 여기 index에 1을 더한 값이 되는데요,

240
00:16:06,919 --> 00:16:09,909
이것은 타겟들이 현재 순서가 아니라 바로 다음 순서에 나올 문자들이기 때문에 그렇습니다.

241
00:16:09,909 --> 00:16:15,269
그러니까 list input에는 25개의 문자에 대응되는 25개의 숫자가 있고, 타겟 문자는 그 숫자들에서 1을 더한 index에 대응되는 문자들입니다.

242
00:16:15,269 --> 00:16:20,689
그러니까 list input에는 25개의 문자에 대응되는 25개의 숫자가 있고, 타겟 문자는 그 숫자들에서 1을 더한 index에 대응되는 문자들입니다.

243
00:16:20,690 --> 00:16:26,480
이것은 sampling 코드입니다.

244
00:16:26,480 --> 00:16:30,659
매 시간 단계에서 RNN을 학습시키면서, 현재 RNN이 어떻게 사고하고 있는지 알아보기 위한 sample을 출력합니다.

245
00:16:30,659 --> 00:16:35,370
매 시간 단계에서 RNN을 학습시키면서, 현재 RNN이 어떻게 사고하고 있는지 알아보기 위한 sample을 출력합니다.

246
00:16:35,370 --> 00:16:40,320
우리가 문자 단위의 RNN을 사용할 때에는

247
00:16:40,320 --> 00:16:43,570
RNN이 매 시간 단계마다 바로 다음에 올 문자들의 순서를 출력합니다.

248
00:16:43,570 --> 00:16:46,379
그러니까 sampling 후 그것을 다시 입력값으로 주고, 다음 sample을 또다시 입력값으로 주는 방식으로 모든 sample을 입력한 다음,

249
00:16:46,379 --> 00:16:49,259
그러니까 sampling 후 그것을 다시 입력값으로 주고, 다음 sample을 또다시 입력값으로 주는 방식으로 모든 sample을 입력한 다음,

250
00:16:49,259 --> 00:16:52,769
그러니까 sampling 후 그것을 다시 입력값으로 주고, 다음 sample을 또다시 입력값으로 주는 방식으로 모든 sample을 입력한 다음,

251
00:16:52,769 --> 00:16:56,549
RNN에게 추상적인 문자열을 만들라고 지시할 수 있게 됩니다.

252
00:16:56,549 --> 00:17:00,549
이게 이 코드의 기능이고, 이것은 조금 있다 살펴볼 sample function을 사용합니다.

253
00:17:00,549 --> 00:17:04,250
여기서는 loss function을 불러옵니다.

254
00:17:04,250 --> 00:17:09,160
loss function은 입력값, 타겟 문자, hprev 을 입력받습니다.

255
00:17:09,160 --> 00:17:13,900
hprev는 h from previous chunk 을 뜻합니다.

256
00:17:13,900 --> 00:17:18,179
우리가 크기가 25인 batch들을 사용하는데,

257
00:17:18,179 --> 00:17:22,400
hidden state에서는 바로 전 batch의 마지막 문자가 무엇인지에 대한 정보가 필요하고, 이 마지막 문자를 다음 batch의 첫 h 에 입력하게 됩니다.

258
00:17:22,400 --> 00:17:26,140
그러니까 h가 batch 에서 그 다음 batch 로 제대로 넘어가기 위해서 h prev을 사용하는 것입니다.

259
00:17:26,140 --> 00:17:30,700
그리고 그 h prev는 backpropagation 할 때만 사용됩니다.

260
00:17:30,700 --> 00:17:35,558
그 h prev을 loss fuction에 입력하면, loss, gradient, weight 행렬, 그리고 bias를 출력합니다.

261
00:17:35,558 --> 00:17:39,319
그 h prev을 loss fuction에 입력하면, loss, gradient, weight 행렬, 그리고 bias를 출력합니다.

262
00:17:39,319 --> 00:17:44,149
여기에서 loss를 print 하고, 여기에선 parameter들을 loss function이 하라는 대로 업데이트합니다.

263
00:17:44,150 --> 00:17:47,429
실제로 업데이트가 되는 것은 여기 adagrad update 라고 적혀 있는 부분이네요.

264
00:17:47,429 --> 00:17:53,100
여기 gradient 계산을 위한 변수들을 제곱한 값들을 계속 더해 줍니다.

265
00:17:53,099 --> 00:17:56,819
그리고 이것들로 adagrad를 업데이트 하죠.

266
00:17:56,819 --> 00:18:00,639
이제 loss funcion을 살펴보겠습니다.

267
00:18:00,640 --> 00:18:05,790
이 블록이 loss fuction이고, foward와 backward 방법들로 이루어져 있습니다.

268
00:18:05,789 --> 00:18:08,990
처음에는 forward pass, 나중에는 초록색으로 적혀 있는 backward pass를 수행합니다.

269
00:18:08,990 --> 00:18:13,130
처음에는 forward pass, 나중에는 초록색으로 적혀 있는 backward pass를 수행합니다.

270
00:18:13,130 --> 00:18:18,919
forward pass에서는 input을 target을 향하게 만듭니다.

271
00:18:18,919 --> 00:18:23,360
여기서 25개의 index를 받지만, 반복문을 25번 실행하는 것이 아니라,

272
00:18:23,359 --> 00:18:27,500
여기 있는 성분이 모두 0인 input vector에 one-hot 인코딩을 하게 됩니다.

273
00:18:27,500 --> 00:18:32,169
그러니까 input에 대응되는 bit를 1로 지정하는 것이죠.

274
00:18:32,169 --> 00:18:34,110
one hot encoding을 이용해서 input을 주고,

275
00:18:34,109 --> 00:18:39,229
밑에 있는 recurrence 공식을 이용해서 계산합니다.

276
00:18:39,230 --> 00:18:42,210
hs[t]는 매 시간 단계의 모든 값들을 기록합니다.

277
00:18:42,210 --> 00:18:46,910
recurrence 공식과 이 두 줄의 코드를 통해 hidden state vector과 output vector 을 계산합니다.

278
00:18:46,910 --> 00:18:50,779
여기서는 softmax function을 이용해서 normalization을 구현합니다.

279
00:18:50,779 --> 00:18:54,440
softmax function에서의 loss는 정답(역자주: 타겟 문자)이 나올 확률의 log를 취하고 거기에 -1을 곱한 값입니다.(역자주: cross entropy loss)

280
00:18:54,440 --> 00:18:58,190
softmax function에서의 loss는 정답(역자주: 타겟 문자)이 나올 확률의 log를 취하고 거기에 -1을 곱한 값입니다.(역자주: cross entropy loss)

281
00:18:58,190 --> 00:19:02,779
지금까지 forward pass 를 살펴보았고, 이제 그래프를 통해 backpropagation을 살펴보겠습니다.

282
00:19:02,779 --> 00:19:06,899
backward pass에서는, 25번째 문자에서 첫번째 문자까지 거슬러 올라갑니다.

283
00:19:06,900 --> 00:19:08,530
backward pass에서는, 25번째 문자에서 첫번째 문자까지 거슬러 올라갑니다.

284
00:19:08,529 --> 00:19:12,899
backward pass에서는, 25번째 문자에서 첫번째 문자까지 거슬러 올라갑니다.

285
00:19:12,900 --> 00:19:16,509
여기서는 softmax, activation 등을 통한 backpropagation이 수행됩니다.

286
00:19:16,509 --> 00:19:19,089
그리고 모든 gradient와 parameter들을 더해주죠.

287
00:19:19,089 --> 00:19:23,379
한 가지 짚고 넘어갈 점은, Whh를 비롯한 행렬에서의 gradient 계산에서 '+='을 사용하고 있다는 것입니다.

288
00:19:23,380 --> 00:19:27,210
한 가지 짚고 넘어갈 점은, Whh를 비롯한 행렬에서의 gradient 계산에서 '+='을 사용하고 있다는 것입니다.

289
00:19:27,210 --> 00:19:31,210
한 가지 짚고 넘어갈 점은, Whh를 비롯한 행렬에서의 gradient 계산에서 '+='을 사용하고 있다는 것입니다.

290
00:19:31,210 --> 00:19:34,590
우리는 매 시간 단계마다 weight 행렬들이 gradient를 받고, 이 값들을 모두 더해 주어야 하기 때문에, 이 행렬을 계속 쓰게 됩니다.

291
00:19:34,589 --> 00:19:37,449
우리는 매 시간 단계마다 weight 행렬들이 gradient를 받고, 이 값들을 모두 더해 주어야 하기 때문에, 이 행렬을 계속 쓰게 됩니다.

292
00:19:37,450 --> 00:19:43,980
그리고 계속해서 backpropagation을 하게 되죠.

293
00:19:43,980 --> 00:19:48,130
여기에서 나온 gradient는 loss function에 사용되고, 결국 parameter를 업데이트하게 됩니다.

294
00:19:48,130 --> 00:19:52,580
마지막으로 sampling function입니다.

295
00:19:52,579 --> 00:19:55,960
여기서 RNN을 지금까지 학습한 training 데이터를 바탕으로 실제로 새로운 문자열 데이터를 출력하게 됩니다.

296
00:19:55,960 --> 00:19:59,058
여기서 RNN을 지금까지 학습한 training 데이터를 바탕으로 실제로 새로운 문자열 데이터를 출력하게 됩니다.

297
00:19:59,058 --> 00:20:02,048
여기서 문자열을 초기화해주었고,

298
00:20:02,048 --> 00:20:06,759
피곤해질 때까지 (역자주: 미리 설정한 recurrence가 끝날 때까지) 다음 작업들을 반복합니다.

299
00:20:06,759 --> 00:20:09,289
recurrence 공식 실행, 각 문자에 대한 확률분포 계산, 샘플링, one-hot 인코딩, 그리고 그 결과물을 다음 시간 단계로 재입력

300
00:20:09,289 --> 00:20:10,450
recurrence 공식 실행, 각 문자에 대한 확률분포 계산, 샘플링, one-hot 인코딩, 그리고 그 결과물을 다음 시간 단계로 재입력

301
00:20:10,450 --> 00:20:15,640
recurrence 공식 실행, 각 문자에 대한 확률분포 계산, 샘플링, one-hot 인코딩, 그리고 그 결과물을 다음 시간 단계로 재입력

302
00:20:15,640 --> 00:20:22,460
이 작업들을 충분히 많은 문자열을 출력할 때까지 계속 수행합니다.

303
00:20:22,460 --> 00:20:27,190
(질문: 안들림 => 답변) 우리는 매 batch 마다 25개의 softmax classifier를 갖고 있습니다.

304
00:20:27,190 --> 00:21:04,680
(답변) 그 classifier 들은 한번에 backpropagation을 진행하고, 반대방향으로 모든 결과물들을 더해주죠.

305
00:21:04,680 --> 00:21:14,910
그게 우리가 이걸 쓰는 이유죠. 다음 질문?

306
00:21:14,910 --> 00:21:19,259
(질문) 여기서 regularization을 쓰나요?

307
00:21:19,259 --> 00:21:23,720
(답변) 여기서는 빠져 있습니다. 일반적으로 RNN에서는 다른 알고리즘만큼 regularization이 흔하게 적용되지는 않습니다.

308
00:21:23,720 --> 00:21:27,269
(답변) 여기서는 빠져 있습니다. 일반적으로 RNN에서는 다른 알고리즘만큼 regularization이 흔하게 적용되지는 않습니다.

309
00:21:27,269 --> 00:21:38,379
(답변) 가끔 아주 좋지 않은 결과를 낳기도 해서, 저는 그냥 사용하지 않을 때도 있습니다. 일종의 hyperparameter이죠. 다음 질문? (질문 안들림)

310
00:21:38,380 --> 00:21:48,260
(답변) 여기서의 문자들은 아주 기초적인 수준입니다. 그래서 실제로 이런 문자가 존재하는지 별로 신경쓰지는 않아요.

311
00:21:48,259 --> 00:21:51,839
(답변) 여기서의 문자들은 아주 기초적인 수준입니다. 그래서 실제로 이런 문자가 존재하는지 별로 신경쓰지는 않아요.

312
00:21:51,839 --> 00:21:56,289
문자들의 index와 그것들의 순서 정도만을 고려할 뿐이죠.

313
00:21:56,289 --> 00:21:58,569
다음 질문?

314
00:21:58,569 --> 00:22:08,009
(질문) space 대신 일정한 segment size(25)를 이용하는 이유가 있나요?

315
00:22:08,009 --> 00:22:13,460
(질문) space 대신 일정한 segment size(25)를 이용하는 이유가 있나요?

316
00:22:13,460 --> 00:22:18,630
(답변) 크기가 25인 batch 말고 space로 구분하는 것 역시 가능할 것 같습니다. 하지만 거기에는 언어에 대한 특별한 가정이 필요해서 권장되지 않아요.

317
00:22:18,630 --> 00:22:22,530
자세한 이유는 좀 있다가 살펴보도록 하겠습니다.

318
00:22:22,529 --> 00:22:25,359
이 코드에는 어떤 문자열도 입력할 수 있어요. 이걸 갖고 여러 가지를 해 볼게요.

319
00:22:25,359 --> 00:22:31,539
여기 우리가 출처를 모르는 어떤 문자열이 있습니다.

320
00:22:31,539 --> 00:22:34,889
그리고 이 문자열을 RNN에 학습시키고, RNN이 문자열을 만들어내게 할 거에요.

321
00:22:34,890 --> 00:22:40,670
예를 들어, 셰익스피어의 모든 작품을 입력할 수 있습니다.

322
00:22:40,670 --> 00:22:44,789
크기가 좀 크긴 하지만, 이건 단지 문자열일 뿐이에요.

323
00:22:44,789 --> 00:22:48,289
크기가 좀 크긴 하지만, 이건 단지 문자열일 뿐이에요.

324
00:22:48,289 --> 00:22:51,909
RNN 셰익스피어의 작품을 학습시키고, 셰익스피어의 시에서의 다음 문자를 예측하게끔 할 수 있습니다.

325
00:22:51,910 --> 00:22:54,650
처음에는 학습이 되어 있지 않기 때문에, 결과물들은 매우 무작위적인 문자열입니다.

326
00:22:54,650 --> 00:22:59,030
처음에는 학습이 되어 있지 않기 때문에, 결과물들은 매우 무작위적인 문자열입니다.

327
00:22:59,029 --> 00:23:03,200
하지만 학습을 통해 RNN은 이 문자열 안에는 단어들이 있고, 단어들 사이에 space가 있고, 쌍따옴표(")의 사용법을 이해하기 되죠.

328
00:23:03,200 --> 00:23:06,930
하지만 학습을 통해 RNN은 이 문자열 안에는 단어들이 있고, 단어들 사이에 space가 있고, 쌍따옴표(")의 사용법을 이해하기 되죠.

329
00:23:06,930 --> 00:23:11,490
하지만 학습을 통해 RNN은 이 문자열 안에는 단어들이 있고, 단어들 사이에 space가 있고, 쌍따옴표(")의 사용법을 이해하기 되죠.

330
00:23:11,490 --> 00:23:16,420
그리고 'here', 'on', 'and so on' 과 같은 기본적인 표현들을 알게 됩니다.

331
00:23:16,420 --> 00:23:18,820
그리고 RNN을 계속 학습시킬수록, 이러한 표현들이 점점 정제되는 것을 확인할 수 있습니다.

332
00:23:18,819 --> 00:23:22,609
예를 들어 "를 한번 사용하면 "를 한번 더 사용해서 인용구를 닫아 주는 것들을 익히는 거죠.

333
00:23:22,609 --> 00:23:26,379
또 문장이 마침표로 끝나는 것 역시 따로 가르치지 않고도 패턴만으로 익히게 됩니다.

334
00:23:26,380 --> 00:23:29,630
또 문장이 마침표로 끝나는 것 역시 따로 가르치지 않고도 통계적 패턴만으로 익히게 됩니다.

335
00:23:29,630 --> 00:23:30,580
그리고 마침내 '셰익스피어 문학' 자체를 생성할 수 있게 되죠.

336
00:23:30,579 --> 00:23:34,349
여기 RNN이 만들어낸 작품을 읽어볼게요.

337
00:23:34,349 --> 00:23:38,740
(읽는 중) "Alas, I think he shall come approached and the day..."

338
00:23:38,740 --> 00:23:42,900
(읽는 중) "Alas, I think he shall come approached and the day..."

339
00:23:42,900 --> 00:23:45,460
(읽는 중) "Alas, I think he shall come approached and the day..."

340
00:23:45,460 --> 00:23:56,909
(질문) 하지만 이것들은 25개가 넘는 문자로 이루어진 문장은 기억할 수가 없기 때문에 제대로 생성할 수 없죠?

341
00:23:56,909 --> 00:24:02,679
(답변) 네 맞습니다. 그거 사실 되게 알아차리기 힘든 부분이라 제가 나중에 말하려고 했었어요.

342
00:24:02,679 --> 00:24:05,980
우리는 셰익스피어 작품이 아니라 다른 것들에도 이것을 활용할 수 있습니다.

343
00:24:05,980 --> 00:24:08,960
이것들은 제가 Justin과 작년에 만들어본 것들입니다.

344
00:24:08,960 --> 00:24:12,990
Justin은 한 대수기하학 책의 LaTeX 소스를 RNN에 학습시켰습니다.

345
00:24:12,990 --> 00:24:18,069
Justin은 한 대수기하학 책의 LaTeX 소스를 RNN에 학습시켰습니다.

346
00:24:18,069 --> 00:24:23,398
그리고 RNN은 수학책을 집필했죠.

347
00:24:23,398 --> 00:24:27,199
물론 RNN은 LaTeX 형식으로 결과물을 출력하지 않아서 저희가 약간 손봐주긴 했지만,

348
00:24:27,200 --> 00:24:30,009
물론 RNN은 LaTeX 형식으로 결과물을 출력하지 않아서 저희가 약간 손봐주긴 했지만,

349
00:24:30,009 --> 00:24:33,890
어쨌든 한두 번 손보고 나니 보시는 바와 같이 수학책이 되었어요.

350
00:24:33,890 --> 00:24:37,200
어쨌든 한두 번 손보고 나니 보시는 바와 같이 수학책이 되었어요.

351
00:24:37,200 --> 00:24:42,460
살펴보면, RNN은 proof(정리)를 쓰는 방법을 배웠네요. 수학적 정리의 끝에는 저렇게 사각형을 쓰죠.

352
00:24:42,460 --> 00:24:47,090
lemma(소정리)를 비롯한 다른 것들도 만들어 냈고요.

353
00:24:47,089 --> 00:24:52,428
그림을 그리는 방법도 배웠네요.

354
00:24:52,429 --> 00:24:56,720
제가 가장 좋아하는 부분은 여기 왼쪽 상단에 있는 "Proof. Omitted" 부분입니다.

355
00:24:56,720 --> 00:24:59,650
RNN도 귀찮았나 봐요 (웃음)

356
00:24:59,650 --> 00:25:05,780
RNN도 귀찮았나 봐요 (웃음)

357
00:25:05,779 --> 00:25:12,480
전반적으로 보면 RNN은 꽤 대수기하학책 같이 보이는 걸 만들어 냈어요.

358
00:25:12,480 --> 00:25:16,160
뭐 세부적인 부분은 제가 대수기하를 잘 몰라서 말하기 그렇지만, 전반적으로 괜찮아요.

359
00:25:16,160 --> 00:25:19,529
저는 이어서 문자 단위 RNN으로 표현할 수 있는 가장 어렵고 추상적인 것들이 무엇이 있을까 생각했고,

360
00:25:19,529 --> 00:25:22,769
소스 코드에 생각이 미쳤습니다.

361
00:25:22,769 --> 00:25:27,879
그래서 리누스 토발즈의 GitHub에 들어가 리눅스의 모든 C 코드를 가져왔습니다.

362
00:25:27,880 --> 00:25:30,850
이 C 코드는 자그마치 700MB나 됩니다.

363
00:25:30,849 --> 00:25:35,079
이 코드를 RNN에게 학습시켰고, RNN은 코드를 생성해 냈습니다.

364
00:25:35,079 --> 00:25:39,849
이게 바로 RNN이 생성해낸 코드입니다.

365
00:25:39,849 --> 00:25:42,949
살펴보면 함수를 생성했고, 변수를 지정하고, 문법적 오류가 거의 없습니다.

366
00:25:42,950 --> 00:25:47,460
변수를 어떻게 사용하는지도 아는 것 같고,

367
00:25:47,460 --> 00:25:53,230
indentation (들여쓰기)도 적절히 했고, 주석도 달았습니다.

368
00:25:53,230 --> 00:25:58,089
괄호를 열고 닫지 않는 등의 실수를 찾아보기가 매우 힘들었습니다.

369
00:25:58,089 --> 00:26:01,808
이런 것들은 RNN이 배우기 가장 쉬운 것들 중 하나거든요.

370
00:26:01,808 --> 00:26:04,058
RNN의 실수들 중에는 쓰이지 않을 변수를 선언하거나, 선언하지도 않은 변수를 불러오기를 시도는 것들이 있었습니다.

371
00:26:04,058 --> 00:26:07,240
RNN의 실수들 중에는 쓰이지 않을 변수를 선언하거나, 선언하지도 않은 변수를 불러오기를 시도는 것들이 있었습니다.

372
00:26:07,240 --> 00:26:09,929
그러니까 아직 매우 높은 단계의 코딩 수준에는 도달하지 못한 거죠.

373
00:26:09,929 --> 00:26:12,509
하지만 그런 것들을 제외하고 보면 꽤 코딩을 잘 했습니다.

374
00:26:12,509 --> 00:26:17,460
새로운 GPU 라이센스에 관한 주석을 다는 방법도 배웠네요.

375
00:26:17,460 --> 00:26:22,009
새로운 GPU 라이센스에 관한 주석을 다는 방법도 배웠네요.

376
00:26:22,009 --> 00:26:25,779
GPL 라이센스 다음에는 #include, 매크로 코드 등이 오는 것도 배웠고요.

377
00:26:25,779 --> 00:26:33,879
(질문) 이건 (아까 보여준) min char-rnn 으로 만들어낸 건가요?

378
00:26:33,880 --> 00:26:37,169
(답변) min char-rnn은 그냥 작동 원리를 알려주기 위해 만들어낸 장난감 같은 거고,

379
00:26:37,169 --> 00:26:41,230
(답변) 실제로는 min char-rnn의 확장판인 torch 기반 char-rnn을 으로 구현했고, GPU를 이용해서 처리했습니다.

380
00:26:41,230 --> 00:26:45,009
(답변) 실제로는 min char-rnn의 확장판인 torch 기반 char-rnn을 으로 구현했고, GPU를 이용해서 처리했습니다.

381
00:26:45,009 --> 00:26:49,269
이 부분은 수업 마지막 부분에 다룰 것인데, 3-layer LSTM 이라는 것입니다.

382
00:26:49,269 --> 00:26:52,289
이건 RNN의 복잡한 버전이라고 생각하면 됩니다.

383
00:26:52,289 --> 00:26:58,839
좀 더 이해가 쉽도록 예를 들어 볼게요.

384
00:26:58,839 --> 00:27:02,089
이건 작년에 저희가 이런 것들을 가지고 만들어본 것들입니다.

385
00:27:02,089 --> 00:27:08,949
저희는 문자 단위 RNN에 신경과학적으로 접근을 해 보았습니다.

386
00:27:08,950 --> 00:27:13,110
hidden state 내부 특정 cell의 excitement(흥분) 여부에 따라 색을 칠해 봤습니다.

387
00:27:13,109 --> 00:27:17,119
hidden state 내부 특정 cell의 excitement(흥분) 여부에 따라 색을 칠해 봤습니다.

388
00:27:17,119 --> 00:27:18,699
hidden state 내부 특정 cell의 excitement(흥분) 여부에 따라 색을 칠해 봤습니다.

389
00:27:18,700 --> 00:27:23,470
보시다시피, hidden state의 뉴런들의 상태를 해석하는 일이 쉽지가 않습니다.

390
00:27:23,470 --> 00:27:27,110
보시다시피, hidden state의 뉴런들의 상태를 해석하는 일이 쉽지가 않습니다.

391
00:27:27,109 --> 00:27:29,829
왜냐하면 어떤 뉴런들은 매우 낮은 단계에서의 작업을 맡거든요.

392
00:27:29,829 --> 00:27:33,859
예를 들면, 'h 다음에 e가 얼마나 자주 오는가' 가 있네요.

393
00:27:33,859 --> 00:27:37,928
하지만 어떤 cell 들은 해석하기가 꽤 용이했습니다.

394
00:27:37,929 --> 00:27:41,830
여기 보시는 것은 인용구 검출 cell 입니다.

395
00:27:41,829 --> 00:27:46,460
이 cell은 처음 따옴표가 나오면 켜지고, 따옴표가 다시 나타나면 꺼집니다.

396
00:27:46,460 --> 00:27:50,610
이건 그냥 backpropagation의 결과로 나온 것입니다.

397
00:27:50,609 --> 00:27:54,329
RNN은 문자열의 길이가 따옴표들의 사이에 있을때와 따옴표 바깥에 있을 때에 다르다는 것을 파악했습니다.

398
00:27:54,329 --> 00:27:57,639
그래서 hidden state의 특정 부분들을 현재 문자들이 인용구 안에 있는지 파악하게 했습니다.

399
00:27:57,640 --> 00:28:00,650
그래서 hidden state의 특정 부분들을 현재 문자들이 인용구 안에 있는지 파악하게 했습니다.

400
00:28:00,650 --> 00:28:05,159
이것이 아까 (질문했던 사람)의 질문에 답을 해줄 것 같은데요,

401
00:28:05,159 --> 00:28:06,500
이 RNN의 seq_length는 100 이었습니다.(역자주: batch 크기가 100)

402
00:28:06,500 --> 00:28:10,269
하지만 실제로 이 인용구들의 크기를 재어 보면 100보다 훨씬 길다는 것을 알 수 있습니다.

403
00:28:10,269 --> 00:28:16,220
제가 보기에 대략 250정도 인 것 같네요.

404
00:28:16,220 --> 00:28:20,190
그러니까 우리는 한 번에 크기가 100인 backpropagation만을 진행했고, RNN에게는 그때만이 유일한 학습 기회입니다.

405
00:28:20,190 --> 00:28:23,460
그러니까 문자열 크기가 100이 넘어가면 그 앞뒤의 dependencies(종속성, 관계) 에 대해서는 직접적으로 학습하지를 않습니다.
406
00:28:23,460 --> 00:28:27,809
그러니까 문자열 크기가 100이 넘어가면 그 앞뒤의 dependencies(종속성, 관계) 에 대해서는 직접적으로 학습하지를 않습니다.

407
00:28:27,809 --> 00:28:31,159
하지만 이 결과는 실제 문자열의 길이보다 작은 크기의 batch 들로 학습한다고 해도, batch 크기보다 긴 문자열에 대해서도 잘 작동할 수 있다는 것을 보여주네요.

408
00:28:31,160 --> 00:28:36,580
하지만 이 결과는 실제 문자열의 길이보다 작은 크기의 batch 들로 학습한다고 해도, batch 크기보다 긴 문자열에 대해서도 잘 작동할 수 있다는 것을 보여주네요.

409
00:28:36,579 --> 00:28:39,859
그러니까 batch 크기는 100이었지만,

410
00:28:39,859 --> 00:28:44,759
크기가 수백이 넘는 문자열의 dependecies 도 잘 잡아낸 것이죠.

411
00:28:44,759 --> 00:28:48,890
이것은 톨스토이의 <전쟁과 평화> 데이터 입니다.

412
00:28:48,890 --> 00:28:52,460
이 데이터 세트는 대략 80문자마다 한 번 줄이 바뀝니다.

413
00:28:52,460 --> 00:28:57,819
이 데이터 세트는 대략 80문자마다 한 번 줄이 바뀝니다.

414
00:28:57,819 --> 00:29:02,470
그리고 우리는 줄 길이 tracking cell을 찾아냈습니다.

415
00:29:02,470 --> 00:29:06,539
이 cell은 줄이 처음 시작하면 1로 시작해서, 문자열이 진행될수록 천천히 그 값이 감소합니다.

416
00:29:06,539 --> 00:29:09,019
RNN은 현재 자신이 어느 시간 단계에 있는지 알아야 하기 때문에 이 기능은 매우 유용합니다.

417
00:29:09,019 --> 00:29:13,059
RNN은 현재 자신이 어느 시간 단계에 있는지 알아야 하기 때문에 이 기능은 매우 유용합니다.

418
00:29:13,059 --> 00:29:15,149
이를 통해서 언제 줄을 바꾸어야 하는지 알 수 있기 때문이죠.

419
00:29:15,150 --> 00:29:19,280
이것 말고도 if 문을 감지하는 cell도 찾아냈고,

420
00:29:19,279 --> 00:29:23,970
인용구과 주석을 감지하는 cell 도 찾아냈고,

421
00:29:23,970 --> 00:29:28,710
상대적으로 deep한 코드를 감지하는 cell 도 찾아냈습니다.

422
00:29:28,710 --> 00:29:33,150
다른 역할을 수행하는 cell 들도 찾을 수 있을 것이고, 중요한 것은 이것들이 전부 backpropagation 에서 나왔다는 겁니다.

423
00:29:33,150 --> 00:29:36,710
되게 마법같은 일이죠.

424
00:29:36,710 --> 00:29:42,130
(질문) 어떻게 cell 하나하나가 흥분했는지 알 수 있었죠?

425
00:29:42,130 --> 00:29:49,110
(답변) 이 LSTM 에서는 대략 2100개의 cell 들이 있었습니다. 저는 그냥 하나하나 다 살펴봤어요.

426
00:29:49,109 --> 00:29:54,589
(답변) 대부분은 규칙을 찾기가 어려웠지만, 약 5%에 해당하는 cell들에 대해서 살펴본 것들과 같은 규칙을 찾을 수 있었습니다.

427
00:29:54,589 --> 00:30:00,429
(질문) 그러니까 어떤 cell들은 켜고, 어떤 cell들은 끄는 방식으로 찾은 건가요?

428
00:30:00,430 --> 00:30:05,310
(답변) 오 제가 질문을 잘못 이해했었네요. 저희는 RNN 전체를 실행시켰고, 특정 hidden state의 흥분 상태를 관찰했습니다.

429
00:30:05,309 --> 00:30:09,679
(답변) 오 제가 질문을 잘못 이해했었네요. 저희는 RNN 전체를 실행시켰고, 특정 hidden state의 흥분 상태를 관찰했습니다.

430
00:30:09,680 --> 00:30:14,470
(답변) 그러니까 그냥 실행은 그대로 하되, 특정 hidden state의 상태를 기록하고 살펴본 것입니다.

431
00:30:14,470 --> 00:30:20,900
이해가 되셨나요?

432
00:30:20,900 --> 00:30:23,940
그러니까 저는 여기서 hidden state 단 한 부분만을 여기 슬라이드에 나타냈습니다.

433
00:30:23,940 --> 00:30:27,740
물론 hidden state 에는 이 부분 말고도 다른 일들을 하는 cell들이 많이 있죠.

434
00:30:27,740 --> 00:30:30,349
이것들은 모두 동시에, 다른 기능을 수행합니다.

435
00:30:30,349 --> 00:30:41,899
(질문) 여기서의 hidden state의 layer은 1개인가요?

436
00:30:41,900 --> 00:30:50,150
(답변) Multi-layer RNN을 말씀하시는 건가요? 그것에 대해서는 좀 있다가 설명드리겠습니다. 여기서는 Multi-layer을 썼지만, Single-layer을 썼어도 결과는 비슷했을 거에요.

437
00:30:50,150 --> 00:31:00,490
(질문: 안들림) (답변): 이 hidden state 들은 -1 ~ 1의 값을 가집니다. tanh 함수의 결과물이거든요.

438
00:31:00,490 --> 00:31:04,120
(답변) 이건 우리가 아직 다루지 않은 LSTM에 대한 것들입니다. 한 cell에 배정된 값은 -1~1 이라는 것 정도만 알아두세요.

439
00:31:04,119 --> 00:31:11,869
(답변) 이건 우리가 아직 다루지 않은 LSTM에 대한 것들입니다. 한 cell에 배정된 값은 -1~1 이라는 것 정도만 알아두세요.

440
00:31:11,869 --> 00:31:15,609
RNN은 매우 잘 작동하고, 이러한 시퀀스 모델을 잘 학습할 수 있습니다.

441
00:31:15,609 --> 00:31:19,039
대략 1년 전에 어떤 사람들이 이걸 컴퓨터 비전-image captioning 분야에 적용해 보았습니다.

442
00:31:19,039 --> 00:31:22,039
대략 1년 전에 어떤 사람들이 이걸 컴퓨터 비전-image captioning 분야에 적용해 보았습니다.

443
00:31:22,039 --> 00:31:25,210
여기서는 어떤 하나의 사진을 가지고 단어의 배열을 생성해 보았는데요,

444
00:31:25,210 --> 00:31:27,840
RNN은 여기서 매우 잘 작동했습니다.

445
00:31:27,839 --> 00:31:32,490
RNN은 여기서 매우 잘 작동했습니다.

446
00:31:32,490 --> 00:31:36,240
여기 한 부분을 보시면,

447
00:31:36,240 --> 00:31:43,039
사실 이건 제 논문이기 때문에 저 사진들은 제가 마음대로 쓸 수 있죠.

448
00:31:43,039 --> 00:31:46,629
CNN에 이미지를 입력했는데요,

449
00:31:46,630 --> 00:31:48,990
잘 살펴보시면 사실 이것은 CNN과 RNN의 두 부분으로 구성되어 있다는 것을 발견할 수 있습니다.

450
00:31:48,990 --> 00:31:51,750
잘 살펴보시면 사실 이것은 CNN과 RNN의 두 부분으로 구성되어 있다는 것을 발견할 수 있습니다.

451
00:31:51,750 --> 00:31:55,460
CNN은 이미지 처리를, RNN은 단어들의 순서 결정을 맡았습니다.

452
00:31:55,460 --> 00:31:58,470
제가 강의 처음에 했던 레고 블록 비유를 기억한다면,

453
00:31:58,470 --> 00:32:01,039
CNN과 RNN을 그림에 보이는 화살표와 같이 연결시킨 것을 이해할 수 잇을 것입니다.

454
00:32:01,039 --> 00:32:04,509
CNN과 RNN을 그림에 보이는 화살표와 같이 연결시킨 것을 이해할 수 잇을 것입니다.

455
00:32:04,509 --> 00:32:07,829
저희가 여기서 잘한 점은 여기서 RNN 단어 생성 모델의 입력값을 적절히 조절했다는 것입니다.

456
00:32:07,829 --> 00:32:11,349
그러니까 아무 텍스트나 RNN에 입력한 것이 아니라,

457
00:32:11,349 --> 00:32:14,939
CNN의 결과물을 RNN의 입력값으로 받아온 것이죠.

458
00:32:14,940 --> 00:32:21,220
좀 더 자세히 설명드리겠습니다. forward pass 부분부터요.

459
00:32:21,220 --> 00:32:24,110
여기 test image가 있습니다.

460
00:32:24,109 --> 00:32:27,679
우리는 이 이미지에서 단어들의 시퀀스를 만들어보고 싶어요.

461
00:32:27,680 --> 00:32:31,240
그래서 다음과 같이 이미지를 먼저 처리했습니다.

462
00:32:31,240 --> 00:32:35,250
먼저 이미지를 CNN에 입력했습니다. 여기서 쓰인 CNN은 VGG net 이었습니다.

463
00:32:35,250 --> 00:32:37,349
그리고 여기 conv들과 maxpool 들을 통과시켰죠.

464
00:32:37,349 --> 00:32:40,149
일반적으로 마지막에는 softmax classifier가 위치합니다.

465
00:32:40,150 --> 00:32:44,440
softmax는 확률분포를 출력하죠. 예를 들어 1000개의 카테고리가 있다면 각 카테고리에 대한 확률분포를요.

466
00:32:44,440 --> 00:32:47,420
근데 여기서 우리는 softmax를 사용하지 않았습니다.

467
00:32:47,420 --> 00:32:50,750
대신 이 끝부분을 RNN의 시작 부분과 연결시켰죠.

468
00:32:50,750 --> 00:32:54,880
RNN 입력에 처음에는 특별한 벡터들을 사용했습니다.

469
00:32:54,880 --> 00:33:00,410
RNN 에 입력되는 벡터들의 차원은 300이었고요,

470
00:33:00,410 --> 00:33:02,700
RNN의 첫 iteration에는 무조건 이 벡터를 사용했습니다.

471
00:33:02,700 --> 00:33:05,750
그럼으로써 RNN이 이것이 시퀀스의 시작임을 파악할 수 있게 했습니다.

472
00:33:05,750 --> 00:33:09,039
그리고 아까 살펴본 recurrence 공식 (Vanilla NN)을 사용했습니다.

473
00:33:09,039 --> 00:33:13,769
그리고 아까 살펴본 recurrence 공식 (Vanilla NN)을 사용했습니다.

474
00:33:13,769 --> 00:33:18,779
아까는 (Wxh*x + Whh*h)과 0으로 초기화되는 h_0을 사용했다면,

475
00:33:18,779 --> 00:33:23,500
아까는 (Wxh*x + Whh*h)과 0으로 초기화되는 h_0을 사용했다면,

476
00:33:23,500 --> 00:33:28,089
아까는 (Wxh*x + Whh*h)과 0으로 초기화되는 h_0을 사용했다면,

477
00:33:28,089 --> 00:33:33,649
이번에는 v를 추가해서 (Wxh*x + Whh*h + Wih*v) 를 사용했습니다.

478
00:33:33,650 --> 00:33:38,040
v는 CNN의 맨 마지막 출력값이고,

479
00:33:38,039 --> 00:33:43,399
Wih는 v에 들어 있는 이미지에 대한 정보를 RNN에게 전달해주기 위한  가중치 행렬입니다.

480
00:33:43,400 --> 00:33:46,380
RNN에 이미지의 정보를 전달해 주는 방법은 실제로 여러 가지가 있고,

481
00:33:46,380 --> 00:33:48,940
이것은 그 중 쉬운 한 방법일 뿐입니다.

482
00:33:48,940 --> 00:33:51,690
이것은 그 중 쉬운 한 방법일 뿐입니다.

483
00:33:51,690 --> 00:33:55,750
t = 0 에서의 y_0 벡터는 시퀀스의 첫번째 단어의 확률분포입니다

484
00:33:55,750 --> 00:34:00,009
t = 0 에서의 y0 벡터는 시퀀스의 첫번째 단어의 확률분포입니다

485
00:34:00,009 --> 00:34:05,490
이것이 작동하는 방식을 설명해 볼게요.

486
00:34:05,490 --> 00:34:09,699
여기 그림에 밀짚모자가 보이시죠

487
00:34:09,699 --> 00:34:12,939
이 부분은 CNN에 의해 '지푸라기 같은' 물체로 인식됩니다.

488
00:34:12,940 --> 00:34:17,039
Wih는 이 부분의 hidden state의 값이 특정 state로 넘어갈 때 '지푸라기'이라는 단어가 출력되게 하는 확률을 높이는 데 영향을 미칩니다.

489
00:34:17,039 --> 00:34:20,519
그래서 '지푸라기 같은' 질감을 가진 이미지가 실제로 '지푸라기'이라는 단어의 출현 확률을 높이는 것이죠.

490
00:34:20,519 --> 00:34:23,940
y0의 값 중 하나가 커지게 되는 방식으로요.

491
00:34:23,940 --> 00:34:28,470
y0의 값 중 하나가 커지게 되는 방식으로요.

492
00:34:28,469 --> 00:34:32,269
이제 RNN은 두 가지 작업을 처리해야 합니다.

493
00:34:32,269 --> 00:34:36,550
다음 순서에 어떤 단어가 올지를 예측하고, 현재 이미지 정보를 기억해야 합니다.

494
00:34:36,550 --> 00:34:40,629
우리가 이 softmax로부터 샘플링을 했을때 실제로 이 부분에서 가장 출현 확률이 높은 단어가 '지푸라기'라면,

495
00:34:40,628 --> 00:34:44,710
우리가 이 softmax로부터 샘플링을 했을때 실제로 이 부분에서 가장 출현 확률이 높은 단어가 '지푸라기'라면,

496
00:34:44,710 --> 00:34:47,519
우리는 이 단어를 기록하고 이것을 다시 RNN에 넣어줍니다.

497
00:34:47,519 --> 00:34:52,190
이 단계에서 우리는 단어 단위 embedding을 사용하고 있습니다.

498
00:34:52,190 --> 00:34:55,750
'지푸라기' 라는 단어는 차원이 300인 벡터의 한 원소입니다.

499
00:34:55,750 --> 00:35:00,010
현재 우리가 여기서 사용하는 단어 사전에는 지푸라기를 비롯한 각기 다른 벡터의 형태로 표시되는 300개의 단어들이 존재합니다.

500
00:35:00,010 --> 00:35:02,940
이 300개의 단어를 RNN에 입력하면 그 출력값 y1은 바로 다음 순서에 올 단어를 예측합니다.

501
00:35:02,940 --> 00:35:07,090
하나는 우리가 이러한 모든 특성을 우리가 얻을 왜 내 두 번째 세계와 순서

502
00:35:07,090 --> 00:35:08,010
그것에서 샘플을 다시

503
00:35:08,010 --> 00:35:12,490
워드 모자 가능성이 있다고 가정 지금 우리는 모자 400 훨씬 나이 프리젠 테이션을

504
00:35:12,489 --> 00:35:18,299
그리고 거기의 분포를 얻을 후 우리는 다시 샘플링하고 우리는 때까지 샘플

505
00:35:18,300 --> 00:35:21,350
우리는 특별한 샘플 및 진정의 끝에있는 기간 토큰

506
00:35:21,349 --> 00:35:24,900
문장하고는 arnaz 지금이에서 생성 할 것을 우리에게 알려줍니다

507
00:35:24,900 --> 00:35:30,280
군대는 그렇게 확인 밀짚 모자 기간이 이미지를 설명했을 포인트

508
00:35:30,280 --> 00:35:34,010
치수와 그의 아내 사진의 수는 단어의 숫자 당신의

509
00:35:34,010 --> 00:35:39,220
특수 토큰과 우리가 항상 먹이 산업을위한 어휘 +1

510
00:35:39,219 --> 00:35:43,609
다른 단어에 해당하는 부문과 얘기 특별한 시작과

511
00:35:43,610 --> 00:35:46,250
우리는 언제나 그 전부 단일 통해 전파

512
00:35:46,250 --> 00:35:49,769
시간은 무작위로이 국유화하거나 당신은 무료로 BG 그물을 초기화 할 수 있습니다

513
00:35:49,769 --> 00:35:52,099
다음 분을 위해 무역

514
00:35:52,099 --> 00:35:56,319
배포판은 다음 그라데이션을 인코딩 한 다음이를 통해 백업

515
00:35:56,320 --> 00:35:59,700
전체 단일 모델로 것이나 그냥 모든 공동에서 훈련하고 얻을

516
00:35:59,699 --> 00:36:08,389
캡션 또는 이미지 캡처 확인 질문을 많이하지만 네 삼백

517
00:36:08,389 --> 00:36:12,609
감정 묻어은 너무 이미지 모든 단어의 단지 독립적있어

518
00:36:12,610 --> 00:36:18,430
그렇게 우리가 그것으로 얻을 파산거야와 관련된 300 번호를 가지고

519
00:36:18,429 --> 00:36:21,769
당신은 무작위로 초기화 한 다음이 더 나은 섹스에 들어갈 백업 할 수 있습니다

520
00:36:21,769 --> 00:36:25,360
그 묻어은 주위 그냥 매개 변수를 다른 이동합니다 오른쪽 그래서

521
00:36:25,360 --> 00:36:30,530
그것에 대해 생각하는 방법은 모두를위한 하나의 홉 표현을 데입니다입니다

522
00:36:30,530 --> 00:36:34,960
단어는 당신은 거대한 W 매트릭스 곳 하나 하나가

523
00:36:34,960 --> 00:36:40,130
그 백 농장과 W 곱셈과 승 300 밖으로하지만 크기가

524
00:36:40,130 --> 00:36:43,530
효과적으로 하나가 부러 밖으로 따 버릴거야있는 뭔가 w

525
00:36:43,530 --> 00:36:47,560
나는 당신이 그 마음에 들지 않는 경우 그래서 그냥 생각이 한랭 전선의 종류의 걸거야

526
00:36:47,559 --> 00:36:50,279
침대에서 단지 하나의 호퍼 프리젠 테이션으로 생각하고 수행 할 수 있습니다

527
00:36:50,280 --> 00:36:58,920
교육에 토큰 네 말에 최대 네 그것의 모델러를 그런 식으로 생각

528
00:36:58,920 --> 00:37:02,769
데이터는 우리가 예술에서 기대하는 올바른 순서는 내가 할 수있는 첫 번째 단어입니다

529
00:37:02,769 --> 00:37:07,969
기대 때문에 매일 훈련 예 일종의 특별이

530
00:37:07,969 --> 00:37:10,288
그리고 진행 토큰

531
00:37:10,289 --> 00:37:28,929
당신이 유선 수 다르게 우리는 모든 단일 상태로 연결이 밝혀

532
00:37:28,929 --> 00:37:32,999
그것은 실제로 당신이 단지에 연결하면 실제로 잘 작동 악화 때문에 작동

533
00:37:32,998 --> 00:37:36,718
시간 단계 최초의 다음 아르 논은이이 두 작업을 저글링하는

534
00:37:36,719 --> 00:37:40,829
그것은 예술과 그것을 통해 기억 할 필요가 무엇 이미지에 대한 기억

535
00:37:40,829 --> 00:37:45,179
또한 이러한 모든 의상을 생산해야하고 어떻게 든 거기에 그렇게하고 싶어

536
00:37:45,179 --> 00:38:04,209
일부는 사실 클래스 직후 나는 당신을 줄 수있는 이유를 전진

537
00:38:04,208 --> 00:38:10,208
단일 인스턴스는 이미지와 단어의 순서와 우리가 대응합니다

538
00:38:10,208 --> 00:38:16,328
여기에 그 단어를 연결 것이고, I를 우리는 이미지를 이야기하고 우리가하여야한다

539
00:38:16,329 --> 00:38:22,159
그래서 와서 당신이 모든 사람들은 바닥에 계획되지 않은 한 기차 시간

540
00:38:22,159 --> 00:38:25,528
이미지 런던과 다음이 그래프를 풀다 당신은 당신의 손실을

541
00:38:25,528 --> 00:38:29,389
당신이 조심 있다면 배경이 다음 이미지의 배치를 할 수 있으며,

542
00:38:29,389 --> 00:38:33,108
그래서 당신의 이미지를 한 경우에는 때로는 서로 다른 길이의 시퀀스가

543
00:38:33,108 --> 00:38:36,199
당신이 난 것을 확인 말을해야하기 때문에 훈련 데이터는 조심해야

544
00:38:36,199 --> 00:38:41,059
아마 다음의 몇 가지를 최대 스무 단어의 배치를 처리하고자

545
00:38:41,059 --> 00:38:44,499
코드에서 당신이 알고에 그 문장이 짧거나 더 이상 필요가있을 것입니다

546
00:38:44,498 --> 00:38:48,188
일부 일부 일부 문장은 다른 사람보다 더 오래 있기 때문에 걱정

547
00:38:48,188 --> 00:38:55,368
우리는 내가 갈 물건이 너무 많은 질문이

548
00:38:55,369 --> 00:39:03,450
그 완전히 공동으로이 모든 것을 전파하도록 네 감사합니다

549
00:39:03,449 --> 00:39:07,538
훈련은 인터넷으로 기차를 미리 할 수​​ 있도록 한 다음 그 단어를 넣어

550
00:39:07,539 --> 00:39:10,190
이하지만 당신은 공동으로 모든 훈련을 원하고 그 큰이야

551
00:39:10,190 --> 00:39:15,429
우리는 우리가 검색 기능을 알아낼 수 있기 때문에 실제로 이점

552
00:39:15,429 --> 00:39:20,368
더 좋은 말은 그래서 당신은이 훈련하는 이미지를 설명하기 위해

553
00:39:20,369 --> 00:39:23,890
실제로 우리가 인구 조사 자료에이 시도는 일반적인 욕구 중 하나를 설정합니다

554
00:39:23,889 --> 00:39:27,368
마이크로 소프트 코코라고하는 것은, 그래서 그냥 당신이처럼 보이는 무엇의 아이디어를 제공합니다

555
00:39:27,369 --> 00:39:31,499
대략 각 이미지 80 이미지와 다섯 문장의 설명이 있었다

556
00:39:31,498 --> 00:39:35,288
그래서 당신은 단지 사람들에게 아마존 기계 터크를 사용하여 얻은 것은 우리에게주세요

557
00:39:35,289 --> 00:39:39,710
문장 이미지에 대한 설명과 기록 및 데이터 세트를 종료하고

558
00:39:39,710 --> 00:39:43,249
그래서 당신은 당신이 예상 할 수있는이 모델에게 결과의 종류를 훈련 할 때 또는

559
00:39:43,248 --> 00:39:49,078
약 좀이 같은이 너무 이러한 이미지를 설명하는 우리의 무엇이다

560
00:39:49,079 --> 00:39:52,329
이 이것이 검은 셔츠 연주 기타 또는 건설 사람이다라고 말한다

561
00:39:52,329 --> 00:39:55,710
도로 또는 두 젊은 여자에 작업 오렌지 시티 웨스트에서 노동자 재생

562
00:39:55,710 --> 00:40:00,528
레고 장난감이나 소년 그건 아니에요 웨이크 보드에 물론 공중제비를하고있다

563
00:40:00,528 --> 00:40:04,650
웨이크 보드는하지만 매우 재미 실패 사례도 있습니다 가까이있는

564
00:40:04,650 --> 00:40:07,680
또한이 야구 방망이를 들고 어린 소년입니다 보여주고 싶은

565
00:40:07,679 --> 00:40:12,338
이 고양이는 여자의 원격 제어와 함께 소파에 앉아있다

566
00:40:12,338 --> 00:40:15,710
거울 앞의 테디 베어를 들고

567
00:40:15,710 --> 00:40:22,400
여기 질감은 아마 무슨 일이 것은 그것을 만든 것입니다 확신 해요

568
00:40:22,400 --> 00:40:26,289
이 테디 베어가 있다고 생각하고 마지막은 서 창녀입니다

569
00:40:26,289 --> 00:40:30,409
거리 도로의 중간 그래서 분명히 일부 확실하지 아무 말 없다 무엇

570
00:40:30,409 --> 00:40:34,858
이 나온 모델의 단지 간단한 종류 그래서 거기에 무슨 일이 있었

571
00:40:34,858 --> 00:40:37,619
작년 모델의 이러한 종류의 상단에 작업하려고 많은 사람들이 있었다

572
00:40:37,619 --> 00:40:41,559
난 그냥 당신에게 11 레벨의 아이디어를 제공하고자 그들을 더 복잡하게

573
00:40:41,559 --> 00:40:44,929
흥미로운 단지 사람들이 기본 아키텍처를 연주하는 방법에 대한 아이디어를 얻을 수

574
00:40:44,929 --> 00:40:51,329
그래서 이것은 현재 모델에서 발견 경우 지난해 종이는 우리

575
00:40:51,329 --> 00:40:55,608
단지 처음에 시간을 이미지로 한 시간을 공급 한 경우를

576
00:40:55,608 --> 00:40:59,480
이 놀 수있는 것은 실제로 다시 볼 수있는 난폭 한 재발 성 신경 네트워크입니다

577
00:40:59,480 --> 00:41:03,130
무선 않는 작동 기술 화상의 화상 및 참조 부

578
00:41:03,130 --> 00:41:07,180
당신이 허용 등이 모든 단어를 생성하는 등의 단어가 없습니다

579
00:41:07,179 --> 00:41:10,460
실제로 이미지 옆 모습을하고 다른 기능을 찾아

580
00:41:10,460 --> 00:41:13,470
그것은 다음에 설명 할 수 있습니다 당신은 실제로 완전히에서이 작업을 수행 할 수있는 작업

581
00:41:13,469 --> 00:41:17,899
그들은 단지이 말뿐만 아니라 측면을 생성하지 않도록 학습 가능한 방법

582
00:41:17,900 --> 00:41:21,289
여기서 이미지에 다음보고하는 등이 작동하는 방식 만을 수행하지 않습니다

583
00:41:21,289 --> 00:41:24,259
아웃 아르 논하지만 당신은 아마 다음 하나의 시퀀스에 대한 분배있어

584
00:41:24,260 --> 00:41:29,250
하지만 제공이 오는 당신은 발륨은 우리가 전달이 경우 말을 않는

585
00:41:29,250 --> 00:41:37,389
512 활성화 부피 (512) (14)에 의해 14를 얻었고에서 모든 및 주석

586
00:41:37,389 --> 00:41:40,179
우리는 단지 그 분포를 인정하지 않습니다하지만 당신은 또한을 방출 한 시간

587
00:41:40,179 --> 00:41:44,358
모양까지 키처럼 좀입니다 오백열둘 차원 사진

588
00:41:44,358 --> 00:41:48,019
당신은 이미지 옆에 그래서 실제로 나는이 생각하지 않습니다 찾기 위해 원하는 것을

589
00:41:48,019 --> 00:41:51,210
그들은이 특별한 종이에 무슨 짓을하지만, 이것은 당신이 연결할 수 있습니다 한 방법입니다

590
00:41:51,210 --> 00:41:54,510
이 위로이 사진을보고 뭔가는 아르 논에서 방출되는 단지

591
00:41:54,510 --> 00:41:58,430
그냥 약간의 무게와 다음이 그림은 점 수를 사용하여 예측처럼

592
00:41:58,429 --> 00:42:03,618
제품이 모든 (14) (14)에 의해 위치가 그래서 우리는 이러한 모든 점 제품을 함께

593
00:42:03,619 --> 00:42:09,108
우리는 우리가 지금 우리가 다음 우리 (14)의 호환성에 의해 기본적으로 14 계산 달성

594
00:42:09,108 --> 00:42:13,949
그것은 모두 당신의 있도록 그래서 기본적으로 우리는이 모든 것을 정상화 이것에 부드러운 최대를 넣어

595
00:42:13,949 --> 00:42:17,149
이 14 (14)에 의해, 그래서 우리는 이미지를 통해 긴장 부르는이를 얻을 수

596
00:42:17,150 --> 00:42:21,230
아마 이미지에 지금 아르 논에 대한 흥미로운 내용을 통해지도,

597
00:42:21,230 --> 00:42:25,889
우리는이와이 사람의 가중 합을 수행하라는 메시지가이 문제를 사용

598
00:42:25,889 --> 00:42:27,239
현출

599
00:42:27,239 --> 00:42:30,929
그래서 오늘 아침은 기본적으로는 어떻게 생각하는지의 신화는 현재 수

600
00:42:30,929 --> 00:42:36,089
그것에 대한 흥미가 돌아갑니다 당신은의 가중 합을하고 결국

601
00:42:36,090 --> 00:42:39,850
엘리스 팀이 시점에서보고 싶은 기능의 종류

602
00:42:39,849 --> 00:42:44,809
시간 등 섬의 생성 물건, 예를 들어 그것을 결정할 수 있습니다

603
00:42:44,809 --> 00:42:49,400
지금과 같은 객체에 대한보고 싶은 그 확인은 벡터 파일을 인정

604
00:42:49,400 --> 00:42:53,220
물건 같은 개체의 숫자는이 때의 정액과 상호 작용

605
00:42:53,219 --> 00:42:57,379
위원회 주석 어쩌면 그 지역 같은 개체의 일부는 오는

606
00:42:57,380 --> 00:43:01,700
점등 및 천장처럼 떨어지는 정품 인증에서이지도를 참조

607
00:43:01,699 --> 00:43:05,949
4514 화나게하고 당신은 그 부분에 관심을 집중 결국

608
00:43:05,949 --> 00:43:10,059
이 상호 작용을 통해 그래서 당신은 기본적으로 그냥 할 수있는 조회 이미지

609
00:43:10,059 --> 00:43:14,130
이미지에 당신은 문장을 설명하고 그래서이 뭔가 우리 동안

610
00:43:14,130 --> 00:43:17,360
부드러운 구금으로 참조 실제로 몇 강연이가는 것

611
00:43:17,360 --> 00:43:21,050
그래서 우리는 군대가 실제로하지 않은 수있는이 같은 일을 다루려고

612
00:43:21,050 --> 00:43:26,880
선택적 입력을 처리하는 등의 수입을 통해 관심과 그 그래서 I

613
00:43:26,880 --> 00:43:30,030
그냥 당신에게 그 무엇의 미리보기를 제공하기 위해 약 한 시간 그것을 가지고 싶어

614
00:43:30,030 --> 00:43:34,490
우리가 중 한 가지 방법으로 우리의 삶을 더 복잡하게하려면 이제 괜찮아 보이는

615
00:43:34,489 --> 00:43:39,259
이 당신을 제공합니다, 그래서 우리가 그 층을 쌓아하는 것입니다 할 수있는 당신이 더 많은 것을 알고

616
00:43:39,260 --> 00:43:43,570
깊은 물건은 일반적으로 더 나은 우리가에 가지 방법 중 하나를이를 시작하는 방법을 작동

617
00:43:43,570 --> 00:43:46,809
적어도 당신은 재발 성 신경 네트워크를 쌓을 수 많은 방법이있다 그러나이

618
00:43:46,809 --> 00:43:49,409
사람들이 당신이 할 수 실제로 사용하는 것이 바로 그 중 하나입니다

619
00:43:49,409 --> 00:43:53,339
똑바로 그냥 서로 그렇게 한 아르 논에 대한 자극이에 하네스를 연결

620
00:43:53,340 --> 00:43:59,170
우리가 이전에 주 사진의 디렉터 등이 이미지

621
00:43:59,170 --> 00:44:02,750
시간 축이 수평으로 이동 한 다음 우리가 다른이 위쪽으로가는

622
00:44:02,750 --> 00:44:05,960
이 특정 이미지의 의식 등 세 가지 별도의 재발이 있습니다

623
00:44:05,960 --> 00:44:09,858
신경 네트워크는 무게의 자신의 세트와 각각이 대령이다 그

624
00:44:09,858 --> 00:44:16,299
난 그냥 서로 먹이를하지 그래서이 항상 공동으로 더 거기에 훈련되어 작동합니다

625
00:44:16,300 --> 00:44:19,119
기차는 먼저 모든 단지 하나의 경쟁 성장의 두 번째 임기 하나 원

626
00:44:19,119 --> 00:44:22,700
배경으로는 상단이 재발 식을 통해 얻을 수

627
00:44:22,699 --> 00:44:25,980
상아 영국은 여전히​​ 우리는 여전히있어 더 일반적인 규칙을 만들 가능성이 높습니다

628
00:44:25,980 --> 00:44:29,280
똑같은 일을하면 우리는 우리가 복용하고있는 같은 공식을하지 않았다된다

629
00:44:29,280 --> 00:44:35,390
우린 시간 전에에서 아래 아래 깊이와 효과에서 강의

630
00:44:35,389 --> 00:44:39,469
를 절단하고 퍼팅이 w 변환과를 통해 지원

631
00:44:39,469 --> 00:44:40,519
스매싱 10 각

632
00:44:40,519 --> 00:44:44,509
당신이 이것에 대해 약간 혼란스러워하는 경우에 당신이 기억한다면, 그래서 거기있다

633
00:44:44,510 --> 00:44:51,760
WRX H 시간의 X 플러스 당신이 다시 작성할 수 있습니다 whah 시간의 H는 엑손의 연결입니다

634
00:44:51,760 --> 00:44:56,260
하나의 행렬 곱 H 바로 그래서 난에 침을 국가 스틱 것처럼

635
00:44:56,260 --> 00:45:03,680
기본적으로 무슨 일이 끝나는 다음 하나의 열 벡터와 나는이 w 행렬이

636
00:45:03,679 --> 00:45:07,690
최대 일어나고 당신의 WX 연령이 행렬과 WH의 첫 번째 부분

637
00:45:07,690 --> 00:45:12,700
미국에서 두 번째로 당신의 매트릭스의 일부 등 식의이 종류는 기록 될 수있다

638
00:45:12,699 --> 00:45:16,099
식으로 당신은 당신의 입력을 쌓아 단일 W가 어디

639
00:45:16,099 --> 00:45:24,759
변환은 같은 식 있도록 그래서 우리가이는 중지 할 수 있습니다 방법

640
00:45:24,760 --> 00:45:29,780
두 시간 색인되는 이후로 지금 다음이 발표하고

641
00:45:29,780 --> 00:45:33,510
우리는 또한이 더 복잡한이 적층 공유되지 수 있습니다 지금은 한 방향으로 발생

642
00:45:33,510 --> 00:45:37,030
그들을 실제로 그렇게 지금 약간 더 반복 공식을 사용하여

643
00:45:37,030 --> 00:45:40,300
지금까지 우리는 복귀에 대한 매우 간단한 재발 수식으로 보았다

644
00:45:40,300 --> 00:45:44,480
실제로 작품은 실제로 거의 지금과 같은 공식을 사용하고

645
00:45:44,480 --> 00:45:48,170
기본 네트워크는 매우 드물게 우리가 그것에게 부르는 사용합니다 대신 사용되지 않습니다

646
00:45:48,170 --> 00:45:52,059
LSD와 오랜 단기 기억은 그래서 이것은 기본적으로 모든 서류에 사용된다

647
00:45:52,059 --> 00:45:56,500
지금이 공식은 당신이 인 경우도 프로젝트를 사용하는 것입니다

648
00:45:56,500 --> 00:46:00,989
사용이 현재 작동하지만 나는이 시점에서 주목하고 싶은 모든입니다

649
00:46:00,989 --> 00:46:04,729
동일은 알렌과 마찬가지로이 재발 수식은이 단지의

650
00:46:04,730 --> 00:46:09,050
약간 더 복잡한 기능을 확인 우리는 여전히 낮은에서 사진을 촬영하고

651
00:46:09,050 --> 00:46:13,789
그리고 이전의 시간에 입력 같은 깊이 이전 재산이었다

652
00:46:13,789 --> 00:46:18,309
연락 그들 앗 전송을 통해 이르렀 그러나 지금 우리는이 더이

653
00:46:18,309 --> 00:46:21,869
복잡성과 방법을 우리가 실제로이 지점에서 뉴 헤이븐 상태를 달성

654
00:46:21,869 --> 00:46:25,539
시간은 그래서 우리는 단지 약간 더 복잡한되고있어 방법에서 북한 이탈 주민을 결합

655
00:46:25,539 --> 00:46:28,900
아래 실제로 단지 더 상태를 제목에 업데이트를 수행하기 전에

656
00:46:28,900 --> 00:46:33,050
이 동기를 부여 정확히 복잡한 공식은 그래서 우리는 몇 가지 세부 사항에 갈거야

657
00:46:33,050 --> 00:46:41,609
공식 이유는 실제로 오스틴에서 사용할 수있는 더 좋은 생각이 될 수 있습니다

658
00:46:41,608 --> 00:46:49,909
그리고 우리가 지금 당장 그것을 통해 갈거야 의미가 나를 신뢰하게 그렇다면 당신

659
00:46:49,909 --> 00:46:56,480
오후 4시 일부 온라인 비디오를 차단하거나 Google 이미지는 다이어그램을 찾을 수 있습니다로 이동

660
00:46:56,480 --> 00:47:00,989
정말 도움이되지 않는이처럼 사람에게 내가 그를 처음봤을 때 생각

661
00:47:00,989 --> 00:47:04,048
이 사람이 정말 그가 무슨 일이 일어나고 있는지 정말 확신했다 겁처럼 정말 무서워되고

662
00:47:04,048 --> 00:47:08,170
나는 엘리스 팀을 이해하고 난 여전히이 두 다이어그램이 무엇인지 모르는에

663
00:47:08,170 --> 00:47:14,289
나는 목록을 파괴하려고하는거야하고 ​​까다로운 물건의 종류, 그래서 그렇게 확인

664
00:47:14,289 --> 00:47:18,329
그것을 통해 단계의 종류 당신이 정말로이 도면에 강의 있도록 넣어

665
00:47:18,329 --> 00:47:24,220
형식은 우리가 미국의 방정식이 있고 난 그래서 여기에없는 스팀 확인을 위해 완벽하다

666
00:47:24,219 --> 00:47:28,238
우리는이 두 벡터를 가지고 위치를 상단에 여기에 첫 번째 부분에 초점을 맞출 것

667
00:47:28,239 --> 00:47:32,720
아래로부터의 상태에서 이렇게 X와 HHS 이전 전에 사고 있지만,

668
00:47:32,719 --> 00:47:37,848
우리는 변환 W를 통해 지금 모두 잭슨 href가 크기 경우를 만났다

669
00:47:37,849 --> 00:47:40,950
그래서 우리는 어떤을 위해 생산 끝날거야 숫자를 보낼있다

670
00:47:40,949 --> 00:47:46,068
(21)에 의해 제시되었다이 w 매트릭스를 통해 확인 번호는 그래서 우리는 이러한이

671
00:47:46,068 --> 00:47:51,108
그들이 입력 짧은 것 OMG 경우 사 및 차원 벡터 나가뿐만

672
00:47:51,108 --> 00:47:57,328
그리고 G는 나는 당신과 그렇게 ISI없이 신호를 통과 단지를 무엇 확실하지 않다

673
00:47:57,329 --> 00:48:05,859
게이트 및 G는 방법에게 지금이 실제로 작동이 길을 똑바로 세입자 게이트로 이동

674
00:48:05,858 --> 00:48:09,420
그것에 대해 생각하는 가장 좋은 방법은 내가 깜빡 한 가지가 실제로 언급하는 것입니다

675
00:48:09,420 --> 00:48:15,028
이전 슬라이드는 일반적으로 하나의 HVAC 시도 말합니다 할 네트워크를 필요로하지 않습니다

676
00:48:15,028 --> 00:48:18,018
매번 중지하고 그에게 물었다 실제로 두 벡터 모든이

677
00:48:18,018 --> 00:48:23,618
한 시간 때문에 우리는 세포 상태 벡터를 참조 전화를 매도록

678
00:48:23,619 --> 00:48:29,470
시간 단계는 우리가 위험에 두 기관이 있고 그리고에서와 같이 여기 벡터를 참조하십시오

679
00:48:29,469 --> 00:48:33,558
노란색 그래서 우리는 기본적으로 두 벡터 여기 공간에있는 모든 단일 지점을 가지고

680
00:48:33,559 --> 00:48:37,849
그들이하는 일은 그들이 기본적 그래서이 셀 상태에서 작동하고있다

681
00:48:37,849 --> 00:48:41,680
전에 당신 아래의 내용에 따라 해당 사용자 컨텍스트 당신은 결국

682
00:48:41,679 --> 00:48:45,199
이들과 함께 세포 상태에서 작동

683
00:48:45,199 --> 00:48:50,509
그리고 옹 요소와 그것에 대해 생각하는 새로운 방법 내가 통해 갈거야된다

684
00:48:50,510 --> 00:48:58,290
이 0 또는 1 우리가 원하는 I NO처럼 이진 않습니다에 대해이 방법을 많이 생각합니다

685
00:48:58,289 --> 00:49:01,199
그들에게 우리가 그들을 게이트의 해석이 생각하고 싶다 갖고 싶어 할 수

686
00:49:01,199 --> 00:49:05,449
영웅이 그들이다 그것의로 우리는 물론 우리가 원하기 때문에 그들에게 이상 신호를 만들

687
00:49:05,449 --> 00:49:08,348
우리는하지만, 모든 것을 통해 전파 백업 할 수 있도록이 미분 될 수 있습니다

688
00:49:08,349 --> 00:49:11,960
우리의 상황에 기반을 계산 한 바로 진 것들로 이노 생각

689
00:49:11,960 --> 00:49:17,740
항상 여기서 뭘에서이 참조 다음 당신은 무엇을 기준으로 그를 볼 수있는

690
00:49:17,739 --> 00:49:22,250
이 문은 다음과 디아즈 우리는이 페이지의 값을 데이트 끝날거야 무슨

691
00:49:22,250 --> 00:49:29,289
특히이 에피소드는 TUS을 종료하는 데 사용됩니다 게이트를 잊지

692
00:49:29,289 --> 00:49:34,869
(20) 태양 전지 등의 보호소 가장 생각되는 세포들을 재설정

693
00:49:34,869 --> 00:49:38,700
우리와 함께 (20)이 상호 작용보다 기본적으로 우리가 할 수있는 하나 최근 이러한 카운터

694
00:49:38,699 --> 00:49:42,368
이것은 자신의 레이저 포인터가 부족합니다 곱셈의 요소입니다

695
00:49:42,369 --> 00:49:45,530
배터리 때문에

696
00:49:45,530 --> 00:49:50,140
상호 작용 0 당신은 우리가를 재설정 할 수 있도록 그 셀을 제로 것이다 볼 수 있습니다

697
00:49:50,139 --> 00:49:53,969
카운터 그리고 우리는 또한 우리는이를 통해 추가 할 수있는 카운터에 추가 할 수 있습니다

698
00:49:53,969 --> 00:50:00,459
상호 작용 I 번 G와 11 사이와 G는 부정적 일 사이이기 때문에

699
00:50:00,460 --> 00:50:05,900
(10)에 기본적으로 한 12 매 있도록 모든 세포 사이의 숫자를 추가

700
00:50:05,900 --> 00:50:09,338
우리는이를 재설정 할 수있는 모든 세포에서 이러한 카운터를 하나의 시간 단계

701
00:50:09,338 --> 00:50:13,588
국가 2012 케이트를 잊어 버렸거나 우리는 하나 사이의 숫자를 추가 할 수 있습니다

702
00:50:13,588 --> 00:50:18,039
12 그래서 확인을 하나 하나 셀은 우리가 다음 셀 업데이트 및 수행 방법

703
00:50:18,039 --> 00:50:24,029
업데이트가 찌그러 세포 그렇게 10 HFC는 셀을 숙청되고 끝 머리

704
00:50:24,030 --> 00:50:28,760
그렇게 만 셀 상태의 일부와 위로로 누출이 업데이트에 의해 변조

705
00:50:28,760 --> 00:50:33,500
숨겨진 상태가이 벡터에 의해 변조 오 그래서 우리는 단지의 일부를 공개 선택

706
00:50:33,500 --> 00:50:39,530
암탉 상태와 학습 가능 방법으로 세포는 몇 가지가있다

707
00:50:39,530 --> 00:50:43,910
에 하이라이트의 종류 여기에 아마 여기에 가장 혼란스러운 부분에 우리가 걸이다

708
00:50:43,909 --> 00:50:47,500
여기에 D I 배 하나 하나 사이의 숫자를 추가하지만 가지의

709
00:50:47,500 --> 00:50:51,809
우리는 단지 거기 G가 있다면 대신 다음 이미 사이에 이름 : Jeez 때문에 혼란

710
00:50:51,809 --> 00:50:56,679
8 11 왜 우리는 내가 여러 번 G 무엇을하지 실제로 우리가 제공하는 모든 필요합니까 우리

711
00:50:56,679 --> 00:50:58,279
원하는에 의해 바다를 구현하는 것입니다

712
00:50:58,280 --> 00:51:02,330
하나 하나 사이의 숫자는 그래서는 대한 내 성 부품의 종류의

713
00:51:02,329 --> 00:51:08,989
마지막으로 내가 한 대답은 당신이 G에 대해 생각하면 그것의 기능 있다고 생각합니다

714
00:51:08,989 --> 00:51:16,159
당신의 문맥의 선형 함수는 하나의 기회가 오른쪽으로 레이저 프린터가 없습니다

715
00:51:16,159 --> 00:51:26,649
확인 그래서 G는 G 그래서 확인을 지역 310 세의 함수의 선형 함수로

716
00:51:26,650 --> 00:51:30,579
우리가 청바지를 추가 한 경우 10 시간 등에 의해 숙청 이전에 접촉하는 경우

717
00:51:30,579 --> 00:51:35,349
추가하여, 그래서 나는 시간 그녀는 그 종류의 매우 간단한 함수 같은 것

718
00:51:35,349 --> 00:51:38,929
이 난 후 실제로 더 있어요 곱셈 상호 작용을 갖는

719
00:51:38,929 --> 00:51:42,710
실제로 우리가 추가하는 것을 표현 할 수 있습니다 풍부한 기능

720
00:51:42,710 --> 00:51:47,010
이전 테스트의 기능을 생각하는 또 다른 방법으로 상태를 몸통

721
00:51:47,010 --> 00:51:50,620
이 약이 기본적으로 방법이 두 개념을 분리하는 것

722
00:51:50,619 --> 00:51:54,159
많은 우리가 G 인 셀 상태로 추가 싶어하고 우리가 원하는 수행

723
00:51:54,159 --> 00:51:58,129
나는 우리가 실제로 무엇을이 조작 가능성이 있으므로 모든 상태를 해결

724
00:51:58,130 --> 00:52:03,280
또한 될 수 있음이 두 디커플링에 의해 통해 천재 우리가 원하는 이동

725
00:52:03,280 --> 00:52:08,470
동적 측면에서 몇 가지 좋은 특성을 가지고 어떻게이 모든 증기 기차하지만,

726
00:52:08,469 --> 00:52:12,039
우리는 단지 그 오스틴 공식처럼 결국 나는 실제로 갈거야

727
00:52:12,039 --> 00:52:14,059
자세한 세부 사항에서이뿐만 아니라 통해

728
00:52:14,059 --> 00:52:21,400
확인 상기 제 1 상호 작용 이제 셀 C가 흐르는으로 이것에 대해 생각하고

729
00:52:21,400 --> 00:52:28,269
여기 그래서 경제적으로 그 시그 모이 약간의 DOTC 그렇게 노력하다

730
00:52:28,269 --> 00:52:32,559
곱셈의 상호 작용으로 자신을 게이팅 F 제로는 것입니다 그래서 만약

731
00:52:32,559 --> 00:52:38,409
셀을 차단하고 세포학 부분이 기본적으로 제공되는 카운터를 재설정

732
00:52:38,409 --> 00:52:44,799
당신은 완은 기본적으로 하위 상태 누수가 유일한 상태로 추가하고있다

733
00:52:44,800 --> 00:52:51,100
언덕 상태로하지만 너무 의해 문이 가도록 한 후 10 시간 통해

734
00:52:51,099 --> 00:52:55,380
전기 만 결정 사실로 밝혀 몇 가지 상태에있는 부품

735
00:52:55,380 --> 00:52:59,610
매각하지 않았다 숨겨진 그리고 당신은 알 수가이 고속도로뿐만 아니라,

736
00:52:59,610 --> 00:53:03,720
STM의 다음 반복으로 이동뿐만 아니라 실제로까지 폐쇄

737
00:53:03,719 --> 00:53:07,159
상위 계층이 우리가 실제로 종료 상태 교리의 머리이기 때문에

738
00:53:07,159 --> 00:53:11,250
까지 우리 위에 팀으로보고하거나이 예측에 간다

739
00:53:11,250 --> 00:53:14,510
이 기본적으로 방법을 풀다 때 그래서 그것이 가지처럼 보이는

740
00:53:14,510 --> 00:53:19,270
지금은 내 자신 그게 전부의 혼란도를 가지고있는이 나는 우리가 끝난 것 같아요

741
00:53:19,269 --> 00:53:24,550
그러나 아래에서 입력 벡터를 얻을 수와 최대 당신은 당신의 자신의 상태에서이

742
00:53:24,550 --> 00:53:26,090
(248)

743
00:53:26,090 --> 00:53:31,030
그들은 다음 차원 벡터 및 모든 거 알아 fije 네 성문을 결정

744
00:53:31,030 --> 00:53:35,110
는 셀 상태에서 동작하고, 셀의 상태가 변조 방법을 종료

745
00:53:35,110 --> 00:53:38,610
당신이 한 번 실제로 우리는 일부 국가를 설정하고 하나 사이에 번호를 추가하면

746
00:53:38,610 --> 00:53:42,630
(12) 국가의 셀 상태는 그것의 일부는 학습 가능에서 누수 밖으로 누출

747
00:53:42,630 --> 00:53:45,840
방법 및 다음 중 하나를 예측까지 갈 수 또는 다음에 갈 수 있습니다

748
00:53:45,840 --> 00:53:52,269
미국 팀의 반복은 향후 그래서 그게 그렇게이 그렇게 추한 모습입니다

749
00:53:52,269 --> 00:53:58,429
문제는 당신의 마음에 아마 그래서 우리는 거 야 우리가 간다 않은 이유입니다

750
00:53:58,429 --> 00:54:02,649
이 특별한 방법 I에서이 Look을 수행하는 이유가 뭔가의 모든 통해

751
00:54:02,650 --> 00:54:05,639
알고 싶어한다 분석가 많은 다양한 있다는 것을이 시점이

752
00:54:05,639 --> 00:54:09,309
이 시점하지만 강의 사람들의 말은 이런 식으로 많이 연주

753
00:54:09,309 --> 00:54:12,840
우리는 종류의 합리적인 것 같은 것으로이에 수렴했지만

754
00:54:12,840 --> 00:54:15,510
당신이 실제로하지 않는이에 수 많은 작은 비틀기가있다

755
00:54:15,510 --> 00:54:18,930
당신 같은 사람들 게이트의 일부를 제거 할 수 있습니다 많은하여 성능을 저하

756
00:54:18,929 --> 00:54:20,359
아마 연루 등

757
00:54:20,360 --> 00:54:25,200
당신은 할 수의 악취가이 바다가 될 수 볼 밝혀 그것을 잘 작동합니다

758
00:54:25,199 --> 00:54:28,619
일반적으로하지만 좌석의 어린 나이로 때로는 약간 더 있었다 I

759
00:54:28,619 --> 00:54:33,869
우리는 CSI가의 비트와 함께 결국 왜를위한 아주 좋은 이유가 생각하지 않습니다

760
00:54:33,869 --> 00:54:37,039
괴물하지만 실제로 좀 법무부 카운터의 측면에서 의미가 생각

761
00:54:37,039 --> 00:54:40,739
그 0으로 재설정 할 수 있습니다 또는 당신은 하나 (12)을 사이에 작은 숫자를 추가 할 수 있습니다

762
00:54:40,739 --> 00:54:46,039
지금은 좋은 실제로 비교적 단순한 이해하는 것처럼 그렇게는 가지이다

763
00:54:46,039 --> 00:54:49,300
이것은 우리 자신보다 훨씬 더 그리고 우리는 약간에 가야 정확하게 이유

764
00:54:49,300 --> 00:54:55,330
다른 그림은 재발 성 신경 있도록 구별을 그립니다

765
00:54:55,329 --> 00:54:59,259
어떤 상태 벡터 권리가 네트워크 당신은 그것을 통해 운영하고 있고이있어

766
00:54:59,260 --> 00:55:02,260
완전히이 재발 식을 통해로 변신 그래서 당신은 종료

767
00:55:02,260 --> 00:55:06,280
시간 물건 시간에서 상태 벡터를 변경까지 당신은 미국 것을 알 수 있습니다

768
00:55:06,280 --> 00:55:11,140
팀 대신 셀 미국이 흐르는 우리가 효과적으로 무슨 일을하고있다

769
00:55:11,139 --> 00:55:15,250
우리는 세포에서 찾고 그것의 일부는 국가의 머리에 누수로

770
00:55:15,250 --> 00:55:19,329
우리가 이득을 다음 잊어 버린 경우 셀에서 동작하는 방법을 결정하는 상태

771
00:55:19,329 --> 00:55:22,869
기본적으로 그냥하여 셀을 조정 끝

772
00:55:22,869 --> 00:55:28,509
함수로 쳐다 보면서 몇 가지 물건이 그래서 그래서 여기 활성 상호 작용

773
00:55:28,510 --> 00:55:33,040
우리는 영혼의 상태를 변경 결국 그것이 무엇이든 셀 상태의 다음

774
00:55:33,039 --> 00:55:37,190
대신 바로이 첨가제는 대신, 그래서 그것을 변환의

775
00:55:37,190 --> 00:55:38,429
변형

776
00:55:38,429 --> 00:55:42,929
그런 상호 작용이나 뭐 이제이 실제로 뭔가 당신을 생각 나게한다

777
00:55:42,929 --> 00:55:48,839
우리가 이미 염두에두고 클래스에 적용되었음을 그, 그래 맞아

778
00:55:48,840 --> 00:55:53,240
그래서이 같은 사실은 고체와 같은 일이 이렇게 기본적으로 직렬 공진입니다

779
00:55:53,239 --> 00:55:56,299
일반적으로 우리가 표현 거주자가 변화하고 진정으로

780
00:55:56,300 --> 00:56:00,019
여기에이 스킵 연결 및 당신은 기본적으로 주민들이를 볼 수 있습니다

781
00:56:00,019 --> 00:56:04,690
우리가 지금 여기이 X이 때문에 첨가제의 상호 작용 우리는 약간의 계산에 기초 않는다

782
00:56:04,690 --> 00:56:10,240
다음 섹스 그리고 우리는 행위와 첨가제의 상호 작용을 가지고 있고 그래서는이다

783
00:56:10,239 --> 00:56:12,959
같은 멋진로 발생하는 기본 주민들의 블록과 그 사실의

784
00:56:12,960 --> 00:56:18,440
물론 우리는 우리가 여기있어 이러한 상호 작용을 가지고 전은 세포이며, 우리가 간다

785
00:56:18,440 --> 00:56:22,619
다음 몇 가지 기능은 당신과 떨어져 우리는이 세포 상태 만에 추가 할 수

786
00:56:22,619 --> 00:56:26,900
LSD와는 달리 주민들은 또한 추가 된 날짜를 잊지하시기 바랍니다있다

787
00:56:26,900 --> 00:56:31,519
이뿐만 아니라 신호의 일부를 차단하도록 선택할 경우 제어를 잊지 있지만,

788
00:56:31,519 --> 00:56:33,679
그렇지 않으면 나는 그것이 가지 생각 때문에 대통령처럼 매우 보인다

789
00:56:33,679 --> 00:56:36,710
보고 아키텍처와 매우 유사 종류에 수렴하고 그 재미

790
00:56:36,710 --> 00:56:40,429
보인다 곳은 재발 성 신경 네트워크에서 끝의 두 소득을 작동

791
00:56:40,429 --> 00:56:43,809
같은 동적으로 어떻게 든 실제로 이러한 첨가제를 가지고 훨씬 좋네요이다

792
00:56:43,809 --> 00:56:48,739
당신이 실제로 훨씬 더 효과적으로 그렇게 전파 할 수 있도록 상호 작용

793
00:56:48,739 --> 00:56:49,779
그 시점에

794
00:56:49,780 --> 00:56:53,860
분석 팀 사이의 뒷면 전파 역학에 대해 생각

795
00:56:53,860 --> 00:56:57,760
특히 미국 팀에 좀 그라디언트를 주입하면 매우 명확하고

796
00:56:57,760 --> 00:57:01,120
가끔 내가 생기를 주입하고이 그림의 끝을 보자, 그래서 만약 여기에

797
00:57:01,119 --> 00:57:05,239
다음이 플러스 상호 작용은 바로 여기 그냥 재료 고속도로처럼

798
00:57:05,239 --> 00:57:09,299
이 동영상은 모든 탭 추가 상호 작용 오른쪽으로 흐르는 것 같은

799
00:57:09,300 --> 00:57:13,240
내가 그라데이션 시간의 어느 지점을 연결하는 경우 버전은 동일하므로 분산 때문에

800
00:57:13,239 --> 00:57:16,849
여기에 단지 물론 그라데이션도 다시 모든 방법을 날려 가고

801
00:57:16,849 --> 00:57:20,809
이러한 행위를 통해 흘러 그들이에 자신의 재료를 기여 결국

802
00:57:20,809 --> 00:57:25,630
독서 흐름합니다하지만 당신은 우리가 우리의 강렬한으로 참조 무​​엇으로 끝낼 수 없을거야

803
00:57:25,630 --> 00:57:30,110
이 그라디언트 그냥 제로로 이동을 사망 어디에 문제가 지역 사라지는라고

804
00:57:30,110 --> 00:57:32,880
당신은 다시 통해 전파 내가 예를 보여 드리겠습니다로

805
00:57:32,880 --> 00:57:36,640
완전히이 조금 수중 음파 탐지기에서 발생하는 이유 떨어져 지금 우리는이 배니싱이

806
00:57:36,639 --> 00:57:40,670
나는 당신을 보여줄 것 그라데이션 문제는 이유는이 때문에 애널리스트 오전 발생

807
00:57:40,670 --> 00:57:45,210
그냥 판의 고속도로 매 시간 단계의 이러한 구배가

808
00:57:45,210 --> 00:57:47,130
우리는 위의 미국 팀에 주입

809
00:57:47,130 --> 00:57:54,829
그냥 세포를 통과하고 등급이에서 마무리 결국하지 않습니다

810
00:57:54,829 --> 00:57:57,339
어쩌면 내가 몇 가지 질문을 가리 혼란 기능에 대한 질문이 있습니다

811
00:57:57,338 --> 00:58:01,849
여기하지만 마지막으로 한 다음 그 후 나는 arnaz가에 있었던 이유에 갈거야

812
00:58:01,849 --> 00:58:03,059
그린 즈 버러

813
00:58:03,059 --> 00:58:09,789
예 000 벡터가 중요한 것입니다

814
00:58:09,789 --> 00:58:13,400
내가 하나가 특별히 매우 중요 아니라고 생각 밝혀

815
00:58:13,400 --> 00:58:16,660
나는 스페이스 오디세이 그들이 대답 할 다른 무엇을 보여 드리겠습니다 종이가있다

816
00:58:16,659 --> 00:58:21,719
정말 거기에이 걸릴 물건 아웃하지만 물건을 연주 또한 같은있다

817
00:58:21,719 --> 00:58:25,588
당신이 그렇게이 셀 상태가 여기에있을 수 추가 할 수 있습니다 사람들의 연결

818
00:58:25,588 --> 00:58:29,538
사람들이 정말 재생할 수 있도록 실제로 입력으로 더 나은 숨겨진 상태에 넣어

819
00:58:29,539 --> 00:58:32,049
이 아키텍처 그들은 바로 이러한 반복을 많이 시도

820
00:58:32,048 --> 00:58:37,230
방정식과 거의 모든 약 동일한 일부 작동 당신이 우리와 끝까지

821
00:58:37,230 --> 00:58:40,490
그것을 우리는 약간은 매우 가지 혼란이있는, 그래서 때로는 있었다있어

822
00:58:40,489 --> 00:58:45,699
그들은했다 어디 용지를 표시하려면이 방법은 그들이 DS 업데이트를 처리

823
00:58:45,699 --> 00:58:49,538
방정식은 업데이트 방정식을 통해 나무를 내장하고있다 그리고 그들은했다

824
00:58:49,539 --> 00:58:52,950
이 같은 무작위 돌연변이 물건과 서로 다른 잔디의 모든 종류의 시도

825
00:58:52,949 --> 00:58:57,028
사용자가 업데이트 할 수 그들 대부분은 그들 중 일부의 일부를 파괴에 대해 작동

826
00:58:57,028 --> 00:58:59,858
정말보다 훨씬 더 않습니다처럼은 동일하지만 아무것도에 대한 작업

827
00:58:59,858 --> 00:59:08,150
분석 팀과 질문 재발 성 신경 네트워크가 왜 가고있다

828
00:59:08,150 --> 00:59:15,389
또한 끔찍한 역류 비디오

829
00:59:15,389 --> 00:59:22,000
와 재발 성 신경 네트워크에서 사라지는 그라데이션 문제를 보여주는

830
00:59:22,000 --> 00:59:29,250
모두에 대해 우리가 재발보고있는 것처럼 우리가 여기에 표시하고 줄기

831
00:59:29,250 --> 00:59:33,039
많은 기간 많은 시간 단계에 걸쳐 신경망 다음 주입 그라데이션

832
00:59:33,039 --> 00:59:36,760
그것은 백 스물여덟번째 시간 단계의 말을 우리는 파산하고

833
00:59:36,760 --> 00:59:40,028
네트워크를 통해 재료와 우리는 그라데이션이 무엇인지보고있는

834
00:59:40,028 --> 00:59:44,699
용 나는 체중의 입력 타입 숨겨진 매트릭스 하나에 모든 행렬 생각

835
00:59:44,699 --> 00:59:49,009
한 시간 간격 때문에 실제로 통해 전체 업데이트를 얻기 위해 그 기억

836
00:59:49,010 --> 00:59:52,289
다시 우리가 실제로 여기에 모든 그라디언트를 추가하고 그래서 무엇 무엇이다

837
00:59:52,289 --> 00:59:56,760
어떻게 여기에 표시되는 것은 배경으로 우리는 단지에서 성분을 주입하는 것입니다

838
00:59:56,760 --> 01:00:00,799
우리가 시간과 강한 조각을 통해 배경을 120 시간 단계

839
01:00:00,798 --> 01:00:04,088
그 전파의 당신이보고있는 것은 미국 팀이 당신을 많이 준다이다

840
01:00:04,088 --> 01:00:06,699
많이있다, 그래서이 역 전파에 걸쳐 그라데이션

841
01:00:06,699 --> 01:00:11,000
단지 바로이 기술을 통해 흐르는되는 정보는 전원 사망

842
01:00:11,000 --> 01:00:15,210
그냥 욕심 우리는 추방은 그냥 아무 거기에 작은 숫자가된다라고

843
01:00:15,210 --> 01:00:18,750
내가 단계 그렇게되는 시간에 대해 표시를 생각이 경우 너무 그라데이션

844
01:00:18,750 --> 01:00:22,679
우리가하지 않았다 주입 모든 정보와 10 배 단계 등

845
01:00:22,679 --> 01:00:26,149
네트워크를 통해 흘러 모든 때문에 매우 긴 종속성을 배울 수 있습니다

846
01:00:26,150 --> 01:00:29,720
우리가 왜이 볼 수 있도록 상관 관계 구조는 아래가 사망 한

847
01:00:29,719 --> 01:00:39,399
조금 동적으로 발생이 채널이 너무 재미 그가처럼 몇 가지 코멘트

848
01:00:39,400 --> 01:00:40,490
YouTube 또는 뭔가

849
01:00:40,489 --> 01:00:44,779
그래

850
01:00:44,780 --> 01:00:53,170
확인 그래서 우리가 재발 성 신경 네트워크가 여기 아주 간단한 예를 살펴 보자

851
01:00:53,170 --> 01:00:56,300
내가 보여주는 아니에요이 재발 성 신경 네트워크에 당신을 위해 전개거야 것을

852
01:00:56,300 --> 01:01:03,960
우리가있어 모든 입력은 자신의 상태 업데이트가 너무 whaaa 교회와 대기 상태가

853
01:01:03,960 --> 01:01:07,260
상호 작용을 칠 숨겨진 나는 기본적으로 재발을 전달하려고 해요

854
01:01:07,260 --> 01:01:12,380
신경망 때문에 T-오십를 사용하고 여기에 내가 어떤 차 시간 단계를하지를 않습니다

855
01:01:12,380 --> 01:01:16,260
내가 무슨 일을하고있어 WHAS 시간을 그 위에 다음 이전 세입자와 물건과입니다

856
01:01:16,260 --> 01:01:20,570
그래서 이것은 모든 입력 벡터를 무시 들어오는 단지 전진 패스입니다

857
01:01:20,570 --> 01:01:25,280
단지 WHAS 시간 H 임계 값 WHAS 시간 세이 임계 값 등

858
01:01:25,280 --> 01:01:29,500
그 전진 패스의 다음 뒤로 여기가 연출하고있어 여기서 통과

859
01:01:29,500 --> 01:01:33,820
마지막 단계에서 여기에 임의의 기울기에 의해 50 시간 단계에서 매우

860
01:01:33,820 --> 01:01:37,880
뒤쪽으로 이동 한 후 무작위 및 그라데이션을 주입 나는 그렇게 백업

861
01:01:37,880 --> 01:01:41,059
당신은 백업이 권한을 통해 여기 내가 사용하고 있습니다 통해 백업해야 할 때

862
01:01:41,059 --> 01:01:46,170
오히려 곱셈 등 400 WH보다 곱셈 어를 통해 배경을 얻을

863
01:01:46,170 --> 01:01:51,800
그래서 여기서주의 할 것은 여기에서 매우이다 나는 개발자 브라운 백을하고있는 중이 야

864
01:01:51,800 --> 01:01:54,980
수입을 어디에서 관련 바로 잡고 아무것도 통해 전파

865
01:01:54,980 --> 01:02:02,309
나는 WH 시간마다 작업을 제로보다 작은 여기서 포기하고 있었다

866
01:02:02,309 --> 01:02:06,570
우리가 실제로 WH 행렬 곱 경우 우리는 그렇게 비선형 성을하기 전에

867
01:02:06,570 --> 01:02:09,570
당신이 실제로 무슨 일을 볼 때가는 매우 펑키 뭔가가있다

868
01:02:09,570 --> 01:02:13,300
당신이 시간을 통해 뒤로 이동으로 NHS의 구배이 DHS에

869
01:02:13,300 --> 01:02:18,160
당신이 보는 것처럼 매우 걱정입니다 재미있는 구조의 매우 종류가 있습니다

870
01:02:18,159 --> 01:02:22,210
등이 우리가 여기 무슨 일을하는지와 같은 루프에 연결되는 방식

871
01:02:22,210 --> 01:02:33,409
두 시간 간격

872
01:02:33,409 --> 01:02:43,849
제로 그래 나는 생각하고 가끔 어쩌면 반군이 모든 있었다 출력의

873
01:02:43,849 --> 01:02:47,630
죽은 당신을 죽일 수 듯하지만 그건 정말 문제 아니다

874
01:02:47,630 --> 01:02:51,470
더 걱정 문제는 그 모든 쇼가 될 것 잘하지만 착용 한 생각

875
01:02:51,469 --> 01:02:55,500
사람들이 쉽게 우리가 걸 볼 수 있습니다뿐만 아니라 발견 할 수 있습니다 문제

876
01:02:55,500 --> 01:03:00,380
때문에에 또 다시 이상이 whah 행렬 곱

877
01:03:00,380 --> 01:03:04,840
앞으로 우리가 매일 반복에 awhh 곱 통과

878
01:03:04,840 --> 01:03:09,670
다시 우리가이 전파 결국 모든 숨겨진 상태를 통해 전파

879
01:03:09,670 --> 01:03:13,820
무형 문화 유산 konnte 체스와 backrub 어 공식은 실제로 것을 밝혀

880
01:03:13,820 --> 01:03:19,000
당신은 whah 행렬 곱 인사말 신호를 가지고 우리는 종료

881
01:03:19,000 --> 01:03:26,199
그라데이션이 whah 유지를 곱한 도착까지 그 다음 WH 관계자를 곱한

882
01:03:26,199 --> 01:03:32,019
그렇게 우리는 그렇게하지 ​​매트릭스 W​​H 나이 오십 번 곱 결국

883
01:03:32,019 --> 01:03:37,509
이 가진 문제는 녹색 신호는 기본적으로 두 가지 경우처럼 일어날 수 있다는 것입니다

884
01:03:37,510 --> 01:03:41,080
당신은 아마 규모 행렬없는 스칼라 값 작업에 대한 생각

885
01:03:41,079 --> 01:03:45,469
그때 임의의 번호를 가지고 있다면 두 번째 번호가 나는 유지

886
01:03:45,469 --> 01:03:48,509
그래서 또 다시 두 번째 숫자에 의해 첫 번째 숫자를 곱한

887
01:03:48,510 --> 01:03:55,990
다시 그 순서는 바로 같은 플레이 자신의 경우에 무엇을 이동 않습니다

888
01:03:55,989 --> 01:04:01,849
번호 하나 내가 죽거나 아직 경우 두 번째 번호를 정확히 절전 모드로 전환

889
01:04:01,849 --> 01:04:05,119
일년 실제로 폭발하지만, 그렇지 않는 경우에만 위치하도록

890
01:04:05,119 --> 01:04:09,679
정말 나쁜 일이 죽을 중 하나 일어나고 또는 우리는 우리가 큰이 여기 폭발

891
01:04:09,679 --> 01:04:12,659
도시 우리는 하나의 번호가없는 있지만, 사실은이 같은 일이 일어난다이다

892
01:04:12,659 --> 01:04:16,599
그것의 일반화는 WHS 장축 반경 스펙트럼에서 일어나는

893
01:04:16,599 --> 01:04:21,839
이는 그 행렬의 최대 고유 한 후보다 큰 것이다

894
01:04:21,840 --> 01:04:25,220
이 시민은 완전히 사망의 1도 이하의 경우 무선 신호가 폭발

895
01:04:25,219 --> 01:04:30,549
그래서 기본적으로 박사 탄 때문에이 재발이 매우 이상한이 있기 때문에

896
01:04:30,550 --> 01:04:34,680
공식 우리는 매우 끔찍 역학에 결국 그리고 그것은 매우 불안정입니다

897
01:04:34,679 --> 01:04:39,949
그냥 그렇게 연습이 처리 된 방법을 폭발하고 또는 사망했다

898
01:04:39,949 --> 01:04:44,439
당신은 폭발 그라디언트에게 인사말 마치 하나의 간단한 하키를 제어 할 수 있습니다

899
01:04:44,440 --> 01:04:45,720
폭발 당신은 그것을 클릭

900
01:04:45,719 --> 01:04:50,789
그래서 사람들은 실제로 매우 누덕 누덕 기운 솔루션처럼하지만 경우에이 관행을

901
01:04:50,789 --> 01:04:55,119
두 번 다섯 분 노먼 린 크램 펫 (25) 요소 위에합니까을 읽고있는 나

902
01:04:55,119 --> 01:04:58,150
당신이 저하되어 클리핑을 수행 할 수 있도록 그런 일이 그 방법을의

903
01:04:58,150 --> 01:05:01,829
폭발 등급을 매기는 문제를 해결하고 당신은 당신이 기록하고있어하지 않습니다

904
01:05:01,829 --> 01:05:06,049
더 이상 폭발 그러나 녹색당은 여전히​​ 직장과 엘리스에서 카니발에서 사라질 수 있습니다

905
01:05:06,050 --> 01:05:08,310
팀 때문에 이들의 사라지는 그라데이션 문제에 아주 좋은 것입니다

906
01:05:08,309 --> 01:05:12,429
단지와 첨가제의 상호 작용에 따라 변화되는 세포의 고속도로

907
01:05:12,429 --> 01:05:17,309
당신은 당신이이기 때문에 경우에 당신이 경우 구배는 단지 그들이 아래로 죽지 않을 날려

908
01:05:17,309 --> 01:05:21,000
이러한 이유 대략이다처럼 같은 나이 또는 무언가에 의해 곱

909
01:05:21,000 --> 01:05:26,909
단지 더 동적으로 우리는 항상 팀 그래서 우리는 그라데이션 클리핑을 수행 할

910
01:05:26,909 --> 01:05:30,149
일반적으로 달라스 팀의 기울기가 잠재적으로 폭발 할 수 있기 때문에

911
01:05:30,150 --> 01:05:33,400
여전히 그들은 일반적으로 사라하지 않는했다

912
01:05:33,400 --> 01:05:48,608
재발 성 신경 네트워크뿐만 아니라에 대한 엘리스 팀은 분명하지 않다 어디를

913
01:05:48,608 --> 01:05:53,769
당신이 플러그 것입니다 정확히 같은이 식의 명확하지에 뛰어들 것

914
01:05:53,769 --> 01:06:00,619
상대적으로 어디에 아마 대신 G에서 월의 많은 다음에 참석하기 때문에

915
01:06:00,619 --> 01:06:08,690
여기 huug하지만 재판매는 바로 이렇게 하나의 방향으로 성장할 것

916
01:06:08,690 --> 01:06:11,980
어쩌면 당신은 실제로 좋은 아니에요 작게 있도록 만드는 끝낼 수 없다

917
01:06:11,980 --> 01:06:18,539
난 당신이 알고있는 가정 아이디어는 이렇게 연결하는 명확한 방법이 없습니다 기본적으로됩니다

918
01:06:18,539 --> 01:06:25,380
여기에 행을 너무 좋아 한 것은 나는이 초 고속도로의 측면에서 그 통지

919
01:06:25,380 --> 01:06:29,780
네 개의 얻을 문이있을 때이 그라디언트 이러한 관점은 실제로 고장

920
01:06:29,780 --> 01:06:33,310
네 개의 얻을 때 때문에 케이트의 우리는 이러한 행위의 일부를 잊을 수있는 곳

921
01:06:33,309 --> 01:06:37,150
내가 문을 잊지 때마다 곱셈 상호 작용은 다음에 그것과 세가와

922
01:06:37,150 --> 01:06:41,470
다음 그라데이션을 죽이고 물론 역류 때문에 이러한 슈퍼 중단됩니다

923
01:06:41,469 --> 01:06:45,250
당신이없는 경우 고속도로 가지 사실 어느 문을 잊지하지만 당신은 경우

924
01:06:45,250 --> 01:06:50,000
a는 다음 그라디언트를 죽일 수 그들의줬고, 그래서 실제로 잊지했다

925
01:06:50,000 --> 01:06:54,710
우리는 우리와 함께 연주 할 때 팀은 우리가 가끔 사람들이 때 가정 오스틴의 사용이다

926
01:06:54,710 --> 01:06:58,099
긍정적 인 편견 때문에 함께 초기화에 그들이 처음 잊지 얻을

927
01:06:58,099 --> 01:06:58,769
에 의한

928
01:06:58,769 --> 01:07:05,699
나에 설정하는 것을 잊지 항상 종류의 내가 처음에 생각 해제

929
01:07:05,699 --> 01:07:08,679
그래서 처음에 녹색 아주 잘 이야기하고 미국 팀은 배울 수있는 방법

930
01:07:08,679 --> 01:07:12,779
그 해당 바이어스 용으로 나중에 사람들이 재생되도록 한 번에 그들을 차단하기

931
01:07:12,780 --> 01:07:17,530
수십 년 때때로 그래서 여기에 지난 밤 나는 그 비용을 언급하고 싶었다

932
01:07:17,530 --> 01:07:21,580
공간이 그래서 많은 사람들은 기본적으로이 꽤 플레이 한

933
01:07:21,579 --> 01:07:26,119
그들이 아키텍처로 다양한 변화를 시도 오디세이 용지 거기

934
01:07:26,119 --> 01:07:32,829
잠재적 인 변화의 큰 숫자 이상이 검색을 수행하려고 여기에 종이

935
01:07:32,829 --> 01:07:36,940
LST 방정식 그리고 그들은 많은 검색을했고, 그들은 아무것도 찾지 못했습니다

936
01:07:36,940 --> 01:07:42,300
그건 그냥 애널리스트 오전 너무 좋아하고있어보다 실질적으로 더 잘 작동

937
01:07:42,300 --> 01:07:45,560
또한 상대적으로 실제로 인기가 있고 내가 실제로 것 GRU

938
01:07:45,559 --> 01:07:50,159
당신이 콜로세움 그것의 변화를 개의 DRU 사용 할 수 있습니다 것이 좋습니다

939
01:07:50,159 --> 01:07:54,460
그것은 짧은 점이다 대해도 좋은 상호 작용으로 결정했다

940
01:07:54,460 --> 01:07:59,400
작은 공식과 단지 하나있는 테네시을 갖지 않는 트랙터

941
01:07:59,400 --> 01:08:03,130
구현은 현명한 단지 하나가 가진 기억 단지 좋네요 있도록 만 H가

942
01:08:03,130 --> 01:08:07,590
단지 작은 간단한 일이 같은 앞으로 과​​거 두 가지 요인에 차질

943
01:08:07,590 --> 01:08:12,190
그 불쾌한의 혜택의 대부분을 갖고있는 것 같아요하지만 그래서는 GRU과라고

944
01:08:12,190 --> 01:08:16,730
거의 항상 멋진에 대한 내 경험에 작동하고 그래서 당신은 수도

945
01:08:16,729 --> 01:08:19,939
그것을 사용하려는 또는 당신은 그들이 모두 좀 동일한 대해 알고 마지막 시간을 사용할 수 있습니다

946
01:08:19,939 --> 01:08:28,088
그래서 누군가가 마구는 아주 좋은하지만의 RaWR하고 실제로하지 않는 것입니다

947
01:08:28,088 --> 01:08:29,130
아주 잘 작동

948
01:08:29,130 --> 01:08:32,420
소유즈 미국 팀은 무엇을 그들에 대해 좋은 데요 것은 이상한 갖는 것입니다 대신 사용된다

949
01:08:32,420 --> 01:08:36,000
그리스을 허용 이러한 첨가제의 상호 작용은 매우 잘 재생 당신은하지 않습니다

950
01:08:36,000 --> 01:08:39,579
사라지는 품종 문제를 얻을 우리는 여전히 폭발에 대해 조금 걱정

951
01:08:39,579 --> 01:08:44,269
이 사람들은 때때로 내가이 여자 클립을 참조하는 것이 일반적 그래서 문제를 공급

952
01:08:44,270 --> 01:08:46,670
더 간단한 구조가 정말하려고하는 말 것

953
01:08:46,670 --> 01:08:50,838
연결과 무슨 깊은 거기에 뭔가 오는 방법을 이해

954
01:08:50,838 --> 01:08:53,899
주민과 엘리스 팀 사이에 이들에 대해 뭔가 깊은있다

955
01:08:53,899 --> 01:08:57,579
나는 우리가 아직 정확히 그 이유는 완전히 이해되지 것 같아요 상호 작용

956
01:08:57,579 --> 01:09:02,210
그래서 잘 작동하고 어떤 부분은 시원했고, 그래서 우리가 필요하다고 생각

957
01:09:02,210 --> 01:09:05,119
공간 이론과 경험을 모두 이해하고 그것은 매우이야

958
01:09:05,119 --> 01:09:10,979
벌리고 연구의 영역과 그래서 그래서

959
01:09:10,979 --> 01:09:23,469
스포츠 (10) 그러나 나는 내가 그렇지 않은 그래서 폭발 가정 할 수 클래스의 끝

960
01:09:23,470 --> 01:09:27,020
명확 왜 것이라고하지만 당신은 세포 상태로 그라데이션을 주입 유지

961
01:09:27,020 --> 01:09:30,069
그래서 어쩌면 때때로 큰 얻을 수 있습니다 저하

962
01:09:30,069 --> 01:09:33,960
그것은 그들을 수집하는 것이 일반적이지만 중요 할 수 있으므로 한 시간으로 아마 생각

963
01:09:33,960 --> 01:09:40,829
그리고, 나는 그 시점하지만 비뇨기과 기초 I에 대해 확실히 백퍼센트 아니에요

964
01:09:40,829 --> 01:09:46,640
흥미로운 무슨 생각 그래 나는 우리가 여기까지해야한다고 생각하지 않습니다 있지만 난

965
01:09:46,640 --> 01:09:47,569
여기에 질문을 드리겠습니다
